<!DOCTYPE html>
<html lang="en">



<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport"
    content="width=device-width,initial-scale=1.0,minimum-scale=1.0,maximum-scale=1.0,user-scalable=no" />

  <title>Challenge at CVPR 2023 Autonomous Driving Workshop | OpenDriveLab</title>
  <meta name="keywords"
    content="OpenDriveLab events, OpenDriveLab news, Autonomous Driving, E2EAD, BEV, OpenLane, Bird-eye-view Perception, End-to-end Autonomous Driving, Shanghai AI Lab">
  <meta name="author" content="OpenDriveLab">
  <link rel="icon" type="image/png" href="/style/img/team_logo.png">
  <link rel="stylesheet" type="text/css" href="/style/cvpr2023/css/all.css" media="all">
  <link rel="stylesheet" type="text/css" href="/style/cvpr2023/css/track1.css" media="all">
  <link rel="stylesheet" type="text/css" href="/style/cvpr2023/css/track2.css" media="all">
  <link rel="stylesheet" type="text/css" href="/style/cvpr2023/css/track3.css" media="all">
  <link rel="stylesheet" type="text/css" href="/style/cvpr2023/css/track4.css" media="all">
  <link href="/style/css/mailing_list.css?1" rel="stylesheet" type="text/css" media="all">


  <!-- latex support -->
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      }
    };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

  <link href="/style/css/footer.css?11" rel="stylesheet" type="text/css" media="all">
  <link href="/style/css/head_event.css" rel="stylesheet" type="text/css" media="all">
  <style>
    .r_ul a {
      padding: 0px 0px;
    }

    @media screen and (min-width:1024px) {
      .r_ul.r_ul_this {
        position: fixed;
        top: 0;
        bottom: auto;
      }
    }
  </style>
</head>



<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-L7VEDHS6G8"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-L7VEDHS6G8');
</script>




<body class="home">



  <div class="header" href="/"> <a class="head_fanhui" href="/"></a>
    <div class="rnav m" style="margin-left: 86%;"></div>
  </div>



  <!-- banner -->

  <div class="bannerw">
    <div class="banner" style="background-image: url(/style/cvpr2023/img/index_banner_s.jpg); background-size: cover;">
      <div class="banner_dian"></div>

      <div class="banner_textw">
        <div class="bg">
          <div class="juzhongduiqi">
            <div>

              <div class="t_text">
                <img src="/style/cvpr2023/img/cvpr.png" class="title_cvpr" />
                <img src="/style/cvpr2023/img/ieee.svg" class="title_cvpr" />
              </div>

              <div class="t_text">
                <img src="/style/cvpr2023/img/auto.svg" class="title_auto" />
                <br>
                <img src="/style/cvpr2023/img/challeng.svg" class="title_challeng" />
              </div>

              <p class="r_t" style="user-select: none; font-size: 15px; line-height: 0px;">
                In conjunction with
              </p>

              <!-- <p>
                  <img src="/style/cvpr2023/img/cvpr2023.svg" class="titel_cvpr" data-swiper-parallax-x="-1600" />
                </p> -->
              <p class="my_subtitle">
                CVPR 2023 Workshops
              </p>


              <div class="banner_text">
                <p>
                  <a href="/e2ead/cvpr23.html">
                    End-to-End Autonomous Driving Workshop
                  </a>
                </p>
                <p>
                  <a href="https://vcad.site/">
                    Vision-Centric Autonomous Driving Workshop
                  </a>
                </p>
              </div>

              <p class="r_time" style="font-size: 15px;">
                June 18, Vancouver, Canada
              </p>

              <p class="r_time" style="font-size: 15px;">
                Also Join: <a href="/e2ead/cvpr23_CN.html" class="my_subtitle_a">自动驾驶论坛 @ 北京</a>
              </p>

              <!-- <p>
                  <img src="/style/cvpr2023/img/sh_ai_lab.png" class=" banner_img" data-swiper-parallax-x="-1600" />
                </p> -->



              <!-- <span class="lj_button" data-swiper-parallax-x="-4000"  data-hd="#Contact">Contact</span>  -->
              <!-- <span class="lj_button2" data-swiper-parallax-x="-4000"  data-hd="#Participation">Participate (closed)</span> -->
              <!-- <span class="lj_button2" data-swiper-parallax-x="-4000"  data-hd="#rules">Updated Rules</span> -->

              <p class="r_time" style="line-height: 150%; user-select: auto; font-size: 15px;">
                The challenge wraps up with success! Thanks for your participation!
                <br>
                If you wish to add new / modify results to the <b style="text-decoration: underline;">OpenLane Topology</b> or <b style="text-decoration: underline;">3D Occupancy Prediction</b> challenge, please drop us an email to <a class='my_subtitle_email' href="mailto:contact@opendrivelab.com">contact@opendrivelab.com</a>
              </p>

            </div>
          </div>
        </div>
      </div>

    </div>



    <!-- navigation bar -->

    <div class="r_ul">
      <ul class="bg">
        <li data-gd="#tttttt" class="li_mobile"><a href="/"><img src="/style/img/team_logo_long.png"
              style="display: inline; width: 50%; margin-left: -3%;"></a></li>
        <li data-gd=".bg_why"><a href="#overview">Overview</a></li>
        <li data-gd="#Track1"><a href="#openlane_topology">OpenLane Topology</a></li>
        <li data-gd="#Track2"><a href="#online_hd_map_construction">Online HD Map Construction</a></li>
        <li data-gd="#Track3"><a href="#3d_occupancy_prediction">3D Occupancy Prediction</a></li>
        <li data-gd="#Track4"><a href="#nuplan_planning">nuPlan Planning</a></li>
        <li data-gd="#rules"><a href="#general_rules">General Rules</a></li>
        <li data-gd="#FAQ"><a href="#faq">FAQ</a></li>
      </ul>
    </div>

  </div>


  <div id="mailing_list"></div>


  <!-- overview -->

  <div class="bg_imgx">
    <div class="bg_why">

      <div class="bg" id="overview">

        <h3>
          <b class="title" target="_blank" id="Overview"><a href="#overview"
              style="color: rgb(51, 51, 51);">Overview<img src="/style/img/icon/link.png" class="title_link" /></a></b>
        </h3>

        <br>

        <h4>
          <b>Why these Challenges?</b>
        </h4>
        <p>
          <!-- Autonomous driving is developing fast. 
            Although still deemed important, the requirement for cutting-edge algorithms is no longer to achieve as high as mAP for object detectors, or to recognize lanes as conventional segmentation. 
            We believe the <topic>future</topic> of autonomous driving algorithms is to <topic>bond perception closely with planning</topic>. 
            As such, we introduce four <topic>curated, brand-new</topic> challenges following such a philosophy. -->
          The field of autonomous driving (AD) is rapidly advancing, and while cutting-edge algorithms remain a crucial
          component, the emphasis on achieving high mean average precision (mAP) for object detectors or conventional
          segmentation for lane recognition is no longer paramount. Rather, we posit that the future of AD algorithms
          lies in the integration of perception and planning. In light of this, we propose four newly curated challenges
          that embody this philosophy.
        </p>

      </div>



      <!-- <div class="why_list">
          <ul>
            <li>
              <h2>OpenLane Topology Challenge</h2>
              Go beyond conventional lane line detection as segmentation. 
              Recognizing lanes as an abstraction of the scene - <code>centerline</code>, and building the topology between lanes and traffic elements. 
              Such a <code>topology</code> is to facilitate planning and routing.
            </li>
            <li>
              <h2>Online HD Map Construction Challenge</h2>
              Traditional mapping pipelines require a vast amount of human effort to maintain, which limits their scalability. 
              This task aims to dynamically construct local <code>maps</code> with rich semantics based on onboard sensors. 
              The vectorized map can be further utilized by downstream tasks.
            </li>
            <li>
              <h2>3D Occupancy Prediction Challenge</h2>
              The representation of 3D bounding boxes is not enough to describe general objects (obstacles).
              Instead, inspired by the concept in Robotics, we deem general object detection as an <code>occupancy</code> representation to cover more irregularly shaped objects (e.g., protruding).
              The output could also be fed as cost volume for planning. 
              This idea is also endorsed by <a href="https://www.mobileye.com/ces-2023">Mobileye at CES 2023</a> and <a href="https://www.youtube.com/watch?v=ODSJsviD_SU">Tesla AI Day 2022</a>.
            </li>
            <li>
              <h2>nuPlan Planning Challenge</h2>
              To verify the effectiveness of the newly-designed modules in perception, we need an ultimate planning framework with
              a <code>closed-loop</code> setting.
              Previous motion planning benchmarks focus on short-term motion forecasting and are limited to open-loop evaluation. 
              <code>nuPlan</code> introduces long-term planning of the ego vehicle and corresponding metrics.
            </li>
          </ul>
        </div> -->

    </div>



    <div class="bg">

      <img alt="motivation" src="/style/cvpr2023/img/overview_2_t_s.jpg">

      <h3 class="mot_ts" style="text-transform: none;">
        Motivation of the Challenges: bond perception more closely with planning.
      </h3>

    </div>
    <br>



    <div class="track_nav">

      <dl data-hd="#Track1">
        <dt class="track">Track 1</dt>
        <dd>OpenLane Topology Challenge</dd>
        <dt class="text">
          Go beyond conventional lane line detection as segmentation.
          Recognizing lanes as an abstraction of the scene - <eachtrack>centerline</eachtrack>, and building the
          topology between lanes and traffic elements.
          Such a <eachtrack>topology</eachtrack> is to facilitate planning and routing.
        </dt>
      </dl>

      <dl data-hd="#Track2">
        <dt>Track 2</dt>
        <dd>Online HD Map Construction Challenge</dd>
        <dt class="text">
          Traditional mapping pipelines require a vast amount of human effort to maintain, which limits their
          scalability.
          This task aims to dynamically construct local <eachtrack>maps</eachtrack> with rich semantics based on onboard
          sensors.
          The vectorized map can be further utilized by downstream tasks.
        </dt>
      </dl>

      <dl data-hd="#Track3">
        <dt>Track 3</dt>
        <dd>3D Occupancy Prediction Challenge</dd>
        <dt class="text">
          The representation of 3D bounding boxes is not enough to describe general objects (obstacles).
          Instead, inspired by the concept in Robotics, we deem general object detection as an <eachtrack>occupancy
          </eachtrack> representation to cover more irregularly shaped objects (e.g., protruding).
          The output could also be fed as cost volume for planning.
          This idea is also endorsed by <eachtrack>Mobileye at CES 2023</eachtrack> and <eachtrack>Tesla AI Day 2022
          </eachtrack>.
        </dt>
      </dl>

      <dl data-hd="#Track4">
        <dt>Track 4</dt>
        <dd>nuPlan Planning Challenge</dd>
        <dt class="text">
          To verify the effectiveness of the newly-designed modules in perception, we need an ultimate planning
          framework with
          a <eachtrack>closed-loop</eachtrack> setting.
          Previous motion planning benchmarks focus on short-term motion forecasting and are limited to open-loop
          evaluation.
          <eachtrack>nuPlan</eachtrack> introduces long-term planning of the ego vehicle and corresponding metrics.
        </dt>
      </dl>

    </div>
  </div>



  <div class="bg_huise kin Contact_br">
    <div class="bg">

      <!-- Participation -->

      <!-- <h4>
          <b id="Participation">Participation</b>
        </h4>

        <p> 
          For participation, you can create a team at EvalAI, and then make a submission. 
          A valid submission would be automatically viewed as a successful participation.
          No more than <b>ten</b> individuals are allowed in a team.
          For more details please refer to <a data-gd="#FAQ">FAQ</a> and description in each track.
          <br><br>
          <iframe src="https://docs.google.com/forms/d/e/1FAIpQLSepfNu34EF80BIA7DwbqVOUD3hCytIXAU42gIIXna3OwiTlgA/viewform?embedded=true" width=100% height=400px frameborder="0" marginheight="20" marginwidth="20">正在加载…</iframe>
        </p> -->



      <!-- <button class="btn_from" > <i class="tb"></i> Participation</button>
        <div class="open_from"> <div class="close"></div>
          <div class="open_from_in">
        
          <iframe
                        src="https://docs.google.com/forms/d/e/1FAIpQLSepfNu34EF80BIA7DwbqVOUD3hCytIXAU42gIIXna3OwiTlgA/viewform?embedded=true"
                      frameborder="0" >正在加载…</iframe>
        </div>
        </div> -->



      <br>



      <!-- baseline -->

      <h4>
        <b>Get Started</b>
      </h4>

      <p>
        Baseline models are provided. Please check out to GitHub repository for each track.
        <br>
        Moreover, we introduce the multimodal multitask general large model <a
          href="https://github.com/OpenGVLab/InternImage">InternImage</a> to serve as a strong backbone.
        Check out <a href="https://github.com/OpenGVLab/InternImage/tree/master/autonomous_driving">here</a> for
        implementation details.
      </p>



      <br>



      <!-- contact -->

      <h4>
        <b id="Contact">Contact</b>
      </h4>

      <p>
        <a href="mailto:workshop-e2e-ad@googlegroups.com" class="h_imga">
          <img src="https://img.shields.io/badge/email-Send-important?logo=gmail&amp" alt="Send E-mail"
            style="height: 25px" />
        </a>
        Contact us via <code>workshop-e2e-ad@googlegroups.com</code> with the prefix [CVPR 2023 E2EAD].
      </p>

      <p>
        <a href="https://join.slack.com/t/opendrivelab/shared_invite/zt-1rcp42b35-Wc5I0MhUrahlM5qDeJrVqQ"
          class="h_imga" target="_blank">
          <img src="https://img.shields.io/badge/Slack-Join-important?logo=slack&amp" alt="Join Slack"
            style="height: 25px" />
        </a>
        Join Slack to chat with Challenge organizers. Please follow the guidelines in <code>#general</code> and join the
        track-specific channels.
      </p>

      <!-- <p>
        <a href="style/img/wechatgroup.jpeg" class="h_imga" target="_blank">
          <img src="https://img.shields.io/badge/WeChat-Join-important?logo=wechat&amp" alt="Join Wechat"
            style="height: 25px" />
        </a>
        Join WeChat group to chat with Challenge organizers.
      </p> -->



      <br>



      <!-- host -->

      <h4>
        <b>Host</b>
      </h4>

      <p>
        The year 2023's edition of the Challenge is hosted by:
        <br><br>
      <div class="link_logo">
        <ul>
          <li>
            <a href="/" target="_blank">
              <img src="/style/cvpr2023/img/link_1.svg">
              <p>
                <b class="host">Shanghai AI Lab<br>OpenDriveLab</b>
              </p>
            </a>
          </li>
          <li>
            <a href="http://group.iiis.tsinghua.edu.cn/~marslab/#" target="_blank">
              <img src="/style/cvpr2023/img/200px-Tsinghua_University_Logo.svg.png" class="qinghua_logo">
              <p>
                <br>
                <b class="host">Tsinghua University<br>MARS Lab</b>
              </p>
            </a>
          </li>
          <li>
            <a href="https://motional.com" target="_blank">
              <img src="/style/cvpr2023/img/link_3.svg">
              <p>
                <b class="host">Motional<br>nuPlan / nuScenes Team</b>
              </p>
            </a>
          </li>
        </ul>
      </div>
      </p>
<!-- 
      <br>

      <h4>
        <b>Sponsor</b>
      </h4>

      <p>
      
      <div class="link_logo" style="width: 95%;">
        <ul>
          <li>
            <a href="https://www.hp.com" target="_blank">
              <img src="/style/cvpr2023/img/hp.svg"  class="qinghua_logo">
              <br><br>
              <p>
                <b class="host">HP</b>
              </p>
            </a>
          </li>
        </ul>
      </div>
      </p> -->


    </div>
  </div>

  <div class="bg_huise kin" style="background-color: #f2f2f2; margin-bottom: 70px" ;>
    <div class="bg">

      <h3>
        <b class=" title">Summary</b>
      </h3>

      <p>
        <!-- During the Autonomous Driving Challenge, we witnessed intensive competition in the AI community.
          Numerous minds from China, the United States, the United Kingdom, and beyond join to tackle the formidable tasks in the autonomous driving domain.
          With over <b class="blue_emphasize">270</b> teams from <b class="blue_emphasize">15</b> countries and regions participating, the challenge has been a true showcase of global talent and innovation.
          Over the course of <b  class="blue_emphasize">2,300</b> submissions, the top spot has been fiercely contested, with the lead changing hands on nearly a daily basis.
          <br><br>
          The challenge period may have come to an end, but the pursuit of excellence continues.
          Over <b  class="blue_emphasize">20</b> technical reports are submitted to share their ideas.
          After a small break, all tracks will be back online as the regular test servers or for the upcoming challenges.
          For the future plans and details, please stay tuned on our website.
          <br><br>
          When drawing a summary of the challenge, we found that the competition in the 3D Occupancy Prediction track is so intensive.
          Thus, we set up an extra Innovation Award to honor your remarkable efforts.
          For all tracks, the top-performing teams are entitled to Outstanding Champion or Honorable Runner-up to show our respect.  -->

        The Autonomous Driving Challenge at CVPR 2023 just wrapped up! We have witnessed an intensive engagement from
        the community. Numerous minds from universities and corporations, including China, Germany, France, Singapore,
        United States, United Kingdom, etc., join to tackle the challenging tasks for autonomous driving. With over <b
          class="blue_emphasize">270</b> teams from <b class="blue_emphasize">15</b> countries (regions), the challenge
        has been a true showcase of global talent and innovation. Over the course of <b class="blue_emphasize">2,300</b>
        submissions, the top spot has been fiercely contested. We received a few inquiries on the eligibility, challenge
        rules, technical reports. Rest assured that all concerns have been appropriately addressed. The fairness and
        integrity of the Challenge has always been our highest priority.
        <!-- <br><br>
          Over <b class="blue_emphasize">20</b> technical reports are submitted and carefully examined by the Award Committee. 
           -->
      </p>
    </div>

  </div>



  <!-- track 1 -->

  <div class="bg" id="openlane_topology">

    <h3>
      <b class="title" target="_blank" id="Track1">
        <a href="#openlane_topology">
          Track 1
          <br>
          OpenLane Topology
          <img src="/style/img/icon/link.png" class="title_link" />
        </a>
      </b>
    </h3>

    <a href="https://github.com/OpenDriveLab/OpenLane-V2">
      <img src="https://img.shields.io/badge/website-GitHub-darkgreen" alt="GitHub" class="h_img" />
    </a>

    <a href="https://github.com/OpenDriveLab/OpenLane-V2">
      <img src="https://img.shields.io/github/stars/OpenDriveLab/OpenLane-V2?style=social" alt="GitHub" class="h_img" />
    </a>

    <a href="https://github.com/OpenDriveLab/OpenLane-V2">
      <img src="https://img.shields.io/github/forks/OpenDriveLab/OpenLane-V2?style=social" alt="GitHub" class="h_img" />
    </a>

    <a href="https://eval.ai/web/challenges/challenge-page/1925">
      <img src="https://img.shields.io/badge/submission-EvalAI-darkgreen" alt="EvalAI" class="h_img" />
    </a>



    <br>
    <br>
    <p>
      <div class="innovation">
        We are happy to announce an important update to the OpenLane family, featuring two sets of additional data and annotations, namely <b class="blue_emphasize">Standard-definition (SD) Map</b> and <b class="blue_emphasize">Map Element Bucket</b>. Check out our <a href="https://github.com/OpenDriveLab/OpenLane-V2" target="_blank">GitHub repository</a> for more details on the update, leaderboard, and upcoming challenge in 2024.
        <br><br>
        <div style="display: flex; text-align: center;">
          <img src='/style/cvpr2023/img/lanesegment.png' class="imghalf"/>
          <img src='/style/cvpr2023/img/sdmap.png'  class="imghalf" />
        </div>
      </div>
    </p>
    <br>

    <h4>
      <b style="text-transform: none;">Leaderboard (Server remains active)</b>
    </h4>

    <p>
      <li style="list-style-type:disc;"># Participating Teams: 34</li>
      <li style="list-style-type:disc;"># Countries and Regions: 4</li>
      <li style="list-style-type:disc;"># Submissions: 700+</li>
      <!-- In the OpenLane Topology Challenge, 34 teams from 3 different countries and regions have made more than 700 submissions. -->
      <!-- As shown on the public leaderboard, most teams have an OLS around the average score of 36. 
        The top-performing team is winning with a significant margin of about 10 OLS. -->
      The majority of the methods were able to achieve OLS within the range of 30 to 40. It is noteworthy that one
      method in particular emerged as the clear frontrunner, demonstrating a remarkably superior performance with an OLS
      of 55.
    </p>

    <div class="scrollable">
      <table style="text-align: center;" class="table_list">
        <thead>
          <tr style="width: 100%;">
            <th style="width: 5%; min-width: 75px;">Rank</th>
            <th style="width: 5%; min-width: 100px;">Country / Region</th>
            <th style="width: 20%;">Institution</th>
            <th style="width: 10%; min-width: 175px;"><b>OLS (primary)</b></th>
            <th style="width: 20%;">Team Name</th>
            <th style="width: 10%;">$\text{DET}_{l}$</th>
            <th style="width: 10%;">$\text{DET}_{t}$</th>
            <th style="width: 10%;">$\text{TOP}_{ll}$</th>
            <th style="width: 10%;">$\text{TOP}_{lt}$</th>
          </tr>
        </thead>
        <tbody>
          <tr class="page">
            <td colspan="2"><button id="prev">Previous</button></td>
            <td colspan="1"></td>
            <td colspan="1"><button id="next">Next</button></td>
            <td colspan="5">
              Page <input type="text" id="curpage" value="1"> of <span id="total">30</span>
              &nbsp;&nbsp;&nbsp;
              <select name="" id="pageOption">
                <option value="10">10rows</option>
                <option value="20">20rows</option>
                <!-- <option value="25">25rows</option> -->
                <!-- <option value="50">50rows</option>
                  <option value="100">100rows</option> -->
              </select>
            </td>
          </tr>
        </tbody>
      </table>

    </div>
    <p class="footer_ts">
      * Not involed in the final official ranking.
    </p>
    <!-- <p class="footer_ts">
        * &nbsp; For the real-time leaderboard please refer to <a href="https://eval.ai/web/challenges/challenge-page/1925/leaderboard">EvalAI</a>.
        <br>
        ** $\text{F-Score}$ for lane detection is not taken into consideration in both the challenge and leaderboard.
      </p> -->
    <!-- <img src="/style/img/events/challenge_2023/leaderboard_track_1.jpg" class="leaderboard"> -->
    <!-- <p>
        Above is the final leaderboard. A notification was sent from <a href="javaScript:">openlane-topology@googlegroups.com</a>. If you did not receive the notification, please specify your email and team name to <a href="mailto:wanghuijie@pjlab.org.cn">wanghuijie@pjlab.org.cn</a>.
      </p> -->
    <p>
      <img src='/style/cvpr2023/img/blomb1.png' style="width: 20px; user-select: none;" /> <b
        class="blue_emphasize">Innovation Award goes to "PlatypusWhisperers" for</b><br>
    <div class="innovation">The team PlatypusWhisperers introduces an approach called TopoMask, offering an innovative
      solution for predicting centerlines in road topology. By utilizing an instance-mask based formulation and a
      quad-direction label representation, TopoMask effectively addresses the overlapping issue of centerlines and
      extends segmentation-based manner to scene understanding tasks.</div>
    </p>

    <div class="container tm-container-gallery">
      <div class="row">
        <div class="col-2">
        </div>
        <div class=" col-6">
          <p>If you use the challenge dataset in your paper, please consider citing the following BibTex:</p>


          <div
            style="background-color: rgba(242, 242, 242,1); padding-top: 10px; padding-bottom: 10px; border-radius: 10px; white-space: nowrap; overflow-x: scroll;">
            &nbsp;&nbsp;&nbsp;@inproceedings{wang2023openlanev2,
            <br>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;title={OpenLane-V2: A Topology Reasoning Benchmark for Unified 3D HD Mapping},
            <br>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;author={Wang, Huijie and Li, Tianyu and Li, Yang and Chen, Li and Sima, Chonghao and Liu, Zhenbo and Wang, Bangjun and Jia, Peijin and Wang, Yuting and Jiang, Shengyin and Wen, Feng and Xu, Hang and Luo, Ping and Yan, Junchi and Zhang, Wei and Li, Hongyang},
            <br>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;booktitle={NeurIPS},
            <br>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;year={2023}
            <br>
            &nbsp;&nbsp;&nbsp;}
          </div>
        </div>
        <div class="col-">
        </div>

      </div>

    </div>

    <div class="container tm-container-gallery">
      <div class="row">
        <div class="col-2">
        </div>
        <div class=" col-6">
          <br>

          <div
            style="background-color: rgba(242, 242, 242,1); padding-top: 10px; padding-bottom: 10px; border-radius: 10px; white-space: nowrap; overflow-x: scroll;">
            &nbsp;&nbsp;&nbsp;@article{li2023toponet,
            <br>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;title={Graph-based Topology Reasoning for Driving Scenes},
            <br>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;author={Li, Tianyu and Chen, Li and Wang, Huijie and Li, Yang and Yang, Jiazhi and Geng, Xiangwei and Jiang, Shengyin and Wang, Yuting and Xu, Hang and Xu, Chunjing and Yan, Junchi and Luo, Ping and Li, Hongyang},
            <br>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;journal={arXiv preprint arXiv:2304.05277},
            <br>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;year={2023}
            <br>
            &nbsp;&nbsp;&nbsp;}
          </div>
        </div>
        <div class="col-">
        </div>

      </div>

    </div>

    <br><br>



    <h4>
      <b>Task Description</b>
    </h4>

    <p>
      The <a href="https://github.com/OpenDriveLab/OpenLane-V2">OpenLane-V2</a> dataset* is the perception and reasoning
      benchmark for scene structure in autonomous driving.
      Given multi-view images covering the whole panoramic field of view,
      participants are required to deliver not only perception results of lanes and traffic elements but also topology
      relationships among lanes and between lanes and traffic elements simultaneously.
    </p>

    <p class="footer_ts">
      * The dataset, OpenLane-V2 at Shanghai AI Lab, is named as <b class="road_genome">Road Genome</b> at Huawei and
      publically as OpenLane-Huawei.
    </p>



    <br>



    <h4>
      <b>Participation</b>
    </h4>
    <p>
      The primary metric is <a href="https://github.com/OpenDriveLab/OpenLane-V2#task">OpenLane-V2 Score (OLS)</a>,
      which comprises evaluations on three sub-tasks.
      On the website, we provide tools for
      <a href="https://github.com/OpenDriveLab/OpenLane-V2/blob/master/docs/devkit.md#openlanev2dataset">data
        access</a>,
      <a href="https://github.com/OpenDriveLab/OpenLane-V2#train-a-model">training models</a>,
      <a
        href="https://github.com/OpenDriveLab/OpenLane-V2/blob/master/docs/devkit.md#openlanev2evaluation">evaluations</a>,
      and <a
        href="https://github.com/OpenDriveLab/OpenLane-V2/blob/master/docs/devkit.md#openlanev2visualization">visualization</a>.
      To submit your results on <a href="https://eval.ai/web/challenges/challenge-page/1925/overview">EvalAI</a>, please
      follow the <a
        href="https://github.com/OpenDriveLab/OpenLane-V2/blob/master/docs/submission.md#submission">submission
        instructions</a>.
    </p>



    <br>



    <!-- <h4>
        <b>Important dates</b>
      </h4>

      <table class="itable">
        <tbody>
          <tr >
            <td>Challenge Period Open</td><td>March 15, 2023</td>
          </tr>
          <tr >
            <td>Challenge Period End</td><td>June 01, 2023</td>
          </tr>
          <tr >
            <td>Technical Report Deadline</td><td>June 09, 2023</td>
          </tr>
          <tr >
            <td>Winner Announcement</td><td>June 13, 2023</td>
          </tr>
        </tbody>
      </table>


      
      <br> -->







    <h4><b>Award</b></h4>

    <table class="itable">
      <tbody>
        <tr>
          <td><img src='/style/cvpr2023/img/rank01.png' style="width: 20px; user-select: none;" /> Outstanding Champion
          </td>
          <td>USD $15,000</td>
        </tr>
        <tr>
          <td><img src='/style/cvpr2023/img/rank02.png' style="width: 20px; user-select: none;" /> Honorable Runner-up
          </td>
          <td>USD $5,000</td>
        </tr>
        <tr>
          <td><img src='/style/cvpr2023/img/blomb1.png' style="width: 20px; user-select: none;" /> Innovation Award
          </td>
          <td>USD $5,000</td>
        </tr>
      </tbody>
    </table>



    <br>



    <h4>
      <b>Contact</b>
    </h4>

    <li style="list-style-type:disc">
      Huijie Wang (OpenDriveLab), <code>wanghuijie@pjlab.org.cn</code>
    </li>
    <li style="list-style-type:disc">
      Slack channel: <code>#openlane-challenge-2023</code>
    </li>



    <br><br>



    <h4>
      <b>Related Literature</b>
    </h4>

    <li style="list-style-type:disc;">
      <a href="https://arxiv.org/pdf/2304.05277.pdf">
        Topology Reasoning for Driving Scenes
      </a>
    </li>

    <li style="list-style-type:disc;">
      <a href="https://arxiv.org/pdf/2304.10440.pdf">
        OpenLane-V2: A Topology Reasoning Benchmark for Scene Understanding in Autonomous Driving
      </a>
    </li>

    <li style="list-style-type:disc;">
      <a href="https://arxiv.org/pdf/2203.11089.pdf">
        PersFormer: 3D Lane Detection via Perspective Transformer and the OpenLane Benchmark
      </a>
    </li>

    <li style="list-style-type:disc;">
      <a
        href="https://openaccess.thecvf.com/content/ICCV2021/papers/Can_Structured_Birds-Eye-View_Traffic_Scene_Understanding_From_Onboard_Images_ICCV_2021_paper.pdf">
        Structured Bird's-Eye-View Traffic Scene Understanding From Onboard Images
      </a>
    </li>

    <li style="list-style-type:disc;">
      <a href="https://arxiv.org/pdf/2208.14437.pdf">
        MapTR: Structured Modeling and Learning for Online Vectorized HD Map Construction
      </a>
    </li>



    <br>
    <br>
    <br>
  </div>



  <!-- track 2 -->

  <div class="bg_huise">
    <div class="bg" id="online_hd_map_construction">
      <br>
      <br>
      <br>

      <h3>
        <b class="title" target="_blank" id="Track2">
          <a href="#online_hd_map_construction">
            Track 2
            <br>
            Online HD Map Construction
            <img src="/style/img/icon/link.png" class="title_link" />
          </a>
        </b>
      </h3>

      <a href="https://github.com/Tsinghua-MARS-Lab/Online-HD-Map-Construction-CVPR2023">
        <img src="https://img.shields.io/badge/website-GitHub-darkcyan" alt="GitHub" class="h_img" />
      </a>

      <a href="https://github.com/Tsinghua-MARS-Lab/Online-HD-Map-Construction-CVPR2023">
        <img
          src="https://img.shields.io/github/stars/Tsinghua-MARS-Lab/Online-HD-Map-Construction-CVPR2023?style=social"
          alt="GitHub" class="h_img" />
      </a>

      <a href="https://github.com/Tsinghua-MARS-Lab/Online-HD-Map-Construction-CVPR2023">
        <img
          src="https://img.shields.io/github/forks/Tsinghua-MARS-Lab/Online-HD-Map-Construction-CVPR2023?style=social"
          alt="GitHub" class="h_img" />
      </a>

      <a href="https://eval.ai/web/challenges/challenge-page/1954/overview">
        <img src="https://img.shields.io/badge/submission-EvalAI-darkcyan" alt="EvalAI" class="h_img" />
      </a>



      <br>
      <br>
      <br>

      <h4>
        <b>Leaderboard</b>
      </h4>

      <p>
        <li style="list-style-type:disc;"># Participating Teams: 42</li>
        <li style="list-style-type:disc;"># Countries and Regions: 3</li>
        <li style="list-style-type:disc;"># Submissions: 500+</li>
        <!-- In the Online HD Map Construction Challenge, 42 teams from 3 different countries and regions have made more than 500 submissions. -->
        <!-- Despite tiny tiny variance of 1 from the 2nd to the 5th position, the winner take up a significant margin. -->
        The competition among the second to sixth positions on the leaderboard was highly intense, with a difference of
        less than 3 points in mAP between them. In contrast, the first place holder demonstrated a significant lead of
        almost 10 points in mAP.
      </p>

      <div class="scrollable">
        <table style="text-align: center;" class="table_list_2">
          <thead>
            <tr style="width: 100%;">
              <th style="width: 5%; min-width: 75px;">Rank</th>
              <th style="width: 5%; min-width: 100px;">Country / Region</th>
              <th style="width: 20%;">Institution</th>
              <th style="width: 10%; min-width: 175px;"><b>mAP (primary)</b></th>
              <th style="width: 20%; ">Team Name</th>
              <th style="width: 10%;">Ped Crossing</th>
              <th style="width: 10%;">Divider</th>
              <th style="width: 10%;">Boundary</th>
            </tr>
          </thead>
          <tbody>
            <tr class="page_2">
              <td colspan="2"><button id="prev_2">Previous</button></td>
              <td colspan="1"></td>
              <td colspan="1"><button id="next_2">Next</button></td>
              <td colspan="4">
                Page <input type="text" id="curpage_2" value="1"> of <span id="total_2">30</span>
                &nbsp;&nbsp;&nbsp;
                <select name="" id="pageOption_2">
                  <option value="10">10rows</option>
                  <option value="20">20rows</option>
                  <!-- <option value="25">25rows</option> -->
                  <!-- <option value="50">50rows</option>
                  <option value="100">100rows</option> -->
                </select>
              </td>
            </tr>
          </tbody>
        </table>

      </div>
      <!-- <div class="scrollable">
        <table style="text-align: center;">
          <thead>
            <tr>
              <th style="background-color: rgb(1, 126, 126);">Rank</th>
              <th style="background-color: rgb(1, 126, 126);">Method</th>
              <th style="background-color: rgb(1, 126, 126);"><b>mAP <i>(primary metric)</i></b></th>
              <th style="background-color: rgb(1, 126, 126);">$AP_{div}\text{**}$</th>
              <th style="background-color: rgb(1, 126, 126);">$AP_{bound}\text{**}$</th>
              <th style="background-color: rgb(1, 126, 126);">$AP_{pc}\text{**}$</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>1</td>
              <td>Host_75980_Team</td>
              <td>42.11</td>
              <td>50.11</td>
              <td>40.26</td>
              <td>35.95</td>
            </tr>
          </tbody>
        </table>
      </div> -->

      <!-- <p class="footer_ts">
        * &nbsp; For the real-time leaderboard please refer to <a href="https://eval.ai/web/challenges/challenge-page/1954/leaderboard">EvalAI</a>.
        <br>
        ** $AP$ on different classes: lane divider (div), boundary (bound), and pedestrian crossing (pc).
      </p> -->
      <!-- <img src="/style/img/events/challenge_2023/leaderboard_track_2.png" class="leaderboard"> -->
      <!-- <p>
        Above is the final leaderboard. A notification was sent from <a href="javaScript:">online-hd-map-construction@googlegroups.com</a>. If you did not receive the notification, please specify your email and team name to <a href="mailto:wanghuijie@pjlab.org.cn">wanghuijie@pjlab.org.cn</a>.
      </p> -->

      <p>
        <img src='/style/cvpr2023/img/blomb1.png' style="width: 20px; user-select: none;" /> <b
          class="blue_emphasize">Innovation Award goes to "MACH" for</b><br>
      <div class="innovation">The team MACH introduces the MaskDino method into map detection tasks, combining the
        advantages of both vectorization and rasterization as two different map representations. They also propose a
        simple and practical post-processing-based method for model ensembling. These contributions exhibit strong
        novelty.</div>
      </p>

      <div class="container tm-container-gallery">
        <div class="row">
          <div class="col-2">
          </div>
          <div class=" col-6">
            <p>If you use the challenge dataset in your paper, please consider citing the following BibTex:</p>


            <div
              style="background-color: rgb(230, 230, 230); padding-top: 10px; padding-bottom: 10px; border-radius: 10px; white-space: nowrap; overflow-x: scroll;">
              &nbsp;&nbsp;&nbsp;@article{liu2022vectormapnet,
              <br>
              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;title={Vectormapnet: End-to-end vectorized hd map learning},
              <br>
              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;author={Liu, Yicheng and Wang, Yue and Wang, Yilun and Zhao, Hang},
              <br>
              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;journal={arXiv preprint arXiv:2206.08920},
              <br>
              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;year={2022}
              <br>
              &nbsp;&nbsp;&nbsp;}
            </div>
          </div>
          <div class="col-">
          </div>

        </div>

      </div>




      <br>



      <h4>
        <b>Task Description</b>
      </h4>

      <p>
        Compared to conventional lane detection, the constructed HD map provides more semantics information with
        multiple categories.
        Vectorized polyline representations are adopted to deal with complicated and even irregular road structures.
        Given inputs from onboard sensors (cameras), the goal is to construct the complete local HD map.
      </p>



      <br>



      <h4>
        <b>Participation</b>
      </h4>

      <p>
        The primary metric is mAP based on Chamfer distance over three categories, namely lane divider, boundary, and
        pedestrian crossing.
        Please refer to our <a
          href="https://github.com/Tsinghua-MARS-Lab/Online-HD-Map-Construction-CVPR2023">GitHub</a> for details on
        <a
          href="https://github.com/Tsinghua-MARS-Lab/Online-HD-Map-Construction-CVPR2023/blob/master/resources/docs/data.md">data</a>
        and <a
          href="https://github.com/Tsinghua-MARS-Lab/Online-HD-Map-Construction-CVPR2023/blob/master/resources/docs/get_started.md#evaluation">evaluation</a>.
        Submission is conducted on <a href="https://eval.ai/web/challenges/challenge-page/1954/overview">EvalAI</a>.
      </p>



      <br>



      <!-- <h4>
        <b>Important dates</b>
      </h4>

      <table class="itable">
        <tbody>
          <tr >
            <td>Challenge Period Open</td><td>March 15, 2023</td>
          </tr>
          <tr >
            <td>Challenge Period End</td><td>May 26, 2023</td>
          </tr>
          <tr >
            <td>Notification</td><td>May 27, 2023</td>
          </tr>
          <tr >
            <td>Technical Report Deadline</td><td>June 05, 2023</td>
          </tr>
          <tr >
            <td>Winner Announcement</td><td>June 11, 2023</td>
          </tr>
        </tbody>
      </table>



      <br> -->






      <h4>
        <b>Award</b>
      </h4>

      <table class="itable">
        <tbody>
          <tr>
            <td><img src='/style/cvpr2023/img/rank01.png' style="width: 20px; user-select: none;" /> Outstanding
              Champion</td>
            <td>USD $15,000</td>
          </tr>
          <tr>
            <td><img src='/style/cvpr2023/img/rank02.png' style="width: 20px; user-select: none;" /> Honorable Runner-up
            </td>
            <td>USD $5,000</td>
          </tr>
          <tr>
            <td><img src='/style/cvpr2023/img/blomb1.png' style="width: 20px; user-select: none;" /> Innovation Award
            </td>
            <td>USD $5,000</td>
          </tr>
        </tbody>
      </table>



      <br>



      <h4>
        <b>Contact</b>
      </h4>

      <li style="list-style-type:disc">
        Tianyuan Yuan (MARS Lab), <code>yuantianyuan01@gmail.com</code>
      </li>
      <li style="list-style-type:disc">
        Slack channel: <code>#map-challenge-2023</code>
      </li>



      <br><br>



      <h4>
        <b>Related Literature</b>
      </h4>

      <li style="list-style-type:disc">
        <a href="https://arxiv.org/pdf/2107.06307.pdf">
          HDMapNet: An Online HD Map Construction and Evaluation Framework
        </a>
      </li>
      <li style="list-style-type:disc">
        <a href="https://arxiv.org/pdf/2206.08920.pdf">
          VectorMapNet: End-to-end Vectorized HD Map Learning
        </a>
      </li>
      <li style="list-style-type:disc">
        <a href="https://arxiv.org/pdf/2301.04470.pdf">
          InstaGraM: Instance-level Graph Modeling for Vectorized HD Map Learning
        </a>
      </li>



      <br>
      <br>
      <br>


    </div>
  </div>



  <!-- track 3 -->

  <div class="bg" id="3d_occupancy_prediction">
    <br>
    <br>
    <br>

    <h3>
      <b class="title" target="_blank" id="Track3">
        <a href="#3d_occupancy_prediction">
          Track 3
          <br>
          3D Occupancy Prediction
          <img src="/style/img/icon/link.png" class="title_link" />
        </a>

      </b>
    </h3>



    <a href="https://github.com/CVPR2023-Occupancy-Prediction-Challenge/CVPR2023-Occupancy-Prediction-Challenge">
      <img src="https://img.shields.io/badge/website-GitHub-darkred" alt="GitHub" class="h_img" />
    </a>

    <a href="https://github.com/CVPR2023-Occupancy-Prediction-Challenge/CVPR2023-Occupancy-Prediction-Challenge">
      <img
        src="https://img.shields.io/github/stars/CVPR2023-Occupancy-Prediction-Challenge/CVPR2023-Occupancy-Prediction-Challenge?style=social"
        alt="GitHub" class="h_img" />
    </a>

    <a href="https://github.com/CVPR2023-Occupancy-Prediction-Challenge/CVPR2023-Occupancy-Prediction-Challenge">
      <img
        src="https://img.shields.io/github/forks/CVPR2023-Occupancy-Prediction-Challenge/CVPR2023-Occupancy-Prediction-Challenge?style=social"
        alt="GitHub" class="h_img" />
    </a>

    <a href="https://eval.ai/web/challenges/challenge-page/2045/overview">
      <img src="https://img.shields.io/badge/submission-EvalAI-darkred" alt="EvalAI" class="h_img" />
    </a>


    <br>
    <br>
    <p>
      <div class="innovation">
        We are happy to announce the <b class="blue_emphasize">Largest 3D Occupancy Prediction Benchmark</b> in autonomous driving. Check out our <a href="https://github.com/OpenDriveLab/OpenScene" target="_blank">GitHub repository</a> for more details on the dataset, leaderboard, and upcoming challenge in 2024.
        <br><br>
        <div style="display: flex; text-align: center;">
          <img src='/style/cvpr2023/img/openscene.jpg' class="imghalf"/>
          <img src='/style/cvpr2023/img/allinone.jpg'  class="imghalf" />
        </div>
      </div>
    </p>
    <br>


    <h4>
      <b style="text-transform: none;">Leaderboard (Server remains active)</b>
    </h4>

    <p>
      <li style="list-style-type:disc;"># Participating Teams: 149</li>
      <li style="list-style-type:disc;"># Countries and Regions: 10</li>
      <li style="list-style-type:disc;"># Submissions: 400+</li>
      <!-- In the 3D Occupancy Prediction Challenge, 149 teams from 10 different countries and regions have made more than 400 submissions. -->
      <!-- The intensive competition is depicted by a variance smaller than 10 for teams in the middle, and the winning margin is much smaller compared to other tracks. -->
      This track featured one of the most fiercely contested tracks, with almost 150 participating teams. The difference
      in scores between the top 20 teams was less than 10 points.
    </p>

    <div class="scrollable">
      <table style="text-align: center;" class="table_list_3">
        <thead>
          <tr style="width: 100%;">
            <th style="width: 5%; min-width: 75px;">Rank</th>
            <th style="width: 5%; min-width: 100px;">Country / Region</th>
            <th style="width: 20%;">Institution</th>
            <th style="width: 10%; min-width: 175px;"><b>mIoU (primary)</b></th>
            <th style="width: 20%;min-width: 300px;">Team Name</th>
          </tr>
        </thead>
        <tbody>
          <tr class="page_3">
            <td colspan="2"><button id="prev_3">Previous</button></td>
            <td colspan="1"></td>
            <td colspan="1"><button id="next_3">Next</button></td>
            <td colspan="1">
              Page <input type="text" id="curpage_3" value="1"> of <span id="total_3">30</span>
              &nbsp;&nbsp;&nbsp;
              <select name="" id="pageOption_3">
                <option value="10">10rows</option>
                <option value="20">20rows</option>
                <!-- <option value="25">25rows</option> -->
                <!-- <option value="50">50rows</option>
                  <option value="100">100rows</option> -->
              </select>
            </td>
          </tr>
        </tbody>
      </table>

    </div>

    <p>
      <img src='/style/cvpr2023/img/blomb1.png' style="width: 20px; user-select: none;" /> <b
        class="blue_emphasize">Innovation Award goes to "NVOCC" for</b><br>
    <div class="innovation">This innovation of team NVOCC deviates from the conventional 3D-2D or 2D-3D priors and
      offers fresh insights into the development of view transformation modules. The FB-OCC method demonstrates
      substantially improved performance in comparison to prior approaches.</div>
    </p>
    <p>
      <img src='/style/cvpr2023/img/blomb1.png' style="width: 20px; user-select: none;" /> <b
        class="blue_emphasize">Innovation Award goes to "occ_transformer" for</b><br>
    <div class="innovation">The present innovation by the team occ_transformer sheds light on the disparities between
      the detection model and the OCC model, and introduces an initial strategy for transforming bounding boxes (bbox)
      into OCC representations.</div>
    </p>

    <div class="container tm-container-gallery">
      <div class="row">
        <div class="col-2">
        </div>
        <div class=" col-6">
          <p>If you use the challenge dataset in your paper, please consider citing the following BibTex:</p>


          <div
            style="background-color: rgba(242, 242, 242,1); padding-top: 10px; padding-bottom: 10px; border-radius: 10px; white-space: nowrap; overflow-x: scroll;">
            &nbsp;&nbsp;&nbsp;@article{sima2023_occnet,
            <br>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;title={Scene as Occupancy},
            <br>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;author={Chonghao Sima and Wenwen Tong and Tai Wang and Li Chen and Silei Wu and Hanming Deng  and Yi Gu and Lewei Lu and Ping Luo and Dahua Lin and Hongyang Li},
            <br>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;year={2023},
            <br>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;eprint={2306.02851},
            <br>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;archivePrefix={arXiv},
            <br>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;primaryClass={cs.CV},
            <br>
            &nbsp;&nbsp;&nbsp;}
          </div>
        </div>
        <div class="col-">
        </div>

      </div>

    </div>

    <br>

    <div class="container tm-container-gallery">
      <div class="row">
        <div class="col-2">
        </div>
        <div class=" col-6">


          <div
            style="background-color: rgba(242, 242, 242,1); padding-top: 10px; padding-bottom: 10px; border-radius: 10px; white-space: nowrap; overflow-x: scroll;">
            &nbsp;&nbsp;&nbsp;@article{tian2023occ3d,
            <br>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;title={Occ3D: A Large-Scale 3D Occupancy Prediction Benchmark for
            Autonomous Driving},
            <br>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;author={Tian, Xiaoyu and Jiang, Tao and Yun, Longfei and Wang, Yue and
            Wang, Yilun and Zhao, Hang},
            <br>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;journal={arXiv preprint arXiv:2304.14365},
            <br>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;year={2023}
            <br>
            &nbsp;&nbsp;&nbsp;}
          </div>
        </div>
        <div class="col-">
        </div>

      </div>

    </div>
    <br>


    <!-- <div class="scrollable">
        <table style="text-align: center;">
          <thead>
            <tr>
              <th style="background-color: rgb(139, 14, 14);">Rank</th>
              <th style="background-color: rgb(139, 14, 14);">Method</th>
              <th style="background-color: rgb(139, 14, 14);"><b>mIoU <i>(primary metric)</i></b></th>
              <th style="background-color: rgb(139, 14, 14);">$\text{F-Score**}$</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>-</td>
              <td>-</td>
              <td>-</td>
              <td>-</td>
            </tr>
          </tbody>
        </table>
      </div> -->

    <!-- <p class="footer_ts">
        * &nbsp; For the real-time leaderboard please refer to <a href="https://eval.ai/web/challenges/challenge-page/2045/leaderboard">EvalAI</a>.
        <br>
        ** $\text{F-Score}$ is not taken into consideration in both the challenge and leaderboard.
      </p> -->
    <!-- <img src="/style/img/events/challenge_2023/leaderboard_track_3.jpg" class="leaderboard"> -->
    <!-- <p>
        Above is the final leaderboard. A notification was sent from <a href="javaScript:">3d-occupancy-prediction-{1,2,3}@googlegroups.com</a> (please ignore duplicate emails). If you did not receive the notification, please specify your email and team name to <a href="mailto:wanghuijie@pjlab.org.cn">wanghuijie@pjlab.org.cn</a>.
      </p> -->
    <!-- <p class="footer_ts">
        * Under review.
      </p> -->



    <br>



    <h4>
      <b>Task Description</b>
    </h4>

    <p>
      Unlike previous perception representations, which depend on predefined geometric primitives or perceived data
      modalities,
      occupancy enjoys the flexibility to describe entities in arbitrary shapes.
      In this track, we provide a large-scale <a
        href="https://github.com/CVPR2023-Occupancy-Prediction-Challenge/CVPR2023-Occupancy-Prediction-Challenge">occupancy
        benchmark</a>.
      Given multi-view images covering the whole panoramic field of view,
      participants are needed to provide the occupancy state and semantics of each voxel in 3D space for the complete
      scene.
    </p>



    <br>



    <h4>
      <b>Participation</b>
    </h4>

    <p>
      The primary metric of this track is mIoU.
      On the <a
        href="https://github.com/CVPR2023-Occupancy-Prediction-Challenge/CVPR2023-Occupancy-Prediction-Challenge">website</a>,
      we provide detailed information for the dataset, evaluation, and submission instructions.
      The test server is hosted on <a href="https://eval.ai/web/challenges/challenge-page/2045/overview">EvalAI</a>.
    </p>



    <br>



    <!-- <h4>
        <b>Important dates</b>
      </h4>

      <table class="itable">
        <tbody>
          <tr >
            <td>Dataset and Devkit Release</td><td>February 20, 2023</td>
          </tr>
          <tr >
            <td>Challenge Period Open</td><td>Opened</td>
          </tr>
          <tr >
            <td>Challenge Period End</td><td>June 01, 2023</td>
          </tr>
          <tr >
            <td>Notification</td><td>June 03, 2023</td>
          </tr>
          <tr >
            <td>Technical Report Deadline</td><td>June 10, 2023</td>
          </tr>
          <tr >
            <td>Winner Announcement</td><td>June 12, 2023</td>
          </tr>
        </tbody>
      </table>

      

      <br> -->








    <h4>
      <b>Award</b>
    </h4>

    <table class="itable">
      <tbody>
        <tr>
          <td><img src='/style/cvpr2023/img/rank01.png' style="width: 20px; user-select: none;" /> Outstanding Champion
          </td>
          <td>USD $15,000</td>
        </tr>
        <tr>
          <td><img src='/style/cvpr2023/img/rank02.png' style="width: 20px; user-select: none;" /> Honorable Runner-up
          </td>
          <td>USD $5,000</td>
        </tr>
        <tr>
          <td><img src='/style/cvpr2023/img/blomb1.png' style="width: 20px; user-select: none;" /> Innovation Award * 2
          </td>
          <td>USD $5,000</td>
        </tr>
      </tbody>
    </table>



    <br>



    <h4>
      <b>Contact</b>
    </h4>
    <li style="list-style-type:disc">
      Chonghao Sima (OpenDriveLab), <code>chonghaosima@gmail.com</code>
    </li>
    <li style="list-style-type:disc">
      Xiaoyu Tian (MARS Lab), <code>cntxy001@gmail.com</code>
    </li>
    <li style="list-style-type:disc">
      Slack channel: <code>#occupancy-challenge-2023</code>
    </li>



    <br><br>



    <h4>
      <b>Related Literature</b>
    </h4>

    <li style="list-style-type:disc;">
      <a href="https://arxiv.org/pdf/2306.02851.pdf">
        Scene as Occupancy
      </a>
    </li>
    <li style="list-style-type:disc">
      <a href="https://arxiv.org/pdf/2003.04618.pdf">
        Convolutional Occupancy Networks
      </a>
    </li>
    <li style="list-style-type:disc">
      <a href="https://arxiv.org/pdf/2203.03875.pdf">
        Occupancy Flow Fields for Motion Forecasting in Autonomous Driving
      </a>
    </li>
    <li style="list-style-type:disc">
      <a
        href="https://openaccess.thecvf.com/content/CVPR2022/papers/Cao_MonoScene_Monocular_3D_Semantic_Scene_Completion_CVPR_2022_paper.pdf">
        MonoScene: Monocular 3D Semantic Scene Completion
      </a>
    </li>
    <li style="list-style-type:disc">
      <a href="https://arxiv.org/pdf/2301.00527.pdf">
        Diffusion Probabilistic Models for Scene-Scale 3D Categorical Data
      </a>
    </li>


    <br>
    <br>
    <br>
  </div>



  <!-- track 4 -->

  <div class="bg_huise">
    <br>
    <br>
    <br>
    <div class="bg" id="nuplan_planning">

      <h3><b class="title" target="_blank" id="Track4" style="text-transform: none;">
          <a href="#nuplan_planning">
            Track 4
            <br>
            nuPlan Planning
            <img src="/style/img/icon/link.png" class="title_link" />
          </a>

        </b></h3>

      <a href="https://github.com/motional/nuplan-devkit">
        <img src="https://img.shields.io/badge/website-GitHub-darkblue" alt="GitHub" class="h_img" />
      </a>

      <a href="https://github.com/motional/nuplan-devkit">
        <img src="https://img.shields.io/github/stars/motional/nuplan-devkit?style=social" alt="GitHub" class="h_img" />
      </a>

      <a href="https://github.com/motional/nuplan-devkit">
        <img src="https://img.shields.io/github/forks/motional/nuplan-devkit?style=social" alt="GitHub" class="h_img" />
      </a>

      <a href="https://eval.ai/web/challenges/challenge-page/1856/overview">
        <img src="https://img.shields.io/badge/submission-EvalAI-darkblue" alt="EvalAI" class="h_img" />
      </a>



      <br>
      <br>
      <br>

      <h4>
        <b>Leaderboard</b>
      </h4>

      <p>
        <li style="list-style-type:disc;"># Participating Teams: 52</li>
        <li style="list-style-type:disc;"># Countries and Regions: 11</li>
        <li style="list-style-type:disc;"># Submissions: 600+</li>
        <!-- In the nuPlan Planning Challenge, 52 teams from 11 different countries and regions have made more than 600 submissions. -->
        <!-- The variance of around 4 and small gaps of scores for the top three teams witness the competition for the champion. -->
        This track was characterized by the greatest diversity among this challenge, with participation from teams
        representing up to 11 countries and regions. The difference in scores between the top 5 teams was less than 10
        points.
      </p>

      <div class="scrollable">
        <table style="text-align: center;" class="table_list_4">
          <thead>
            <tr style="width: 100%;">
              <th style="width: 5%; min-width: 75px;">Rank</th>
              <th style="width: 5%; min-width: 100px;">Country / Region</th>
              <th style="width: 20%; ">Institution</th>
              <th style="width: 10%; min-width: 175px;"><b>Overall Score (primary)</b></th>
              <th style="width: 20%; ">Team Name</th>
              <th style="width: 10%;"><b>CH1 Score</b></th>
              <th style="width: 10%;"><b>CH2 Score</b></th>
              <th style="width: 10%;"><b>CH3 Score</b></th>
            </tr>
          </thead>
          <tbody>
            <tr class="page_4">
              <td colspan="2"><button id="prev_4">Previous</button></td>
              <td colspan="1"></td>
              <td colspan="1"><button id="next_4">Next</button></td>
              <td colspan="4">
                Page <input type="text" id="curpage_4" value="1"> of <span id="total_4">30</span>
                &nbsp;&nbsp;&nbsp;
                <select name="" id="pageOption_4">
                  <option value="10">10rows</option>
                  <option value="20">20rows</option>
                  <!-- <option value="25">25rows</option> -->
                  <!-- <option value="50">50rows</option>
                  <option value="100">100rows</option> -->
                </select>
              </td>
            </tr>
          </tbody>
        </table>

      </div>

      <p>
        <img src='/style/cvpr2023/img/blomb1.png' style="width: 20px; user-select: none;" /> <b
          class="blue_emphasize">Innovation Award goes to "AID" for</b><br>
      <div class="innovation">GameFormer is a Transformer-based model that utilizes hierarchical game theory for
        interactive prediction and planning. The approach incorporates novel level-k decoders in the prediction model
        that iteratively refine the future trajectories of interacting agents, as well as a learning process that
        regulates the predicted behaviors of agents given the prediction results.</div>
      </p>
      <p>
        <img src='/style/cvpr2023/img/icons8-feather-emoji-96.png' style="width: 20px; user-select: none;" /> <b
          class="blue_emphasize">Honorable Mention for Innovation Award goes to "raphamas" for</b><br>
      <div class="innovation">MBAPPE leverages an MCTS on a partially learned environment of nuPlan. This method infers
        trajectories by integrating consecutive actions maximizing the cumulative reward, measured through exploration
        and evaluation within MBAPPE’s internal simulation. Decisions and choices of MBAPPE are explainable, reliable
        and reproductible as we have access to each step of the internal thought process.</div>
      </p>

      <!-- <p>
        <img src='/style/cvpr2023/img/blomb1.png' style="width: 15px; user-select: none;"/> 
        <span></span>
      </p> -->

      <div class="container tm-container-gallery">
        <div class="row">
          <div class="col-2">
          </div>
          <div class=" col-6">
            <p>If you use the challenge dataset in your paper, please consider citing the following BibTex:</p>


            <div
              style="background-color: rgb(230, 230, 230); padding-top: 10px; padding-bottom: 10px; border-radius: 10px; white-space: nowrap; overflow-x: scroll;">
              &nbsp;&nbsp;&nbsp;@INPROCEEDINGS{nuplan,
              <br>
              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;title={NuPlan: A closed-loop ML-based planning benchmark for
              autonomous vehicles},
              <br>
              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;author={H. Caesar, J. Kabzan, K. Tan et al.,},
              <br>
              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;booktitle={CVPR ADP3 workshop},
              <br>
              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;year=2021
              <br>
              &nbsp;&nbsp;&nbsp;}
            </div>
          </div>
          <div class="col-">
          </div>

        </div>

      </div>
      <br>


      <!-- <div class="scrollable">
        <table style="text-align: center;">
          <thead>
            <tr>
              <th style="background-color: rgb(25, 25, 145);">Rank</th>
              <th style="background-color: rgb(25, 25, 145);">Team</th>
              <th style="background-color: rgb(25, 25, 145);"><b>mean score <i>(primary metric)</i></b></th>
              <th style="background-color: rgb(25, 25, 145);">Challenge 1</th>
              <th style="background-color: rgb(25, 25, 145);">Challenge 2</th>
              <th style="background-color: rgb(25, 25, 145);">Challenge 3</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>1</td>
              <td>Host_68305_Team (Baseline)</td>
              <td>0.15</td>
              <td>0.15</td>
              <td>0.39</td>
              <td>0.61</td>
            </tr>
          </tbody>
        </table>
      </div> -->

      <!-- <p class="footer_ts">
        * For the real-time leaderboard please refer to <a href="https://eval.ai/web/challenges/challenge-page/1856/leaderboard">EvalAI</a>.
      </p> -->
      <!-- <img src="/style/img/events/challenge_2023/leaderboard_track_4.png" class="leaderboard"> -->
      <!-- <p>
        Above is the final leaderboard. Notifications are sent to prize winners from <a href="mailto:pat.karnchanachari@motional.com">pat.karnchanachari@motional.com</a>.
      </p> -->

      <br>



      <h4>
        <b>Task Description</b>
      </h4>
      <p>
        Previous benchmarks focus on short-term motion forecasting and are limited to open-loop evaluation.
        <a href="https://www.nuscenes.org/nuplan">nuPlan</a> introduces long-term planning of the ego vehicle and
        corresponding metrics.
        Provided as docker containers, submissions are deployed for simulation and evaluation.
      </p>



      <br>



      <h4>
        <b>Participation</b>
      </h4>
      <p>
        The primary metric is the mean score over three increasingly complex modes:
        <a href="https://nuplan-devkit.readthedocs.io/en/latest/competition.html#challenge-1-open-loop">open-loop</a>,
        <a
          href="https://nuplan-devkit.readthedocs.io/en/latest/competition.html#challenge-2-closed-loop-non-reactive-agents">closed-loop
          non-reactive agents</a>,
        and <a
          href="https://nuplan-devkit.readthedocs.io/en/latest/competition.html#challenge-3-closed-loop-reactive-agents">closed-loop
          reactive agents</a>.
        Participants can follow the <a
          href="https://nuplan-devkit.readthedocs.io/en/latest/competition.html#getting-started">steps</a> to begin the
        competition.
        To submit your results on <a href="https://eval.ai/web/challenges/challenge-page/1856/overview">EvalAI</a>,
        please
        follow the <a
          href="https://nuplan-devkit.readthedocs.io/en/latest/competition.html#making-a-submission">submission
          instructions</a>.
      </p>



      <br>



      <!-- <h4>
        <b>Important dates</b>
      </h4>

      <table class="itable">
        <tbody>
          <tr >
            <td>Test Phase End</td><td>May 26, 2023</td>
          </tr> -->
      <!-- <tr >
            <td>Notification and Verification</td><td>May 19, 2023</td>
          </tr> -->
      <!-- <tr >
            <td>Winner Announcement</td><td>June 02, 2023</td>
          </tr>
          <tr >
            <td>Technical Report Deadline</td><td>June 09, 2023</td>
          </tr>
          <tr >
            <td>Winner Presentation</td><td>June 18, 2023</td>
          </tr>
        </tbody>
      </table>



      <br> -->







      <h4><b>Award</b></h4>

      <table class="itable">
        <tbody>
          <tr>
            <td><img src='/style/cvpr2023/img/rank01.png' style="width: 20px; user-select: none;" /> Outstanding
              Champion</td>
            <td>USD $10,000</td>
          </tr>
          <tr>
            <td><img src='/style/cvpr2023/img/rank02.png' style="width: 20px; user-select: none;" /> Honorable Runner-up
              (2nd)</td>
            <td>USD $8,000</td>
          </tr>
          <tr>
            <td><img src='/style/cvpr2023/img/rank03.png' style="width: 20px; user-select: none;" /> Honorable Runner-up
              (3rd)</td>
            <td>USD $5,000</td>
          </tr>
          <tr>
            <td><img src='/style/cvpr2023/img/blomb1.png' style="width: 20px; user-select: none;" /> Innovation Award
            </td>
            <td>USD $5,000</td>
          </tr>
        </tbody>
      </table>



      <br>



      <h4>
        <b>Contact</b>
      </h4>

      <li style="list-style-type:disc">
        <a href="https://github.com/motional/nuplan-devkit/issues">GitHub issue</a>
      </li>
      <li style="list-style-type:disc">
        Motional, <code>nuScenes@motional.com</code>
      </li>
      <li style="list-style-type:disc">
        Slack channel: <code>#nuplan-challenge-2023</code>
      </li>



      <br><br>



      <h4>
        <b>Related Literature</b>
      </h4>

      <li style="list-style-type:disc">
        <a href="https://arxiv.org/pdf/2206.03004.pdf">
          Driving in Real Life with Inverse Reinforcement Learning
        </a>
      </li>
      <li style="list-style-type:disc">
        <a
          href="https://openaccess.thecvf.com/content/CVPR2022W/Precognition/papers/Hazard_Importance_Is_in_Your_Attention_Agent_Importance_Prediction_for_Autonomous_CVPRW_2022_paper.pdf">
          Importance Is in Your Attention: Agent Importance Prediction for Autonomous Driving
        </a>
      </li>




      <br>
      <br>
      <br>



    </div>
  </div>





  <div class="bg" id="general_rules">
    <br>
    <br>
    <br>

    <h3>
      <b class="title" target="_blank" id="rules"><a href="#general_rules">General Rules<img
            src="/style/img/icon/link.png" class="title_link" /></a></b>
    </h3>

    <h4 style="text-transform: none;">
      <b>Note Regarding Certificate (June 25, 2023)</b>
    </h4>
    <p>
      Thanks for your participation!
      For those who require a certificate for participation, please specify the names of all team members, the
      institution, the method name (optional), the team name, and the participating track to <a
        href="mailto:wanghuijie@pjlab.org.cn">wanghuijie@pjlab.org.cn</a>.
    </p>

    <br>

    <h4 style="text-transform: none;">
      <b>Note Regarding Submission (May 24, 2023)</b>
    </h4>
    <p>
      Only <b>PUBLIC</b> results shown on the leaderboard will be valid.
      Please ensure your result is made public before the deadline and kept public after the deadline on the
      leaderboard.
    </p>

    <br>

    <h4 style="text-transform: none;">
      <b>Statement Regarding Submission Information (May 15, 2023)</b>
    </h4>
    <p>
      Regarding submissions to all tracks, please make sure the appended information is correct, especially the email
      address.
      After submission deadlines, we will ask participants <b>via email</b> to provide further information for
      qualification and making certificates.
      Any form of late requests for claiming ownership of a particular submission will not be considered.
      Incorrect email addresses will lead to <b>disqualification</b>.
    </p>
    <br>

    <h4 style="text-transform: none;">
      <b>Statement Regarding Leaderboard and Award (April 14, 2023)</b>
    </h4>

    <p>
      The primary objective of this Autonomous Driving Challenge is to facilitate all aspects of autonomous driving.
      Despite the current trend toward data-driven research, we strive to provide opportunities for participants without
      access to massive data or computing resources.
      To this end, we would like to reiterate the following rules:

      <br><br>

      <b>Leaderboard</b>
      <br>
      Certificates will be provided to <b>all participants</b>.
      <br>
      All publicly available datasets and pretrained weights are allowed, including Objects365, Swin-T, DD3D-pretrained
      VoVNet, InternImage, etc.
      <br>
      But the use of private datasets or pretrained weights is prohibited.

      <br><br>

      <b>Award</b>
      <br>
      To claim a cash award, all participants are required to submit a <b>technical report</b>.
      <br>
      Cash awards for the <b>first three places</b> will be distributed based on the rankings on leaderboards. However,
      other factors, such as model sizes and data usage, will be taken into consideration.
      <br>
      As we set up the <b>Innovation Prize</b> to encourage novel and innovative ideas, winners of this award are
      encouraged only to use ImageNet and COCO as external data.

      <br><br>

      The challenge committee reserves all rights for the final explanation of the cash award.


    </p>



    <br>



    <h4>
      <b>Rules</b>
    </h4>

    <p>
      Please refer to <a
        href="https://docs.google.com/document/d/18wEN6XdSW2VpVgSo0hHEKPN6KPptqvOUv5Znc2_lB3o/edit?usp=sharing">rules</a>.
    </p>

    <br>
    <br>
    <br>

  </div>

  <div class="bg_huise">
    <br>
    <br>
    <br>
    <div class="bg" id="faq">

      <h3>
        <b class="title" target="_blank" id="FAQ"><a href="#faq">FAQ<img src="/style/img/icon/link.png"
              class="title_link" /></a></b>
      </h3>
      <br>

      <div class="accordion-body">
        <div class="accordion">

          <hr>

          <div class="container">
            <div class="label"><b>How do we/I download the data?</b></div>
            <div class="content">For each track, we provide links for downloading data in the GitHub repository.
              The repository, which might also contain dataset API, baseline model, and other helpful information, is a
              good start to begin your participation.
            </div>
          </div>

          <hr>

          <div class="container">
            <div class="label"><b>How many times can we/I make a submission?</b></div>
            <div class="content">
              Each track has its submission limit. Please refer to the EvalAI for each track.
              Submissions that error out do not count against this limit.
            </div>
          </div>

          <hr>

          <div class="container">
            <div class="label"><b>How many tracks can we/I take part in?</b></div>
            <div class="content">
              A team can participate in multiple tracks.
              An entity cannot be affiliated with more than one team unless the entity is an academic entity (e.g., a
              university).
            </div>
          </div>

          <hr>

          <div class="container">
            <div class="label"><b>Should we/I use future frames during inference?</b></div>
            <div class="content">
              No future frame is allowed except that it is noted explicitly.
            </div>
          </div>

          <hr>

        </div>
      </div>
    </div>
    <br>
    <br>
    <br>
  </div>



  <div class="bg" style="display: none;">
    <div class="wrapper row3">
      <div class="worldmap">
        <!-- <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=W01goUbw9Xtnjh8gWPvnFjg9_mxO7w6jeXoD72spiX8"></script> -->
        <!-- <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=n&d=W01goUbw9Xtnjh8gWPvnFjg9_mxO7w6jeXoD72spiX8&cmn=ff0000&cmo=ff9953'></script>       -->
        <script type='text/javascript' id='clustrmaps'
          src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=tt&d=W01goUbw9Xtnjh8gWPvnFjg9_mxO7w6jeXoD72spiX8&cmn=ff0000&cmo=ff955c'></script>
      </div>
    </div>
    <div class="clear"></div>
  </div>

  <div class="wrapper row2" id="tttttt" style="display: none;"></div>




  <script src="/style/cvpr2023/js/jquery-3.1.1.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/style/cvpr2023/js/all.js" type="text/javascript" charset="utf-8"></script>
  <script src="/style/cvpr2023/js/track1.js" type="text/javascript" charset="utf-8"></script>
  <script src="/style/cvpr2023/js/track2.js" type="text/javascript" charset="utf-8"></script>
  <script src="/style/cvpr2023/js/track3.js" type="text/javascript" charset="utf-8"></script>
  <script src="/style/cvpr2023/js/track4.js" type="text/javascript" charset="utf-8"></script>
  <script src="/style/js/top_event.js?111" type="text/javascript" charset="utf-8"></script>
  <script src="/style/js/mailing_list.js" type="text/javascript" charset="utf-8"></script>


</body>

</html>
