const ad = [
    {
        title: "MTGS: Multi-Traversal Gaussian Splatting",
        link: "https://arxiv.org/abs/2503.12552",
        image: "/assets/publication/mtgs.jpg",
        author: "Tianyu Li, Yihang Qiu, Zhenhua Wu, Carl Lindström, Peng Su, Matthias Nießner, Hongyang Li",
        note: "arXiv 2025",
        noteoption: '',
        star: "",
        starlink: "",
        icon: [
        ],
        description: "",
        tag: "",
    },
    {
        title: "LaneSegNet: Map Learning with Lane Segment Perception for Autonomous Driving",
        link: "https://arxiv.org/abs/2312.16108",
        image: "/assets/publication/lanesegnet.jpg",
        author: "Tianyu Li, Peijin Jia, Bangjun Wang, Li Chen, Kun Jiang, Junchi Yan, Hongyang Li",
        note: "ICLR 2024",
        noteoption: '',
        star: "https://img.shields.io/github/stars/opendrivelab/LaneSegNet?style=social",
        starlink: "https://github.com/OpenDriveLab/LaneSegNet",
        icon: [
            {
                type: "github",
                link: "https://github.com/OpenDriveLab/LaneSegNet",
            },
        ],
        description: "We advocate Lane Segment as a map learning paradigm that seamlessly incorporates both map geometry and topology information.",
        tag: "",
    },
    {
        title: "Fully Sparse 3D Occupancy Prediction",
        link: "https://arxiv.org/abs/2312.17118",
        image: "/assets/publication/sparseocc.jpg",
        author: "Haisong Liu, Yang Chen, Haiguang Wang, Zetong Yang, Tianyu Li, Jia Zeng, Li Chen, Hongyang Li, Limin Wang",
        note: "ECCV 2024",
        noteoption: '',
        star: "https://img.shields.io/github/stars/MCG-NJU/SparseOcc?style=social",
        starlink: "https://github.com/MCG-NJU/SparseOcc",
        icon: [
            {
                type: "github",
                link: "https://github.com/MCG-NJU/SparseOcc",
            },
        ],
        description: "",
        tag: "",
    },
    {
        title: "Leveraging Vision-Centric Multi-Modal Expertise for 3D Object Detection",
        link: "https://arxiv.org/abs/2310.15670",
        image: "/assets/publication/vcd.jpg",
        author: "Linyan Huang, Zhiqi Li, Chonghao Sima, Wenhai Wang, Jingdong Wang, Yu Qiao, Hongyang Li",
        note: "NeurIPS 2023",
        noteoption: '',
        star: "https://img.shields.io/github/stars/opendrivelab/Birds-eye-view-Perception?style=social",
        starlink: "https://github.com/OpenDriveLab/Birds-eye-view-Perception",
        icon: [
            {
                type: "github",
                link: "https://github.com/OpenDriveLab/Birds-eye-view-Perception",
            },
        ],
        description: "The unified framework to enhance 3D object detection by uniting a multi-modal expert model with a trajectory distillation module.",
        tag: "",
    },
    {
        title: "OpenLane-V2: A Topology Reasoning Benchmark for Unified 3D HD Mapping",
        link: "https://proceedings.neurips.cc/paper_files/paper/2023/hash/3c0a4c8c236144f1b99b7e1531debe9c-Abstract-Datasets_and_Benchmarks.html",
        image: "/assets/publication/openlanev2.jpg",
        author: "Huijie Wang, Tianyu Li, Yang Li, Li Chen, Chonghao Sima, Zhenbo Liu, Bangjun Wang, Peijin Jia, Yuting Wang, Shengyin Jiang, Feng Wen, Hang Xu, Ping Luo, Junchi Yan, Wei Zhang, Hongyang Li",
        note: "NeurIPS 2023 Track Datasets and Benchmarks",
        noteoption: '',
        star: "https://img.shields.io/github/stars/opendrivelab/OpenLane-V2?style=social",
        starlink: "https://github.com/OpenDriveLab/OpenLane-V2",
        icon: [
            {
                type: "github",
                link: "https://github.com/OpenDriveLab/OpenLane-V2",
            },
            {
                type: "dataset",
                link: "https://github.com/OpenDriveLab/OpenLane-V2",
            },
        ],
        description: "The world's first perception and reasoning benchmark for scene structure in autonomous driving.",
        tag: "",
    },
    {
        title: "Delving into the Devils of Bird's-Eye-View Perception: A Review, Evaluation and Recipe",
        link: "https://ieeexplore.ieee.org/document/10321736",
        image: "/assets/publication/bevsurvey.jpg",
        author: "Hongyang Li, Chonghao Sima, Jifeng Dai, Wenhai Wang, Lewei Lu, Huijie Wang, Jia Zeng, Zhiqi Li, Jiazhi Yang, Hanming Deng, Hao Tian, Enze Xie, Jiangwei Xie, Li Chen, Tianyu Li, Yang Li, Yulu Gao, Xiaosong Jia, Si Liu, Jianping Shi, Dahua Lin, Yu Qiao",
        note: "TPAMI 2023",
        noteoption: '',
        star: "https://img.shields.io/github/stars/opendrivelab/bevperception-survey-recipe?style=social",
        starlink: "https://github.com/opendrivelab/bevperception-survey-recipe",
        icon: [
            {
                type: "github",
                link: "https://github.com/opendrivelab/bevperception-survey-recipe",
            },
        ],
        description: "We review the most recent work on BEV perception and provide analysis of different solutions.",
        tag: ["[Setup the Table]"],
    },
    {
        title: "Scene as Occupancy",
        link: "https://arxiv.org/abs/2306.02851",
        image: "/assets/publication/occnet.jpg",
        author: "Chonghao Sima, Wenwen Tong, Tai Wang, Li Chen, Silei Wu, Hanming Deng, Yi Gu, Lewei Lu, Ping Luo, Dahua Lin, Hongyang Li",
        note: "ICCV 2023",
        noteoption: '',
        star: "https://img.shields.io/github/stars/opendrivelab/OccNet?style=social",
        starlink: "https://github.com/OpenDriveLab/OccNet",
        icon: [
            {
                type: "github",
                link: "https://github.com/OpenDriveLab/OccNet",
            },
        ],
        description: "Occupancy serves as a general representation of the scene and could facilitate perception and planning in the full-stack of autonomous driving.",
        tag: "",
    },
    {
        title: "Sparse Dense Fusion for 3D Object Detection",
        link: "https://arxiv.org/abs/2304.04179",
        image: "/assets/publication/sparse_yulu.jpg",
        author: "Yulu Gao, Chonghao Sima, Shaoshuai Shi, Shangzhe Di, Si Liu, Hongyang Li",
        note: "IROS 2023",
        noteoption: '',
        star: "",
        starlink: "",
        icon: [
            {
                type: "github",
                link: "",
            },
        ],
        description: "We propose Sparse Dense Fusion (SDF), a complementary framework that incorporates both sparse-fusion and dense-fusion modules via the Transformer architecture.",
        tag: "",
    },
    {
        title: "HDGT: Heterogeneous Driving Graph Transformer for Multi-Agent Trajectory Prediction via Scene Encoding",
        link: "https://arxiv.org/abs/2205.09753",
        image: "/assets/publication/hdgt.jpg",
        author: "Xiaosong Jia, Penghao Wu, Li Chen, Hongyang Li, Yu Liu, Junchi Yan",
        note: "TPAMI 2023",
        noteoption: '',
        star: "https://img.shields.io/github/stars/OpenPerceptionX/HDGT?style=social",
        starlink: "https://github.com/OpenPerceptionX/HDGT",
        icon: [
            {
                type: "github",
                link: "https://github.com/OpenPerceptionX/HDGT",
            },
        ],
        description: "HDGT formulates the driving scene as a heterogeneous graph with different types of nodes and edges.",
        tag: "",
    },
    {
        title: "Distilling Focal Knowledge from Imperfect Expert for 3D Object Detection",
        link: "https://openaccess.thecvf.com/content/CVPR2023/html/Zeng_Distilling_Focal_Knowledge_From_Imperfect_Expert_for_3D_Object_Detection_CVPR_2023_paper.html",
        image: "/assets/publication/fd3d.jpg",
        author: "Jia Zeng, Li Chen, Hanming Deng, Lewei Lu, Junchi Yan, Yu Qiao, Hongyang Li",
        note: "CVPR 2023",
        noteoption: '',
        star: "",
        starlink: "",
        icon: [
            {
                type: "github",
                link: "",
            },
        ],
        description: "We investigate on how to distill the knowledge from an imperfect expert. We propose FD3D, a Focal Distiller for 3D object detection.",
        tag: "",
    },
    {
        title: "BEVFormer v2: Adapting Modern Image Backbones to Bird's-Eye-View Recognition via Perspective Supervision",
        link: "https://openaccess.thecvf.com/content/CVPR2023/html/Yang_BEVFormer_v2_Adapting_Modern_Image_Backbones_to_Birds-Eye-View_Recognition_via_CVPR_2023_paper.html",
        image: "/assets/publication/bevformerv2.jpg",
        author: "Chenyu Yang, Yuntao Chen, Hao Tian, Chenxin Tao, Xizhou Zhu, Zhaoxiang Zhang, Gao Huang, Hongyang Li, Yu Qiao, Lewei Lu, Jie Zhou, Jifeng Dai",
        note: "CVPR 2023 Highlight",
        noteoption: '',
        star: "",
        starlink: "",
        icon: [
            {
                type: "github",
                link: "",
            },
        ],
        description: "A novel bird's-eye-view (BEV) detector with perspective supervision, which converges faster and better suits modern image backbones.",
        tag: "",
    },
    {
        title: "Graph-based Topology Reasoning for Driving Scenes",
        link: "https://arxiv.org/abs/2304.05277",
        image: "/assets/publication/toponet.jpg",
        author: "Tianyu Li, Li Chen, Huijie Wang, Yang Li, Jiazhi Yang, Xiangwei Geng, Shengyin Jiang, Yuting Wang, Hang Xu, Chunjing Xu, Junchi Yan, Ping Luo, Hongyang Li",
        note: "arXiv 2023",
        noteoption: '',
        star: "https://img.shields.io/github/stars/opendrivelab/TopoNet?style=social",
        starlink: "https://github.com/OpenDriveLab/TopoNet",
        icon: [
            {
                type: "github",
                link: "https://github.com/OpenDriveLab/TopoNet",
            },
        ],
        description: "A new baseline for scene topology reasoning, which unifies heterogeneous feature learning and enhances feature interactions via the graph neural network architecture and the knowledge graph design.",
        tag: "",
    },
    {
        title: "Geometric-aware Pretraining for Vision-centric 3D Object Detection",
        link: "https://arxiv.org/abs/2304.03105",
        image: "/assets/publication/gapretrain.jpg",
        author: "Linyan Huang, Huijie Wang, Jia Zeng, Shengchuan Zhang, Liujuan Cao, Rongrong Ji, Junchi Yan, Hongyang Li",
        note: "arXiv 2023",
        noteoption: '',
        star: "",
        starlink: "",
        icon: [
            {
                type: "github",
                link: "",
            },
        ],
        description: "We propose GAPretrain, a plug-and-play framework that boosts 3D detection by pretraining with spatial-structural cues and BEV representation.",
        tag: "",
    },
    {
        title: "3D Data Augmentation for Driving Scenes on Camera",
        link: "https://link.springer.com/chapter/10.1007/978-981-97-8508-7_4",
        image: "/assets/publication/3daug.jpg",
        author: "Wenwen Tong, Jiangwei Xie, Tianyu Li, Yang Li, Hanming Deng, Bo Dai, Lewei Lu, Hao Zhao, Junchi Yan, Hongyang Li",
        note: "PRCV 2024",
        noteoption: '',
        star: "",
        starlink: "",
        icon: [
            {
                type: "github",
                link: "",
            },
        ],
        description: "We propose a 3D data augmentation approach termed Drive-3DAug to augment the driving scenes on camera in the 3D space.",
        tag: "",
    },
    {
        title: "Towards Capturing the Temporal Dynamics for Trajectory Prediction: a Coarse-to-Fine Approach",
        link: "https://proceedings.mlr.press/v205/jia23a.html",
        image: "/assets/publication/traj_xiaosong.jpg",
        author: "Xiaosong Jia, Li Chen, Penghao Wu, Jia Zeng, Junchi Yan, Hongyang Li, Yu Qiao",
        note: "CoRL 2022",
        noteoption: '',
        star: "",
        starlink: "",
        icon: [
            {
                type: "github",
                link: "",
            },
        ],
        description: "We find taking scratch trajectories generated by MLP as input, a refinement module based on structures with temporal prior, could  boost the accuracy.",
        tag: "",
    },
    {
        title: "BEVFormer: Learning Bird’s-Eye-View Representation From LiDAR-Camera via Spatiotemporal Transformers",
        link: "https://www.computer.org/csdl/journal/tp/2025/03/10791908/22ABgP6PlUQ",
        image: "/assets/publication/bevformer.jpg",
        author: "Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, Tong Lu, Yu Qiao, Jifeng Dai",
        note: "TPAMI 2025",
        noteoption: '',
        star: "https://img.shields.io/github/stars/fundamentalvision/BEVFormer?style=social",
        starlink: "https://github.com/fundamentalvision/BEVFormer",
        icon: [
            {
                type: "github",
                link: "https://github.com/fundamentalvision/BEVFormer",
            },
            {
                type: "zhihu",
                link: "https://zhuanlan.zhihu.com/p/564295059",
            },
        ],
        description: "A paradigm for autonomous driving that applies both Transformer and Temporal structure to generate BEV features.",
        tag: ["[nuScenes First Place]", "[Waymo Challenge 2022 First Place]"],
    },
    {
        title: "PersFormer: 3D Lane Detection via Perspective Transformer and the OpenLane Benchmark",
        link: "https://link.springer.com/chapter/10.1007/978-3-031-19839-7_32",
        image: "/assets/publication/persformer.jpg",
        author: "Li Chen, Chonghao Sima, Yang Li, Zehan Zheng, Jiajie Xu, Xiangwei Geng, Hongyang Li, Conghui He, Jianping Shi, Yu Qiao, Junchi Yan",
        note: "ECCV 2022 Oral",
        noteoption: '',
        star: "https://img.shields.io/github/stars/OpenDriveLab/PersFormer_3DLane?style=social",
        starlink: "https://github.com/OpenDriveLab/PersFormer_3DLane",
        icon: [
            {
                type: "github",
                link: "https://github.com/OpenDriveLab/PersFormer_3DLane",
            },
            {
                type: "dataset",
                link: "https://github.com/OpenDriveLab/OpenLane",
            },
            {
                type: "article",
                link: "https://patrick-llgc.github.io/Learning-Deep-Learning/paper_notes/persformer.html",
            },
            {
                type: "zhihu",
                link: "https://zhuanlan.zhihu.com/p/552908337",
            },
        ],
        description: "PersFormer adopts a unified 2D/3D anchor design and an auxiliary task to detect 2D/3D lanes; we release one of the first large-scale real-world 3D lane datasets, OpenLane.",
        tag: ["[Redefine the Community]"],
    },
];



function adrender() {
    const homepubbody = document.getElementById("pub_ad_algorithm");
    homepubbody.innerHTML = "";
    ad.forEach((item, _) => {
        var innerHTML = `
            <a href="${item.link}" target="_blank" class="hover:opacity-70 flex flex-row laptop:flex-col justify-center">
                <img loading="lazy" src="${item.image}" class="w-full tablet:w-5/6 laptop:w-96"/>
            </a>
            <div class="flex flex-col justify-center flex-1">
                <h3>
                    <a class="hover:text-o-blue" href=${item.link} target="_blank">
                        ${item.title}
                    </a>
                </h3>
                <p class="mt-3 authors">
                    ${item.author}
                </p>
                <div class="flex flex-row gap-6 flex-wrap justify-items-center mt-6">
                    <p class="bg-o-blue text-white p-1 pl-3 pr-3 rounded-3xl">
                        <a ${item.noteoption}>${item.note}</a>
                    </p>
                </div>
                <div class="flex flex-row gap-6 flex-wrap justify-items-center mt-6">
        `;
        if (item.star != "") {
            innerHTML += `
                    <a href="${item.starlink}" target="_blank"><img loading="lazy" src="${item.star}" class="h-8 hover:opacity-70"/></a>
            `;
        }
        item.icon.forEach((i, _) => {
            if (i.type != "github") {
                innerHTML += `                
                    <a href=${i.link} target="_blank"> 
                        <img loading="lazy" src="/assets/icon/${i.type}.png" class="h-8 hover:opacity-70"/> 
                    </a>
                `;
            };
        });
        innerHTML += `
                </div>
                <i class="mt-6 text-o-gray">
        `;
        if (item.tag != "") {
            item.tag.forEach((i, _) => {
                innerHTML += `
                <code>${i}</code>
                `;
            });
        };
        innerHTML += `
                    ${item.description}
                </i>
            </div>
        `;
        const pub = document.createElement("div");
        pub.className = "flex flex-col laptop:flex-row gap-6 laptop:gap-20";
        pub.innerHTML = innerHTML;
        homepubbody.appendChild(pub);
    });
}
