<head>
    <link rel="icon" type="image/png" href="/assets/icon/D_small.png">
    <title>AgiBot World | OpenDriveLab</title>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="keywords" content="AgiBot World, OpenDriveLab, Robotics, Embodied AI, Autonomous Driving">
    <meta name="description" content="OpenDriveLab is committed to exploring cutting-edge embodied AI technology, launching a series of benchmarking work, open source to serve the community, and promote the common development of the industry. Friends who are committed to making influential research are welcome to join!">

    <link href="/ui2025/css/index_tailwind.css" rel="stylesheet">

    <!-- Swiper -->
    <link rel="stylesheet" href="/ui2024/css/swiper-bundle.min.css"/>
    <script src="/ui2024/js/swiper-bundle.min.js"></script>

    <script src="/ui2024/js/jquery-3.7.1.min.js" integrity="sha256-/JqT3SQfawRcv/BIHPThkBvs0OEvtFFmqPF/lYI/Cxo=" crossorigin="anonymous"></script>
</head>



<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-L7VEDHS6G8"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-L7VEDHS6G8');
</script>



<body id="#">



    <header class="sticky top-6 laptop:top-12 flex ml-6 gap-2 h-0">

        <a href="/" class="h-8 flex justify-center items-center bg-white hover:bg-o-light-blue transition delay-100 duration-200 rounded-md p-2">
            <img loading="lazy" class="h-4 w-auto" src="/assets/icon/D.svg">
        </a>

        <a href="/publications/" class="h-8 flex justify-center items-center bg-white text-drama-black hover:bg-o-light-blue transition delay-100 duration-200 rounded-md p-4">
            <b class="font-mono">Publication</b>
        </a>

        <a href="/events/" class="h-8 flex justify-center items-center bg-white text-drama-black hover:bg-o-light-blue transition delay-100 duration-200 rounded-md p-4">
            <b class="font-mono">Event</b>
        </a>

    </header>



    <main id="page" class="w-full bg-drama-black  overflow-x-hidden">



        <!-- Landing -->
        <div class="h-svh laptop:h-high p-6 flex flex-col justify-end items-center bg-[url('/blog/agibot-world/robot.jpg')] bg-center bg-cover bg-black bg-opacity-30 bg-blend-overlay">

            <h1 class="text-white w-full max-w-wider mb-12">
                AgiBot World Colosseo: Dmocratizing Access to Large-Scale, High-Quality Robot Data, Advancing the Pursuit of Scalable, General-Purpose Intelligence
            </h1>

            <div class="flex flex-row w-full max-w-wider mb-12 gap-12">
                <img loading="lazy" src="/assets/brand/shanghai_ai_lab_white_.png" class="h-6"/>
                <img loading="lazy" src="/assets/brand/agibot_white.png" class="h-6"/>
            </div>

            <h5 class="text-white w-full max-w-wider mb-24 laptop:mb-12">
                March 10, 2025
            </h5>

        </div>



        <!-- Content -->
        <div class="flex flex-col items-center gap-6 p-6 pt-24 pb-24 laptop:pt-12 laptop:pb-12">



            <p class="text-white w-full max-w-wide leading-loose">
                Introducing AgiBot World, a large-scale platform comprising over 1 million trajectories across 217 tasks in five deployment scenarios,we achieve an order-of-magnitude increase in data scale compared to existing datasets. Accelerated by a standardized collection pipeline with human-in-the-loop verification, AgiBot World guarantees high-quality and diverse data distribution. It is extensible from grippers to dexterous hands and visuo-tactile sensors for fine-grained skill acquisition. Building on top of data, we introduce Genie Operator-1 (GO-1), a novel generalist policy that leverages latent action representations to maximize data utilization, demonstrating predictable performance scaling with increased data volume. Policies pre-trained on our dataset achieve an average performance improvement of 30% over those trained on Open X-Embodiment, both in in-domain and out-of-distribution scenarios. GO-1 exhibits exceptional capability in real-world dexterous and long-horizon tasks, achieving over 60% success rate on complex tasks and outperforming prior RDT approach by 32%.
            </p>



            <h3 class="text-white w-full max-w-wide leading-loose ">
                Data
            </h3>



            <p class="text-white w-full max-w-wide leading-loose">
                AgiBot World is a full-stack and open-source embodied intelligence ecosystem. Based on the hardware platform developed by us, AgiBot G1, we construct AgiBot World—an open-source robot manipulation dataset collected by more than 100 homogeneous robots, providing high-quality data for challenging tasks spanning a wide spectrum of real-life scenarios. The latest version contains 1,001,552 trajectories, with a total duration of 2976.4 hours, covering 217 specific tasks, 87 skills, and 106 scenes. We go beyond basic tabletop tasks such as pick-and-place in lab environments; instead, concentrate on real-world scenarios involving dual-arm manipulation, dexterous hands, and collaborative tasks. AgiBot World aims to provide an inclusive benchmark to drive the future development of advanced and robust algorithms.
            </p>



            <figure class="w-full max-w-wide mt-3 mb-3 laptop:mt-6 laptop:mb-6">
                <img loading="lazy" class="w-full" src="./pipeline.jpg"/>
                <figcaption class="text-white text-xs mt-3 leading-loose">
                    Figure 1: xxx
                </figcaption>
            </figure>



            <p class="text-white w-full max-w-wide leading-loose">
                Concurrent with feedback collection from data annotators, we adopt a human-in-the-loop approach to assess and refine data quality. This process involves an iterative cycle of collecting a small set of demonstrations, training a policy, and deploying the resulting policy to evaluate data availability. Based on the policy’s performance, we iteratively refine the data collection pipeline to address identified gaps or inefficiencies. For instance, during real-world deployment, the model exhibits prolonged pauses at the onset of actions, aligning with data annotator feedback highlighting inconsistent transitions and excessive idle time in the collected data. In response, we revise the data collection protocols and introduce a post-processing step to eliminate idle frames, thereby enhancing the dataset’s overall utility for policy learning. This feedback-driven methodology ensures continuous improvement in data quality.
            </p>



            <h3 class="text-white w-full max-w-wide leading-loose ">
                Model
            </h3>



            <figure class="w-full max-w-wide mt-3 mb-3 laptop:mt-6 laptop:mb-6">
                <img loading="lazy" class="w-full" src="./20250306-184718.jpeg"/>
                <figcaption class="text-white text-xs mt-3 leading-loose">
                    Figure 2: xxx
                </figcaption>
            </figure>


            <figure class="w-full max-w-wide mt-3 mb-3 laptop:mt-6 laptop:mb-6">
                <img loading="lazy" class="w-full" src="./20250306-184722.jpeg"/>
                <figcaption class="text-white text-xs mt-3 leading-loose">
                    Figure 2: xxx
                </figcaption>
            </figure>


            <figure class="w-full max-w-wide mt-3 mb-3 laptop:mt-6 laptop:mb-6">
                <img loading="lazy" class="w-full" src="./model.jpg"/>
                <figcaption class="text-white text-xs mt-3 leading-loose">
                    Figure 2: xxx
                </figcaption>
            </figure>



            <p class="text-white w-full max-w-wide leading-loose">
                To effectively utilize our high-quality AgiBot World dataset and enhance the policy’s generalizability, we propose a hierarchical Vision-Language-Latent-Action (ViLLA) framework with three training stages. Compared to Vision-Language-Action (VLA) model where action is vision-language conditioned, the ViLLA model predicts latent action tokens, conditioned on the generation of subsequent robot control actions. In Stage 1, we project consecutive images into a latent action space by training an encoder-decoder latent action model(LAM) on internet-scale heterogeneous data. This allows the latent action to serve as an intermediate representation, bridging the gap between general image-text inputs and robotic actions. In Stage 2, these latent actions act as pseudolabels for the latent planner, facilitating embodiment-agnostic long-horizon planning and leveraging the generalizability of the pre-trained VLM. Finally, in Stage 3, we introduce the action expert and jointly train it with the latent planner to support the learning of dexterous manipulation.
            </p>




            <h3 class="text-white w-full max-w-wide leading-loose ">
                xxx todo
            </h3>




            <ul class="list-outside list-disc space-y-6 w-full max-w-wide leading-loose">
                <li class="text-white ml-6">
                    Full-upper-body control: Helix is the first VLA to output high-rate continuous control of the entire humanoid upper body, including wrists, torso, head, and individual fingers.
                </li>
                <li class="text-white ml-6">
                    Multi-robot collaboration: Helix is the first VLA to operate simultaneously on two robots, enabling them to solve a shared, long-horizon manipulation task with items they have never seen before.
                </li>
                <li class="text-white ml-6">
                    Pick up anything: Figure robots equipped with Helix can now pick up virtually any small household object, including thousands of items they have never encountered before, simply by following natural language prompts.
                </li>
            </ul>



            <figure class="w-full max-w-wide mt-3 mb-3 laptop:mt-6 laptop:mb-6">
                <video src="https://videos.ctfassets.net/qx5k8y1u9drj/2a4YQTPs9dnzYKWZQ6c2ni/78812ae00c3232d684e6e5e7ef748a36/VLA_Full_Quality_MASTER_21925A.mp4" controls autoplay></video>
                <iframe height="450" class="w-full" src="https://www.youtube.com/embed/ZnX9SIBTxk4?si=1aqU_IYHxm-r7vYe" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
                <figcaption class="text-white text-xs mt-3 leading-loose">
                    Video 1: Collaborative grocery storage. A single set of Helix neural network weights runs simultaneously on two robots as they work together to put away groceries neither robot has ever seen before
                    <br>
                    Source: <a class="hover:underline" target="_blank">YouTube</a> <a class="hover:underline" target="_blank">bilibili</a>
                </figcaption>
            </figure>



            <h4 class="text-white w-full max-w-wide leading-loose ">
                New Scaling for Humanoid Robotics
            </h4>



            <p class="text-white w-full max-w-wide leading-loose">
                The home presents robotics' greatest challenge. Unlike controlled industrial settings, homes are filled with countless objects–delicate glassware, crumpled clothing, scattered toys–each with unpredictable shapes, sizes, colors, and textures. For robots to be useful in households, they will need to be capable of generating intelligent new behaviors on-demand, especially for objects they've never seen before.
            </p>



            <p class="text-white w-full max-w-wide leading-loose">
                The current state of robotics will not scale to the home without a step change. Teaching robots even a single new behavior currently requires substantial human effort: either hours of PhD-level expert manual programming or thousands of demonstrations. Both are prohibitively expensive when we consider how vast the problem of the home truly is.
            </p>



            <figure class="w-full max-w-wide mt-3 mb-3 laptop:mt-6 laptop:mb-6">
                <img loading="lazy" class="w-full" src="https://images.ctfassets.net/qx5k8y1u9drj/3iC6I99o9zVebi4YAct58Z/c0f52b7200aee4c9638fe9fb1d9a5788/NEW_SCALING_LAWS.png?fm=webp&w=1920&q=70"/>
                <figcaption class="text-white text-xs mt-3 leading-loose">
                    Figure 1: Scaling curves for different approaches to acquiring new robot skills. In conventional heuristic manipulation, skills grow with PhDs who manually script them. In conventional robot imitation learning, skills scale with data collected. With Helix, new skills can be specified on the fly with language.
                </figcaption>
            </figure>



            <div class="w-full max-w-wide mt-3 mb-3 laptop:mt-6 laptop:mb-6">
                <div class="swiper mySwiper">
                    <div class="swiper-wrapper">
                        <div class="swiper-slide">
                            <figure>
                                <img loading="lazy" class="w-full" src="https://images.ctfassets.net/qx5k8y1u9drj/3iC6I99o9zVebi4YAct58Z/c0f52b7200aee4c9638fe9fb1d9a5788/NEW_SCALING_LAWS.png?fm=webp&w=1920&q=70"/>
                            </figure>
                        </div>
                        <div class="swiper-slide">
                            <figure>
                                <img loading="lazy" class="w-full" src="https://images.ctfassets.net/qx5k8y1u9drj/3iC6I99o9zVebi4YAct58Z/c0f52b7200aee4c9638fe9fb1d9a5788/NEW_SCALING_LAWS.png?fm=webp&w=1920&q=70"/>
                            </figure>
                        </div>
                        <div class="swiper-slide">
                            <figure>
                                <img loading="lazy" class="w-full" src="https://images.ctfassets.net/qx5k8y1u9drj/3iC6I99o9zVebi4YAct58Z/c0f52b7200aee4c9638fe9fb1d9a5788/NEW_SCALING_LAWS.png?fm=webp&w=1920&q=70"/>
                            </figure>
                        </div>
                        <div class="swiper-slide">
                            <figure>
                                <img loading="lazy" class="w-full" src="https://images.ctfassets.net/qx5k8y1u9drj/3iC6I99o9zVebi4YAct58Z/c0f52b7200aee4c9638fe9fb1d9a5788/NEW_SCALING_LAWS.png?fm=webp&w=1920&q=70"/>
                            </figure>
                        </div>
                    </div>
                    <div class="swiper-pagination"></div>
                  </div>
            </div>



        </div>

        

    </main>
</body>



<!-- Initialize Swiper -->
<script>
    var swiper = new Swiper(".mySwiper", {
        effect: "cards",
        grabCursor: true,
    });
</script>
