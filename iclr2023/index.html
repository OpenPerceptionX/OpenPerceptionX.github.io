<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport"
        content="width=device-width,initial-scale=1.0,minimum-scale=1.0,maximum-scale=1.0,user-scalable=no" />
<title>ICLR 2023 Autonomous Driving Workshop | OpenDriveLab</title>
<meta name="keywords" content="ICLR 2023 Workshop, OpenDriveLab, Robotics, Autonomous Driving, Embodied AI">
<link rel="icon" type="image/png" href="/assets/icon/D_small.png">
<link href="/ui2023/cvpr2023/css/all.css" rel="stylesheet" type="text/css" media="all">
<link href="/ui2023/css/footer.css?11" rel="stylesheet" type="text/css" media="all">
<link href="/ui2023/css/head_event.css" rel="stylesheet" type="text/css" media="all">

<style>
  .r_ul a{padding:0px 0px;}
@media screen and (min-width:1024px){
	.r_ul.r_ul_this{ position:fixed; top:0; bottom:auto;}
}
</style>
<script src="https://kit.fontawesome.com/8990c176d0.js" crossorigin="anonymous"></script>

</head>



<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-L7VEDHS6G8"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-L7VEDHS6G8');
</script>



<body class="cvpr23 home">
<!-- 如果是首頁加上 樣式 home -->

<div class="header" href="/"> <a class="head_fanhui" href="/"></a>
  <div class="rnav m" style="margin-left: 86%;"></div>
</div>
  <div class="bannerw">
  <div class="banner" style="background-image: url(/ui2023/cvpr2023/iclr2023/banner.jpg); background-blend-mode: overlay; background-color: rgb(63, 63, 63); background-size: cover;">
    <div class="banner_dian"></div>
   
    <div class="banner_textw">
      <div class="bg">
        <div class="juzhongduiqi">
          <div>
		    <!-- <p class="index_end"><img src="/ui2023/cvpr2023/img/indexend.svg"   data-swiper-parallax-x="-1600" /><img src="/ui2023/cvpr2023/img/indexauto.svg"  data-swiper-parallax-x="-1600" /></p> -->
            <!-- <div class="t_text t_text2 m"> <img src="/ui2023/cvpr2023/img/title1.svg"   /><img src="/ui2023/cvpr2023/img/title2.svg"   /> </div>
                   <div class="t_text  pc"> <img src="/ui2023/cvpr2023/img/title.svg" class="title_auto" /></div> -->
          
            <!-- <p><img src="/ui2023/cvpr2023/img/cvpr2023.svg" class="titel_cvpr" data-swiper-parallax-x="-1600" /></p> -->

            <p class="my_title">
              Scene Representations for Autonomous Driving
            </p>

            <br>


            <p class="my_subtitle">
              Hybrid Workshop
            </p>
            <p  class="r_time" style="padding-bottom: 1%;">in conjunction with</p>
            <p class="my_subtitle">
              <a style ="color: goldenrod;"href="https://iclr.cc/">ICLR 2023</a>
            </p>
            <p class="r_time">
              May 5th, Kigali City, Rwanda, Africa
            </p>

            <p>
              <img src="/ui2023/cvpr2023/img/sh_ai_lab.png" class=" banner_img" data-swiper-parallax-x="-1600" />
            </p>
            <br />
            <a class="lj_button3" data-swiper-parallax-x="-4000"  href="https://iclr.cc/virtual/2023/workshop/12831">
              Recording 
              <img
              src="/ui2023/img/icon/iclr.svg"
              style="height: 30px; user-select: none;"
              />
            </a> 
            <a class="lj_button3" data-swiper-parallax-x="-4000"  href="https://space.bilibili.com/503310953">
              Recording 
              <img
              src="/ui2023/img/icon/bilibili.svg"
              style="height: 30px; user-select: none;"
              />
            </a> 
            <a class="lj_button3" data-swiper-parallax-x="-4000"  href="https://www.youtube.com/watch?v=l1fQEaLSBzo&list=PL3N9otbGBVLeyyiFc7mFnSDsFkelVIceI">
              Recording 
              <img
              src="/ui2023/img/icon/youtube_red.png"
              style="height: 30px; user-select: none;"
              />
            </a> 
          </div>
        </div>
      </div>
    </div>
    
  </div>
  
  <div class="r_ul">
    <ul class="bg">
      <li data-gd="#tttttt"class="li_mobile"><a href="/"><img src="/ui2023/img/team_logo_long.png" style="display: inline; width: 50%; margin-left: -3%;"></a></li>
      <li data-gd="#id1"><a href="#overview">Overview</a></li>
      <li data-gd="#id4"><a href="#speakers">Speakers</a></li>
	    <li data-gd="#schedule"><a href="#schedule">Schedule</a></li>
      <li data-gd="#id7"><a href="#call_for_contributions">Call for Contributions</a></li>
      <li data-gd="#id6"><a href="#organizers">Organizers</a></li>
    </ul>
  </div>
  </div>
  <!-- ################################################################################################ -->

  
    
 
    
    
  
  
    <div class="wrapper row2 " id="id1">
      <div class="hoc bg clear" id="overview">
  
  
          <h1 class="h_title"><a href="#overview">Overview<img src="/ui2023/img/icon/link.png" class="title_link"/></a></h1>
    
          <!-- <h2 style="text-transform: none;">
            Welcome to the <a href="https://cvpr2023.thecvf.com/">CVPR 2023</a> Workshop on End-to-End Autonomous Driving!
          </h2> -->
          <p itemprop="description">
            This workshop aims to promote the real-world impact of ML research toward self-driving technology. 
            While ML-based components of modular stacks have been a huge success, there remains progress to be made in the development of integration strategies and intermediate representations. 
            We invite contributions discussing the following topics, in order to empower the next generation of autonomous vehicles:
              <li style="list-style-type:disc;">
                Representation learning for perception, prediction, planning, simulation, etc
              </li>
              <li style="list-style-type:disc;">
                Approaches that account for interactions between traditional sub-components (e.g., joint perception and prediction, end-to-end driving)
              </li>
              <li style="list-style-type:disc;">
                ML / statistical learning approaches to facilitate safety / interpretability / generalization
              </li>
              <li style="list-style-type:disc;">
                Driving environments / datasets for benchmarking ML algorithms
              </li>
              <li style="list-style-type:disc;">
                New perspectives on the future of autonomous driving
              </li>
          </p>
        
  
            <!-- <p>The workshop <a href="/sr4ad/iclr23">Scene Representations For Autonomous Driving</a> on ICLR 2023 will be held on May 5th. Check it out!
            </p> -->
  
            <!-- Contact us at <a class="xh" href="mailto:workshop-e2e-ad@googlegroups.com">workshop-e2e-ad@googlegroups.com</a>.  -->
            <li><i class="fas fa-envelope"></i> Contact us with prefix [ICLR 2023 SR4AD] at <a  class="xh" href="mailto:workshop-e2e-ad@googlegroups.com">workshop-e2e-ad@googlegroups.com</a>.

            <!-- <br><br> -->

            <!-- <a href = "https://docs.google.com/forms/d/e/1FAIpQLScpD9rScD-pT_BwNcHrRWUYxpDDis6qn2MlQ5bZMx3alT5KyQ/viewform?usp=sf_link">Sign Up for Updates via the link</a>.  -->
            
  
            <!-- <p class="color_lvse xh"  > <a href="https://forms.gle/TcmAd2cNNFoo7x829"> Sign up for updates via the link</a>.</p> -->
  
      
        </div>
       
    </div>
  
  
  
  
   <!-- Speakers -->
  
    <div class="wrapper row1" id="id4">
      <section class="hoc bg clear" id="speakers">
        
  
        <h1  class="h_title"><a href="#speakers">Speakers<img src="/ui2023/img/icon/link.png" class="title_link"/></a></h1>
  
 
        
  
        <ul class=" group">
  
          <li>
  
           
              <a href="https://hangzhaomit.github.io" target="_blank">
                <div class="photo">   <img  src="/ui2023/img/team/hang_zhao.jpg" alt="Hang Zhao" ></div>
              
            
            <h1>Hang Zhao</h1>
      <p>  <i class="workshop_speaker_title">Assistant Professor</i>Tsinghua University</p>
              
            
  </a>
          </li>
  
          <li >
            
              <a href="https://jamie.shotton.org" target="_blank">
                <div class="photo">    <img   src="/ui2023/img/team/jamie_shotton.jpg" alt="Jamie Shotton" ></div>
           
            <h1>Jamie Shotton</h1>
           
              <p><i class="workshop_speaker_title">Chief Scientist</i>Wayve</p>
         
              
  
            
          </a>
          </li>

          <li >
            
            <a href="https://yiyiliao.github.io" target="_blank">
              <div class="photo">    <img   src="/ui2023/img/team/yiyi_liao.jpg" alt="Yiyi Liao" ></div>
         
          <h1>Yiyi Liao</h1>
         
            <p><i class="workshop_speaker_title">Assistant Professor</i>Zhejiang University</p>
       
            

          
        </a>
        </li>
  
          <li>
              <a href="https://vas.mpi-inf.mpg.de/dengxin" target="_blank">
                <div class="photo">   <img  src="/ui2023/img/team/dengxin_dai.jpg" alt="Dengxin Dai" ></div>
            
  
            <h1 >Dengxin Dai</h1>
          <p>
              <i class="workshop_speaker_title">Senior Researcher</i>MPI for Informatics</li>
              </p>
      
  
           
          </li>
  
  
      
  
    
  
  
          <li>
            
              <a href="https://aisecure.github.io" target="_blank">
                <div class="photo"> <img  src="/ui2023/img/team/bo_li.jpg" alt="Bo Li" ></div>
            <h1>Bo Li</h1>
             <p>
              <i class="workshop_speaker_title">Assistant Professor</i>UIUC
        
              </p>
  
           
  </a>
          </li>

  
          <li>
            
              <a href="https://people.ee.ethz.ch/~csakarid" target="_blank">
                <div class="photo"> 
                <img  src="/ui2023/img/team/christos_sakaridis.jpg" alt="Christos Sakaridis"></div>
      
  
            <h1 >Christos Sakaridis</h1>
          <p>
              <i class="workshop_speaker_title">Postdoctoral Researcher</i>ETH Zurich
            
              </p>
  
  </a>
          
  
          </li>
  
   
  
          <li >
  
           
              <a href="https://hangqiu.github.io" target="_blank">
                <div class="photo"> 
                <img  src="/ui2023/img/team/hang_qiu.jpg" alt="Hang Qiu" >
        </div>
            <h1>Hang Qiu</h1> 
               <p>
              <i class="workshop_speaker_title">Software Engineer</i>Waymo
          </p>
              
        </a>
  
          </li>
  
        </ul>
  
     
  
        
      </section>
    </div>
  
  
 
    
    <!-- ##### Schedule #### -->
    <div class="wrapper row2 bg_Schedule" id="schedule">
      <div class="hoc bg clear" id="schedule">
        
        <h1 class="h_title"><a href="#schedule">Schedule<img src="/ui2023/img/icon/link.png" class="title_link"/></a></h1>
  
  
        <h2 style="font-size: large; text-transform: none">Please select your time zone: 
  
        
  
        <select id="operator" style="font-size: 0.9rem;" onchange="iclr_time_zone()">
  
          <option value="-13" onclick="iclr_time_zone()">UTC-11 Niue/Pago_Pago</option>
          <option value="-12" onclick="iclr_time_zone()">UTC-10 Honolulu/Adak </option>
          <option value="-11" onclick="iclr_time_zone()">UTC-9 Alaska/Gambier Islands</option>
          <option value="-10" onclick="iclr_time_zone()">UTC-8 Anchorage/Nome</option>
          <option value="-9" onclick="iclr_time_zone()" >UTC-7 Vancouver/Los_Angeles</option>
          <option value="-8" onclick="iclr_time_zone()">UTC-6 Belize</option>
          <option value="-7" onclick="iclr_time_zone()">UTC-5 Chicago</option>
          <option value="-6" onclick="iclr_time_zone()">UTC-4 New_York/Detroit</option>
          <option value="-5" onclick="iclr_time_zone()">UTC-3 Araguaina/Maceio</option>
          <option value="-4" onclick="iclr_time_zone()">UTC-2 Noronha/South_Georgia</option>
          <option value="-3" onclick="iclr_time_zone()">UTC-1 Cape_Verde/Azores</option>
          <option value="-2" onclick="iclr_time_zone()">UTC+0 Ireland/Portugal</option>
          <option value="-1" onclick="iclr_time_zone()">UTC+1 London/Canary</option>
          <option value="0" onclick="iclr_time_zone()" selected="selected">UTC+2 (ICLR venue time) Kigali/Paris/Berlin/Cairo</option>
          <option value="1" onclick="iclr_time_zone()">UTC+3 Moscow/Baghdad</option>
          <option value="2" onclick="iclr_time_zone()">UTC+4 Dubai/Mahe</option>
          <option value="3" onclick="iclr_time_zone()">UTC+5 Qyzylorda/Kerguelen</option>
          <option value="4" onclick="iclr_time_zone()">UTC+6 Almaty/Kolkata</option>
          <option value="5" onclick="iclr_time_zone()">UTC+7 Bangkok/Christmas</option>
          <option value="6" onclick="iclr_time_zone()">UTC+8 Beijing/Hong_Kong</option>
          <option value="7" onclick="iclr_time_zone()">UTC+9 Tokyo/Seoul</option>
          <option value="8" onclick="iclr_time_zone()">UTC+10 Sydney/Brisbane</option>
          <option value="9" onclick="iclr_time_zone()">UTC+11 Lord_Howe/Kosrae</option>
          <option value="10" onclick="iclr_time_zone()">UTC+12 Wallis/Fiji</option>
      </select>
  
  
  
      </h2>
  
  
  
         <div class="schedule-tbl">
          <table>
            <thead>
              <tr>
                <th class="schedule-time">Time</th>
                <th class="schedule-slot">Speaker</th>
                <th class="schedule-description">Theme</th>
                <th class="schedule-meta">Recording</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td class="schedule-time"><span name="time_hour_seleting">8</span>:50,<br class="m"> May <a  name="data_seleting">5</a></td>
                <td class="schedule-slot" ><a href="https://scholar.google.ae/citations?user=ulZxvY0AAAAJ&hl=zh-CN"> Li Chen</a></td>
                <td class="schedule-description">Introduction and opening remarks</td>
                <td class="schedule-description"><a href="https://www.bilibili.com/video/BV1Hk4y1s7Dw"><img src="/ui2023/img/icon/bilibili.svg" style="height: 20px; user-select: none;"/></a><a href="https://www.youtube.com/watch?v=l1fQEaLSBzo&list=PL3N9otbGBVLeyyiFc7mFnSDsFkelVIceI"><img src="/ui2023/img/icon/youtube_red.png" style="height: 20px; user-select: none; margin-left: 20%;"/></a></td>
  
              </tr>
              <tr>
                <td class="schedule-time"><span name="time_hour_seleting">9</span>:00,<br class="m"> May <a  name="data_seleting">5</a></td>
                <td class="schedule-slot"><a href="https://hangqiu.github.io/">Hang Qiu</a></td>
                <td class="schedule-description"><strong>Scene Understanding beyond the Visible</strong></td>
                <td class="schedule-description"><a href="https://www.bilibili.com/video/BV1nm4y187QZ"><img src="/ui2023/img/icon/bilibili.svg" style="height: 20px; user-select: none;"/></a><a href="https://www.youtube.com/watch?v=oz0AnmJZCR4&list=PL3N9otbGBVLeyyiFc7mFnSDsFkelVIceI"><img src="/ui2023/img/icon/youtube_red.png" style="height: 20px; user-select: none; margin-left: 20%;"/></a></td>
              </tr>
              <tr>
                <td class="schedule-time"><span name="time_hour_seleting">9</span>:30,<br class="m"> May <a  name="data_seleting">5</a></td>
                <td class="schedule-slot"> <a href="https://hangzhaomit.github.io/">Hang Zhao</a></td>
                <td class="schedule-description"><strong>Vision-Centric Autonomous Driving: Perception, Prediction and Mapping</strong></td>
                <td class="schedule-description"><a href="https://www.bilibili.com/video/BV1Cg4y1V73X"><img src="/ui2023/img/icon/bilibili.svg" style="height: 20px; user-select: none;"/></a><a href="https://www.youtube.com/watch?v=dKVL0RKUmRQ&list=PL3N9otbGBVLeyyiFc7mFnSDsFkelVIceI"><img src="/ui2023/img/icon/youtube_red.png" style="height: 20px; user-select: none; margin-left: 20%;"/></a></td>  
              </tr>
              <tr>
                <td class="schedule-time"><span name="time_hour_seleting">10</span>:00,<br class="m"> May <a  name="data_seleting">5</a></td>
                <td class="schedule-slot" ><a href="https://scholar.google.ae/citations?user=ulZxvY0AAAAJ&hl=zh-CN"> Li Chen</a></td>
                <td class="schedule-description">Contributions</td>
                <td class="schedule-description"><a href="https://www.bilibili.com/video/BV1MT411b7G6"><img src="/ui2023/img/icon/bilibili.svg" style="height: 20px; user-select: none;"/></a><a href="https://www.youtube.com/watch?v=0ccu72jOh_k&list=PL3N9otbGBVLeyyiFc7mFnSDsFkelVIceI"><img src="/ui2023/img/icon/youtube_red.png" style="height: 20px; user-select: none; margin-left: 20%;"/></a></td>
  
              </tr>
              <tr>
                <td class="schedule-time"><span name="time_hour_seleting">11</span>:00,<br class="m"> May <a  name="data_seleting">5</a></td>
                <td class="schedule-slot">Coffee Break</td>
                <td class="schedule-description">Coffee Break</td>
                <td> - </td>
  
  
              </tr>
              <tr>
                <td class="schedule-time"><span name="time_hour_seleting">11</span>:30,<br class="m"> May <a  name="data_seleting">5</a></td>
                <td class="schedule-slot"> <a href="https://vas.mpi-inf.mpg.de/dengxin/">Dengxin Dai</a></td>
                <td class="schedule-description"><strong>Robust Visual Perception for All Domains: Domain Synthesis, Adaptation, and Generalization</strong></td>
                <td class="schedule-description"><a href="https://www.bilibili.com/video/BV1hh4y1t7k3"><img src="/ui2023/img/icon/bilibili.svg" style="height: 20px; user-select: none;"/></a><a href="https://www.youtube.com/watch?v=TEHHXfbVLs0&list=PL3N9otbGBVLeyyiFc7mFnSDsFkelVIceI"><img src="/ui2023/img/icon/youtube_red.png" style="height: 20px; user-select: none; margin-left: 20%;"/></a></td>
  
              </tr>
              <tr>
                <td class="schedule-time"><span name="time_hour_seleting">12</span>:00,<br class="m"> May <a  name="data_seleting">5</a></td>
                <td class="schedule-slot"><a href="https://yiyiliao.github.io/">Yiyi Liao</a></td>
                <td class="schedule-description"><strong>Towards Generative Photorealistic Simulation</strong></td>
                <td class="schedule-description"><a href="https://www.bilibili.com/video/BV1YL41167U8"><img src="/ui2023/img/icon/bilibili.svg" style="height: 20px; user-select: none;"/></a><a href="https://www.youtube.com/watch?v=SiJU0S3sOgM&list=PL3N9otbGBVLeyyiFc7mFnSDsFkelVIceI"><img src="/ui2023/img/icon/youtube_red.png" style="height: 20px; user-select: none; margin-left: 20%;"/></a></td>
  
              </tr>
              <tr>
                <td class="schedule-time"><span name="time_hour_seleting">12</span>:30,<br class="m"> May <a  name="data_seleting">5</a></td>
                <td class="schedule-slot">Lunch Break</td>
                <td class="schedule-description">Lunch Break</td>
                <td> - </td>
  
              </tr>
              <tr>
                <td class="schedule-time"><span name="time_hour_seleting">13</span>:30,<br class="m"> May <a  name="data_seleting">5</a></td>
                <td class="schedule-slot">  <a href="https://jamie.shotton.org/">Jamie Shotton</a></td>
                <td class="schedule-description"><strong>Learning a Globally Scalable Driving Intelligence</strong></td>
                <td class="schedule-description"><a href="https://www.bilibili.com/video/BV1jo4y1G7dM"><img src="/ui2023/img/icon/bilibili.svg" style="height: 20px; user-select: none;"/></a><a href="https://www.youtube.com/watch?v=25zb5mdQeWQ&list=PL3N9otbGBVLeyyiFc7mFnSDsFkelVIceI"><img src="/ui2023/img/icon/youtube_red.png" style="height: 20px; user-select: none; margin-left: 20%;"/></a></td>
  
              </tr>
              <tr>
                <td class="schedule-time"><span name="time_hour_seleting">14</span>:00,<br class="m"> May <a  name="data_seleting">5</a></td>
                <td class="schedule-slot"><a href="https://mengyeren.com/">Mengye Ren</a></td>
  
                <td class="schedule-description">Contributions</td>
                <td class="schedule-description"><a href="https://www.bilibili.com/video/BV19o4y137ZU"><img src="/ui2023/img/icon/bilibili.svg" style="height: 20px; user-select: none;"/></a><a href="https://www.youtube.com/watch?v=EZE8edy-emc&list=PL3N9otbGBVLeyyiFc7mFnSDsFkelVIceI"><img src="/ui2023/img/icon/youtube_red.png" style="height: 20px; user-select: none; margin-left: 20%;"/></a></td>
  
              </tr>
              <tr>
                <td class="schedule-time"><span name="time_hour_seleting">15</span>:00,<br class="m"> May <a  name="data_seleting">5</a></td>
                <td class="schedule-slot">Coffee Break</td>
                <td class="schedule-description">Coffee Break</td>
                <td> - </td>
  
              </tr>
              <tr>
              <td class="schedule-time"><span name="time_hour_seleting">15</span>:30,<br class="m"> May <a  name="data_seleting">5</a></td>
              <td class="schedule-slot"> <a href="https://people.ee.ethz.ch/~csakarid/">Christos Sakaridis</a></td>
                <td class="schedule-description"><strong>Optimizing Internal Network Representations for Geometric and Semantic Perception</strong></td>
                <td class="schedule-description"><a href="https://www.bilibili.com/video/BV16u411x7zu"><img src="/ui2023/img/icon/bilibili.svg" style="height: 20px; user-select: none;"/></a><a href="https://www.youtube.com/watch?v=oChkKNuUJOs&list=PL3N9otbGBVLeyyiFc7mFnSDsFkelVIceI"><img src="/ui2023/img/icon/youtube_red.png" style="height: 20px; user-select: none; margin-left: 20%;"/></a></td>
  
              </tr>
              <tr>
                <td class="schedule-time"><span name="time_hour_seleting">16</span>:00,<br class="m"> May <a  name="data_seleting">5</a></td>
                <td class="schedule-slot"><a href="https://aisecure.github.io/">Bo Li</a> </td>
                <td class="schedule-description"><strong>Secure and Safe Autonomous Driving in Adversarial Environments</strong></td>
                <td class="schedule-description"><a href="https://www.bilibili.com/video/BV1Uh411w76e"><img src="/ui2023/img/icon/bilibili.svg" style="height: 20px; user-select: none;"/></a><a href="https://www.youtube.com/watch?v=-4IiXBx9_AA&list=PL3N9otbGBVLeyyiFc7mFnSDsFkelVIceI"><img src="/ui2023/img/icon/youtube_red.png" style="height: 20px; user-select: none; margin-left: 20%;"/></a></td>
  
              </tr>
              <tr>
                <td class="schedule-time"><span name="time_hour_seleting">16</span>:30-<span name="time_hour_seleting">17</span>:30,<br class="m"> May <a  name="data_seleting">5</a></td>
                <td class="schedule-slot"><a href="https://mengyeren.com/">Mengye Ren</a></td>
                <td class="schedule-description">Panel Discussion with Hongyang Li, Hang Zhao, Yiyi Liao, Christos Sakaridis, Bo Li</td>
                <td class="schedule-description"><a href="https://www.bilibili.com/video/BV14z4y1b7NX"><img src="/ui2023/img/icon/bilibili.svg" style="height: 20px; user-select: none;"/></a><a href="https://www.youtube.com/watch?v=fRnbSAT0RLs&list=PL3N9otbGBVLeyyiFc7mFnSDsFkelVIceI"><img src="/ui2023/img/icon/youtube_red.png" style="height: 20px; user-select: none; margin-left: 20%;"/></a></td>
  
              </tr>

  
            </tbody>
          </table>
          </div>
          <div>

        </h1>
        </div>
  
      </div>
    </div>
  
  
  
  
  
      <!-- Call For Contributions -->
  
  
      <div class="wrapper row1" id="id7">
        <div class="hoc bg clear" id="call_for_contributions">
      
            <h1 class="h_title" style="text-transform: none;"><a href="#call_for_contributions">Call for Contributions<img src="/ui2023/img/icon/link.png" class="title_link"/></a></h1>
  
     
            <div class="content" >
              <!-- <style>
                label, input, textarea, select, button{display:inline-block; resize:none; outline:none; color:inherit; font-size:inherit; font-family:inherit; vertical-align:middle;}
              </style> -->
      
                <section>
                    <!-- <header>
                      <h2 style="text-transform: none;">There will be a best paper award for each track! <br><br>
                      Selected submissions will be given oral presentations onsite.</h2>
                    </header> -->
                    <h2 style="text-transform: none;">
                      There will be a best paper award for each track! <br>
                      Selected submissions will be given oral presentations onsite.
                    </h2>
                    <br>
                    <!-- <details> -->
                        <!-- <summary > -->
                          <header>
                          <h2>
                            Important Dates
                          </h2>
                          </header>
                        <!-- </summary> -->
                        <li style="list-style-type:disc;">Deadline for first-round submission of contributions : <b>Feb 15, 2023</b>, 23:59 Anywhere on Earth.</li>
                        <ol>
                          <li style="list-style-type:circle;"> <a href="https://openreview.net/group?id=ICLR.cc/2023/Workshop/SR4AD&referrer=%5BHomepage%5D(%2F)">Submission link</a> </li>
                          <li style="list-style-type:circle;">Notification of first-round acceptance: <b>Mar 03, 2023</b></li>
                        </ol>
                        <li style="list-style-type:disc;">Deadline for second-round submission of contributions : <b>Mar 1, 2023</b>, 23:59 Anywhere on Earth.</li>
                        <ol>
                          <li style="list-style-type:circle;"> <a href="https://openreview.net/group?id=ICLR.cc/2023/Workshop/SR4AD&referrer=%5BHomepage%5D(%2F)">Submission link</a> </li>
                          <li style="list-style-type:circle;">Notification of second-round acceptance: <b>Mar 15, 2023</b></li>
                        </ol>
                    <!-- </details> -->
      
      
                    <br>
                    <br>
                    <header>
                      <h2>Submission Tracks</h2>
                    </header>
                    <p>
                      To promote a diversity of content, contributions can be in the form of (1) <b>blog posts</b>, (2) <b>github repositories</b>, or (3)  <b>PDF documents</b> (2-8 pages). Further, there will be two submission tracks:
                    </p>
                        <li style="list-style-type:disc;"><b>Track 1: Research Insight</b></li>
                          <ol>
                            <li style="list-style-type:circle;">Submit an analysis or a reimplementation of <b>others' work</b> that you wrote. You are a reporter / critic in this track.</li>
                            <li style="list-style-type:circle;">Authors will have to declare their conflicts of interest (both positive or negative) with the paper (and authors) they write about (e.g, recent collaboration, same institute, challenge competitor).</li>
                            <li style="list-style-type:circle;">Submissions will be reviewed for <b>significant added value</b> in comparison to the cited paper(s).</li>
                            <li style="list-style-type:circle;">The analysis must be supported by <b>accurate and clear arguments</b>.</li>
                          </ol>
                        <li  style="list-style-type:disc;"><b>Track 2: Original Contribution</b></li>
                          <ol>
                            <li style="list-style-type:circle;">Submit original <b>work of your own</b>, which has not been published previously.</li>
                            <li style="list-style-type:circle;">Submissions will be reviewed for <b>claims supported by convincing evidence</b>.</li>
                          </ol>
                    <br>
                    <br>
                    <header>
                      <h2>Format Guidelines</h2>
                    </header>
                      <li style="list-style-type:disc;">For blog posts, please <b>submit a URL</b> containing the content to be reviewed. There is no specific size or formatting template enforced for this track. Please refer to the <a href="https://iclr-blog-track.github.io/home/#accepted-posts">ICLR 2022 blog post track website</a> for some content examples.</li>
                      <li style="list-style-type:disc;">For repositories, please <b>submit a URL</b> to the primary README.md file that describes how to run the code.</li>
                      <li style="list-style-type:disc;">For PDFs, please use the <a href="https://github.com/ICLR/Master-Template/raw/master/iclr2023.zip">ICLR template</a>. Submissions can be in the extended abstract (2-4 pages) or full paper (4-8 pages) formats. The page limits do not include references or appendices.</li>
                    <br>
                    <br>
                    <header>
                      <h2>Review</h2>
                    </header>
                      <li style="list-style-type:disc;">Submissions will be through OpenReview.</li></li>
                      <li style="list-style-type:disc;">The review process will be <b>single-blind.</b></li>
                      <li style="list-style-type:disc;">Submissions and reviews will be private. Only accepted contributions will be made public.</li>
                      <li style="list-style-type:disc;">The workshop is a <b>non-archival</b> venue and will not have official proceedings. PDF submissions can be subsequently or concurrently submitted to other venues.</li>
                      <li style="list-style-type:disc;">All accepted contributions will be presented as posters.</li>
                      <li style="list-style-type:disc;">At least one co-author of each accepted contribution is expected to register for ICLR 2023 and attend the poster session. <b>Remote attendance is permitted.</b></li>
                      <li style="list-style-type:disc;">All the accepted contributions will be available on our workshop website, though authors can indicate explicitly if they want to opt out.</li>
                    <br>
                    <li><i class="fas fa-envelope"></i> Contact us with prefix [ICLR 2023 SR4AD] at <a  class="xh" href="mailto:workshop-e2e-ad@googlegroups.com">workshop-e2e-ad@googlegroups.com</a>.
                    <!-- <a href="mailto:workshop-e2e-ad@googlegroups.com"><li><i class="fas fa-envelope"></i> Contact Us with Prefix [ICLR 2023 SR4AD]</li></a> -->
                    <!-- Please contact us at workshop-e2e-ad@googlegroups.com -->
                    <br>
                    <br>
                    <br>
                    <header>
                      <h2>Accepted Contributions</h2>
                    </header>
                    <style>
                      .paper_title{font-size: 1rem; color: black;}
                      .paper_author{font-size: 0.8rem;margin-top: -5px;}
                      /* label, input, textarea, select, button{display:inline-block; resize:none; outline:none; color:inherit; font-size:inherit; font-family:inherit; vertical-align:middle;} */
      
      
                    </style>
                      <li style="list-style-type:disc;"><b>Track 1: Research Insight</b></li>
                      <ol>
                        <p class="paper_title"><a class = 'hover_color' style="color: #000;"href="
                          https://openreview.net/forum?id=vMH8jz5OkW                    ">
                          How Do Vision Transformers See Depth in Single Images?                  </a></p>
                        <p class="paper_author" target="_blank"> 
                          Peter Mortimer
                        <br>  <b>Best Paper Award (Track 1)</b>, paper can be found here: <a  href="
                        https://openreview.net/forum?id=vMH8jz5OkW                    ">
                        ICLR 2023 Workshop SR4AD HYBRID                 </a>
                                          </p>
      
                        <p class="paper_title"><a class = 'hover_color' style="color: #000;"href="
                          https://openreview.net/forum?id=0P_RrBXmIAU                    ">
                          Image Reconstruction from Event Cameras for Autonomous Driving                   </a></p>
                        <p class="paper_author" target="_blank"> 
                          Daniel Dauner                   
                          <br>  The paper can be found here: <a  href="
                          https://openreview.net/forum?id=0P_RrBXmIAU                    ">
                          ICLR 2023 Workshop SR4AD HYBRID                 </a>                 </p>
      
      
                        </ol>
      
      
                      <li style="list-style-type:disc;"><b>Track 2: Original Contribution</b></li>
                      <ol>
      
                        <p class="paper_title"><a class = 'hover_color' style="color: #000;"href="
                          https://openreview.net/forum?id=pyi73rdeGP                    ">
                          Benchmarking 3D Perception Robustness to Common Corruptions and Sensor Failure                  </a></p>
                        <p class="paper_author" target="_blank"> 
                          Lingdong Kong, Youquan Liu, Xin Li, Runnan Chen, Wenwei Zhang, Jiawei Ren, Liang Pan, Kai Chen, Ziwei Liu                           <br>  <b>Best Paper Award (Track 2)</b>, The paper can be found here: <a  href="
                          https://openreview.net/forum?id=pyi73rdeGP                    ">
                          ICLR 2023 Workshop SR4AD HYBRID                 </a>         </p>
      
                        <p class="paper_title"><a class = 'hover_color' style="color: #000;"href="
                          https://openreview.net/forum?id=AGoZR5DfKf0                    ">
                          Pedestrian Cross Forecasting with Hybrid Feature Fusion                  </a></p>
                        <p class="paper_author" target="_blank"> 
                          Meng Dong                    <br>  The paper can be found here: <a  href="
                          https://openreview.net/forum?id=AGoZR5DfKf0                    ">
                          ICLR 2023 Workshop SR4AD HYBRID                 </a>                </p>
      
                        <p class="paper_title"><a class = 'hover_color' style="color: #000;"href="
                          https://openreview.net/forum?id=pOpK3kUDJAc                    ">
                          Benchmarking Bird's Eye View Detection Robustness to Real-World Corruptions                  </a></p>
                        <p class="paper_author" target="_blank"> 
                          Shaoyuan Xie, Lingdong Kong, Wenwei Zhang, Jiawei Ren, Liang Pan, Kai Chen, Ziwei Liu                          <br>  The paper can be found here: <a  href="
                          https://openreview.net/forum?id=pOpK3kUDJAc                    ">
                          ICLR 2023 Workshop SR4AD HYBRID                 </a>          </p>
      
                        <p class="paper_title"><a class = 'hover_color' style="color: #000;"href="
                          https://openreview.net/forum?id=qYU4v9zCk9                    ">
                          Improving Data Augmentation for Multi-Modality 3D Object Detection                  </a></p>
                        <p class="paper_author" target="_blank"> 
                          Wenwei Zhang
                          , Zhe Wang, Chen Change Loy                           <br>  The paper can be found here: <a  href="
                          https://openreview.net/forum?id=qYU4v9zCk9                    ">
                          ICLR 2023 Workshop SR4AD HYBRID                 </a>         </p>
      
                        <p class="paper_title"><a class = 'hover_color' style="color: #000;"href="
                          https://openreview.net/forum?id=LW3bRLlY-SA                    ">
                          aiMotive Dataset: A Multimodal Dataset for Robust Autonomous Driving with Long-Range Perception                  </a></p>
                        <p class="paper_author" target="_blank"> 
                          Tamas Matuszka                           <br>  The paper can be found here: <a  href="
                          https://openreview.net/forum?id=LW3bRLlY-SA                    ">
                          ICLR 2023 Workshop SR4AD HYBRID                 </a>         </p>
      
                        <p class="paper_title"><a class = 'hover_color' style="color: #000;"href="
                          https://openreview.net/forum?id=2_2wDwLB9Yd                    ">
                          SaFormer: A Conditional Sequence Modeling Approach to Offline Safe Reinforcement Learning                 </a></p>
                        <p class="paper_author" target="_blank"> 
                          Qin Zhang
                          , Linrui Zhang, Li Shen, Haoran Xu, Bowen Wang, Xueqian Wang, Bo Yuan, Yongzhe Chang                           <br>  The paper can be found here: <a  href="
                          https://openreview.net/forum?id=2_2wDwLB9Yd                    ">
                          ICLR 2023 Workshop SR4AD HYBRID                 </a>         </p>
      
                        <p class="paper_title"><a class = 'hover_color' style="color: #000;"href="
                          https://openreview.net/forum?id=vFmEwcUCRK                    ">
                          Prototypical Context-aware Dynamics Generalization for High-dimensional Model-based Reinforcement Learning                  </a></p>
                        <p class="paper_author" target="_blank"> 
                          Junjie Wang, Yao Mu, Dong Li, qichao Zhang, Dongbin Zhao, Yuzheng Zhuang, Ping Luo, Bin Wang, Jianye Hao                         <br>  The paper can be found here: <a  href="
                          https://openreview.net/forum?id=vFmEwcUCRK                    ">
                          ICLR 2023 Workshop SR4AD HYBRID                 </a>          </p>
      
                        <p class="paper_title"><a class = 'hover_color' style="color: #000;"href="
                          https://openreview.net/forum?id=2wzqshimR2w                    ">
                          Neural MPC-based Decision-making Framework for Autonomous Driving in Multi-Lane Roundabout                  </a></p>
                        <p class="paper_author" target="_blank"> 
                          Yao Mu, Zhiqian Lan, Chang Liu, Ping Luo, Shengbo Eben Li                         <br>  The paper can be found here: <a  href="
                          https://openreview.net/forum?id=2wzqshimR2w                    ">
                          ICLR 2023 Workshop SR4AD HYBRID                 </a>           </p>
                                          
                        <p class="paper_title"><a class = 'hover_color' style="color: #000;"href="
                          https://openreview.net/forum?id=NvkVhPifa4                    ">
                          Label Calibration for Semantic Segmentation Under Domain Shift                  </a></p>
                        <p class="paper_author" target="_blank"> 
                          Ondrej Bohdal
                          , Da Li, Timothy Hospedales                             <br>  The paper can be found here: <a  href="
                          https://openreview.net/forum?id=NvkVhPifa4                    ">
                          ICLR 2023 Workshop SR4AD HYBRID                 </a>       </p>
      
                        <p class="paper_title"><a class = 'hover_color' style="color: #000;"href="
                          https://openreview.net/forum?id=sLPlY6qQ3t1                    ">
                          Semi-Supervised LiDAR Semantic Segmentation with Spatial Consistency Training                  </a></p>
                        <p class="paper_author" target="_blank"> 
                          Lingdong Kong, Jiawei Ren, Liang Pan, Ziwei Liu                             <br>  The paper can be found here: <a  href="
                          https://openreview.net/forum?id=sLPlY6qQ3t1                    ">
                          ICLR 2023 Workshop SR4AD HYBRID                 </a>       </p>
      
                        <p class="paper_title"><a class = 'hover_color' style="color: #000;"href="
                          https://openreview.net/forum?id=BBJJzvYiHc                    ">
                          CRN: Camera Radar Net for Accurate, Robust, Efficient 3D Perception                  </a></p>
                        <p class="paper_author" target="_blank"> 
      
                          Youngseok Kim, Sanmin Kim, Juyeb Shin, Jun Won Choi, Dongsuk Kum                          <br>  The paper can be found here: <a  href="
                          https://openreview.net/forum?id=BBJJzvYiHc                    ">
                          ICLR 2023 Workshop SR4AD HYBRID                 </a>          </p>
      
                        <p class="paper_title"><a class = 'hover_color' style="color: #000;"href="
                          https://openreview.net/forum?id=A0_Q0lpFRb                    ">
                          Use TensorRT network definition API and plugin to deploy 3D object detection algorithm SE-SSD                  </a></p>
                        <p class="paper_author" target="_blank"> 
                          Jingyue Guo                           <br>  The paper can be found here: <a  href="
                          https://openreview.net/forum?id=A0_Q0lpFRb                    ">
                          ICLR 2023 Workshop SR4AD HYBRID                 </a>         </p>
                                          
                       
      
                        </ol>
      
      
      
      
            
      
            </div>
    
              </div>
            </div>
    
    
        </div></div>
  
  
  
  
    <!-- Origanizers -->
    <div class="wrapper row2" id="id6">
      <div class="hoc bg clear" id="organizers">
         
  
          <h1 class="h_title"><a href="#organizers">Organizers<img src="/ui2023/img/icon/link.png" class="title_link"/></a></h1>

          <div class="wrapper">
  
            <div class="grid">
         
                <li>
                  
                    <a href="https://lihongyang.info" target="_blank">
                  <div class="photo"><img  src="/ui2023/img/team/hongyang_li.jpg" alt="Hongyang Li" ></div>
                      
                      <h1>Hongyang Li</h1><p style="color: black;">Shanghai AI Lab</p>
  
                    </a>
                    
  
                  </span>
  
                </li>
             
                <li>
                  
                    <a href="https://mengyeren.com" target="_blank">
                       <div class="photo">     <img  src="/ui2023/img/team/mengye_ren.jpg" alt="Mengye Ren" ></div>
                      
  
                      <h1>Mengye Ren</h1><p style="color: black;">New York University
                      </p>
                    </a>
  
                    
                  </li>
           
           
                <li>
                  
                    <a href="https://scholar.google.ae/citations?user=ulZxvY0AAAAJ" target="_blank">
                       <div class="photo">     <img  src="/ui2023/img/team/li_chen.jpg" alt="Li Chen"></div>
                      
                      <h1>Li Chen</h1><p style="color: black;">Shanghai AI Lab</p>
                    </a>
  
  
                    
  
                  </li>
       
                <li>
                  
                    <a href="https://scholar.google.com/citations?user=dgYJ6esAAAAJ" target="_blank">
                     <div class="photo">     <img  src="/ui2023/img/team/chonghao_sima.jpg" alt="Chonghao Sima" ></div>
                      
  
                      <h1>Chonghao Sima</h1><p style="color: black;">Purdue University</p>
  
                    </a>
                    
                  </li>
  
                <li>
                  
                    <a href="https://kashyap7x.github.io" target="_blank">
                       <div class="photo">     <img  src="/ui2023/img/team/kashyap_chitta.jpg" alt="Kashyap Chitta" ></div>
                      
                      <h1>Kashyap Chitta</h1><p style="color: black;">University of Tübingen</p>
                    </a>
  
                    
                  </li>
              
                <li>
                  
                    <a href="https://sites.google.com/it-caesar.de/homepage" target="_blank">
                      <div class="photo">     <img  src="/ui2023/img/team/holger_caesar.jpg" alt="Holger Caesar"></div>
                      
                      <h1>Holger Caesar</h1><p style="color: black;">TU Delft</p>
                    </a>
  
                    
                  </li>

                  <li>
                    <a href="http://luoping.me" target="_blank">
                      <div class="photo">     
                        <img  src="/ui2023/img/team/ping_luo.jpg" alt="Ping Luo">
                      </div>
                      <h1>Ping Luo</h1>
                      <p style="color: black;">The University of Hong Kong</p>
                    </a>
                  </li>
          
            </div>
          </div>
  
        </div>
  
      </div>
    </div>

    <div class="wrapper row2" id="tttttt" style="display: none;"></div>
 
  

  
 
 




<script src="/ui2023/cvpr2023/js/jquery-3.1.1.min.js" type="text/javascript" charset="utf-8"></script> 
<script src="/ui2023/cvpr2023/js/all.js" type="text/javascript" charset="utf-8"></script>
<script src="/ui2023/js/top_event.js?111" type="text/javascript" charset="utf-8"></script>

<script>

var imgs = [
      "/ui2023/cvpr2023/img/g1.gif",
      "/ui2023/cvpr2023/img/g2.gif",
      "/ui2023/cvpr2023/img/g3.gif",
      "/ui2023/cvpr2023/img/g4.gif",
    ];

    var index = 0;
    var len = imgs.length;

    // 计数器
    var count = 0; //初始化已加载的图片为 0
    // var $progress = $(".progress");

    $.each(imgs, function(i, src) {
      // i 是索引
      // 每遍历一个元素就通过new 来实例化一个img对象
      var imgObj = new Image();

      $(imgObj).on("load error", function() {
        //  $progress.html() 是修改  <p class="progress"></p> 标签的百分比内容
        // ((count + 1) / len) * 100)+ "%" 是用已加载的图片数除以总图片数乘以 100，加上百分号来代表加载进度
        // Math.round() 是取整数

       
        // $progress.html(Math.round(((count + 1) / len) * 100) + "%");

        // 当加载图片数量大于或等于len-1时，隐藏掉loading页
        // console.log(imgs[i])
        $("#g"+i).attr("src", imgs[i]);
        $(".loading_box"+i).hide();
        // if (count >= len - 1) {
         
          
        // }

        count++;
      });
      // 加载完成后为图片的 src 赋值
      imgObj.src = src;
      
    });

    $(".btn").on("click", function() {
      if ($(this).data("btn") === "prev") {
        index--;
        if (index < 0) {
          index = 0;
        }
      } else {
        index++;
        if (index > len - 1) {
          index = len - 1;
        }
      }
      $("#img").attr("src", imgs[index]);
    }); 

</script>
  <script type="text/javascript">
    function iclr_time_zone(){
  
      var Time_hour_select = document.getElementsByName("time_hour_seleting");
      var data_select = document.getElementsByName("data_seleting");
      var o_time = document.getElementById("operator");
      var op_timezone=o_time.options[o_time.selectedIndex].value;//在下拉列表中选中选择的值
      console.log(op_timezone)
  
      // If you want to change the schedule, remember to change the list [Time_hour_select_base] in the function
      var Time_hour_select_base = [8, 9, 9, 10, 11, 11, 12, 12, 13, 14, 15, 15, 16, 16, 17]
      var date_select_base = [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5 ]
  
  
  
      for(var j=0; j < Time_hour_select.length; j++) { 
        Time_hour_select[j].innerHTML= adjust_number(change(Time_hour_select_base[j]) + change(op_timezone));
        data_select[j].innerHTML= adjust_number_data((change(Time_hour_select_base[j]) + change(op_timezone)), date_select_base[j]);
  
      }
  
  
  
    }
      function change(x){
        return parseFloat(x)
      }
  
      function adjust_number(x){
        if (parseFloat(x)>=parseFloat(24)){
          x = parseFloat(x)-parseFloat(24);
        }
        if (parseFloat(x)<parseFloat(0)){
          x = parseFloat(x)+parseFloat(24);
        }
        return parseFloat(x)
      }
  
      function adjust_number_data(x, y){
        if (parseFloat(x)>=parseFloat(24)){
          x = parseFloat(x)-parseFloat(24);
          y = parseFloat(y)+parseFloat(1);
        }
        if (parseFloat(x)<parseFloat(0)){
          x = parseFloat(x)+parseFloat(24);
          y = parseFloat(y)-parseFloat(1);
  
        }
        return parseFloat(y)
      }
      
    </script>
    <div style="display: none;"><script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=TekEjFE3OgWzkGd07D-UhfKuP_l7nKNnSPy_r8-wIeA&cl=ffffff&w=a"></script></div>
</body>
</html>