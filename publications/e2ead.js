const e2ead = [
    {
        title: "Reasoning Multi-Agent Behavioral Topology for Interactive Autonomous Driving",
        link: "https://arxiv.org/abs/2409.18031",
        image: "/assets/publication/betop.jpg",
        author: "Haochen Liu, Li Chen, Yu Qiao, Chen Lv, Hongyang Li",
        note: "NeurIPS 2024",
        noteoption: '',
        star: "https://img.shields.io/github/stars/OpenDriveLab/BeTop?style=social",
        starlink: "https://github.com/OpenDriveLab/BeTop",
        icon: [
            {
                type: "github",
                link: "https://github.com/OpenDriveLab/BeTop",
            },
        ],
        description: "",
        tag: "",
    },
    {
        title: "NAVSIM: Data-Driven Non-Reactive Autonomous Vehicle Simulation and Benchmarking",
        link: "https://arxiv.org/abs/2406.15349",
        image: "/assets/publication/navsim.jpg",
        author: "Daniel Dauner, Marcel Hallgarten, Tianyu Li, Xinshuo Weng, Zhiyu Huang, Zetong Yang, Hongyang Li, Igor Gilitschenski, Boris Ivanovic, Marco Pavone, Andreas Geiger, Kashyap Chitta",
        note: "NeurIPS 2024 Track Datasets and Benchmarks",
        noteoption: '',
        star: "https://img.shields.io/github/stars/autonomousvision/navsim?style=social",
        starlink: "https://github.com/autonomousvision/navsim",
        icon: [
            {
                type: "github",
                link: "https://github.com/autonomousvision/navsim",
            },
        ],
        description: "Data-Driven Non-Reactive Autonomous Vehicle Simulation and Benchmarking.",
        tag: "",
    },
    {
        title: "Generalized Predictive Model for Autonomous Driving",
        link: "https://openaccess.thecvf.com/content/CVPR2024/html/Yang_Generalized_Predictive_Model_for_Autonomous_Driving_CVPR_2024_paper.html",
        image: "/assets/publication/genad.jpg",
        author: "Jiazhi Yang, Shenyuan Gao, Yihang Qiu, Li Chen, Tianyu Li, Bo Dai, Kashyap Chitta, Penghao Wu, Jia Zeng, Ping Luo, Jun Zhang, Andreas Geiger, Yu Qiao, Hongyang Li",
        note: "CVPR 2024 Highlight",
        noteoption: '',
        star: "https://img.shields.io/github/stars/OpenDriveLab/DriveAGI?style=social",
        starlink: "https://github.com/OpenDriveLab/DriveAGI",
        icon: [
            {
                type: "github",
                link: "https://github.com/OpenDriveLab/DriveAGI",
            },
            {
                type: "dataset",
                link: "https://github.com/OpenDriveLab/DriveAGI/tree/main/opendv",
            },
            {
                type: "youtube",
                link: "https://www.youtube.com/watch?v=a4H6Jj-7IC0",
            },
            {
                type: "bilibili",
                link: "https://www.bilibili.com/video/BV1gXCGYMEYV/",
            },
            {
                type: "medium",
                link: "https://medium.com/@opendrivelab/towards-next-level-of-autonomous-driving-via-world-models-aff0eb7fee00",
            },
            {
                type: "slides",
                link: "https://opendrivelab.github.io/content/GenAD_slides_with_vista.pdf",
            },
        ],
        description: "We aim to establish a generalized video prediction paradigm for autonomous driving by presenting the largest multimodal driving video dataset to date, OpenDV-2K, and a generative model that predicts the future given past visual and textual input, GenAD.",
        tag: "",
    },
    {
        title: "Visual Point Cloud Forecasting enables Scalable Autonomous Driving",
        link: "https://openaccess.thecvf.com/content/CVPR2024/html/Yang_Visual_Point_Cloud_Forecasting_enables_Scalable_Autonomous_Driving_CVPR_2024_paper.html",
        image: "/assets/publication/vidar.jpg",
        author: "Zetong Yang, Li Chen, Yanan Sun, Hongyang Li",
        note: "CVPR 2024 Highlight",
        noteoption: '',
        star: "https://img.shields.io/github/stars/OpenDriveLab/ViDAR?style=social",
        starlink: "https://github.com/OpenDriveLab/ViDAR",
        icon: [
            {
                type: "github",
                link: "https://github.com/OpenDriveLab/ViDAR",
            },
        ],
        description: "A new self-supervised pre-training task for end-to-end autonomous driving, predicting future point clouds from historical visual inputs, joint modeling the 3D geometry and temporal dynamics for simultaneous perception, prediction, and planning.",
        tag: "",
    },
    {
        title: "自動駕駛開源數據體系：現狀與未來",
        link: "http://engine.scichina.com/doi/10.1360/SSI-2023-0313",
        image: "/assets/publication/datasurvey.jpg",
        author: "Hongyang Li, Yang Li, Huijie Wang, Jia Zeng, Huilin Xu, Pinlong Cai, Li Chen, Junchi Yan, Feng Xu, Lu Xiong, Jingdong Wang, Futang Zhu, Kai Yan, Chunjing Xu, Tiancai Wang, Fei Xia, Beipeng Mu, Zhihui Peng, Dahua Lin, Yu Qiao",
        note: "中國科學：信息科學",
        noteoption: '',
        star: "https://img.shields.io/github/stars/OpenDriveLab/DriveAGI?style=social",
        starlink: "https://github.com/OpenDriveLab/DriveAGI",
        icon: [
            {
                type: "github",
                link: "https://github.com/OpenDriveLab/DriveAGI",
            },
            {
                type: "researchgate",
                link: "https://www.researchgate.net/publication/375331218_Open-sourced_Data_Ecosystem_in_Autonomous_Driving_the_Present_and_Future?channel=doi&linkId=65467b44ce88b87031c4ceab&showFulltext=true",
            },
            {
                type: "arxiv",
                link: "https://arxiv.org/abs/2312.03408",
            },
        ],
        description: "",
        tag: "",
    },
    {
        title: "Embodied Understanding of Driving Scenarios",
        link: "https://arxiv.org/abs/2403.04593",
        image: "/assets/publication/elm.jpg",
        author: "Yunsong Zhou, Linyan Huang, Qingwen Bu, Jia Zeng, Tianyu Li, Hang Qiu, Hongzi Zhu, Minyi Guo, Yu Qiao, Hongyang Li",
        note: "ECCV 2024",
        noteoption: '',
        star: "https://img.shields.io/github/stars/OpenDriveLab/ELM?style=social",
        starlink: "https://github.com/OpenDriveLab/ELM",
        icon: [
            {
                type: "webpage",
                link: "https://opendrivelab.github.io/elm.github.io/",
            },
            {
                type: "github",
                link: "https://github.com/OpenDriveLab/ELM",
            },
        ],
        description: "Revive driving scene understanding by delving into the embodiment philosophy.",
        tag: "",
    },
    {
        title: "DriveAdapter: Breaking the Coupling Barrier of Perception and Planning in End-to-End Autonomous Driving",
        link: "https://openaccess.thecvf.com/content/ICCV2023/html/Jia_DriveAdapter_Breaking_the_Coupling_Barrier_of_Perception_and_Planning_in_ICCV_2023_paper.html",
        image: "/assets/publication/driveadapted.jpg",
        author: "Xiaosong Jia, Yulu Gao, Li Chen, Junchi Yan, Patrick Langechuan Liu, Hongyang Li",
        note: "ICCV 2023 Oral",
        noteoption: '',
        star: "https://img.shields.io/github/stars/OpenDriveLab/DriveAdapter?style=social",
        starlink: "https://github.com/OpenDriveLab/DriveAdapter",
        icon: [
            {
                type: "github",
                link: "https://github.com/OpenDriveLab/DriveAdapter",
            },
        ],
        description: "A new paradigm for end-to-end autonomous driving without causal confusion issue.",
        tag: "",
    },
    {
        title: "Think Twice before Driving: Towards Scalable Decoders for End-to-End Autonomous Driving",
        link: "https://openaccess.thecvf.com/content/CVPR2023/html/Jia_Think_Twice_Before_Driving_Towards_Scalable_Decoders_for_End-to-End_Autonomous_CVPR_2023_paper.html",
        image: "/assets/publication/thinktwice.jpg",
        author: "Xiaosong Jia, Penghao Wu, Li Chen, Jiangwei Xie, Conghui He, Junchi Yan, Hongyang Li",
        note: "CVPR 2023",
        noteoption: '',
        star: "https://img.shields.io/github/stars/opendrivelab/thinktwice?style=social",
        starlink: "https://github.com/OpenDriveLab/ThinkTwice",
        icon: [
            {
                type: "github",
                link: "https://github.com/OpenDriveLab/ThinkTwice",
            },
        ],
        description: "A scalable decoder paradigm that generates the future trajectory and action of the ego vehicle for end-to-end autonomous driving.",
        tag: "",
    },
    {
        title: "LLM4Drive: A Survey of Large Language Models for Autonomous Driving",
        link: "https://arxiv.org/abs/2311.01043",
        image: "/assets/publication/llm4drive.jpg",
        author: "Zhenjie Yang, Xiaosong Jia, Hongyang Li, Junchi Yan",
        note: "arXiv 2023",
        noteoption: '',
        star: "https://img.shields.io/github/stars/Thinklab-SJTU/Awesome-LLM4AD?style=social",
        starlink: "https://github.com/Thinklab-SJTU/Awesome-LLM4AD",
        icon: [
            {
                type: "github",
                link: "https://github.com/Thinklab-SJTU/Awesome-LLM4AD",
            },
        ],
        description: "A collection of research papers about LLM-for-Autonomous-Driving (LLM4AD).",
        tag: "",
    },
    {
        title: "Policy Pre-Training for End-to-End Autonomous Driving via Self-Supervised Geometric Modeling",
        link: "https://arxiv.org/abs/2301.01006",
        image: "/assets/publication/ppgeo.jpg",
        author: "Penghao Wu, Li Chen, Hongyang Li, Xiaosong Jia, Junchi Yan, Yu Qiao",
        note: "ICLR 2023",
        noteoption: '',
        star: "https://img.shields.io/github/stars/opendrivelab/ppgeo?style=social",
        starlink: "https://github.com/opendrivelab/ppgeo",
        icon: [
            {
                type: "github",
                link: "https://github.com/opendrivelab/ppgeo",
            },
            {
                type: "slides",
                link: "https://docs.google.com/presentation/d/1d0MGh3XCxuZujtYgZ0sr6xsAKZ4uS50p/edit?usp=sharing&ouid=118212253182146260973&rtpof=true&sd=true",
            },
        ],
        description: "An intuitive and straightforward fully self-supervised framework curated for the policy pre-training in visuomotor driving.",
        tag: "",
    },
    {
        title: "Trajectory-guided Control Prediction for End-to-end Autonomous Driving: A Simple yet Strong Baseline",
        link: "https://proceedings.neurips.cc/paper_files/paper/2022/hash/286a371d8a0a559281f682f8fbf89834-Abstract-Conference.html",
        image: "/assets/publication/tcp.jpg",
        author: "Penghao Wu, Xiaosong Jia, Li Chen, Junchi Yan, Hongyang Li, Yu Qiao",
        note: "NeurIPS 2022",
        noteoption: '',
        star: "https://img.shields.io/github/stars/OpenDriveLab/TCP?style=social",
        starlink: "https://github.com/OpenDriveLab/TCP",
        icon: [
            {
                type: "github",
                link: "https://github.com/OpenDriveLab/TCP",
            },
            {
                type: "zhihu",
                link: "https://zhuanlan.zhihu.com/p/532665469",
            },
        ],
        description: "Take the initiative to explore the combination of controller based on a planned trajectory and perform control prediction.",
        tag: ["[Carla First Place]"],
    },
    {
        title: "ST-P3: End-to-End Vision-Based Autonomous Driving via Spatial-Temporal Feature Learning",
        link: "https://link.springer.com/chapter/10.1007/978-3-031-19839-7_31",
        image: "/assets/publication/stp3.jpg",
        author: "Shengchao Hu, Li Chen, Penghao Wu, Hongyang Li, Junchi Yan, Dacheng Tao",
        note: "ECCV 2022",
        noteoption: '',
        star: "https://img.shields.io/github/stars/OpenDriveLab/st-p3?style=social",
        starlink: "https://github.com/OpenDriveLab/st-p3",
        icon: [
            {
                type: "github",
                link: "https://github.com/OpenDriveLab/st-p3",
            },
            {
                type: "zhihu",
                link: "https://zhuanlan.zhihu.com/p/544387122",
            },
        ],
        description: "A spatial-temporal feature learning scheme towards a set of more representative features for perception, prediction and planning tasks simultaneously.",
        tag: "",
    },
];



function e2eadrender() {
    const homepubbody = document.getElementById("pub_end_to_end_ad");
    homepubbody.innerHTML = "";
    e2ead.forEach((item, _) => {
        var innerHTML = `
            <a href="${item.link}" target="_blank" class="hover:opacity-70 flex flex-row laptop:flex-col justify-center">
                <img loading="lazy" src="${item.image}" class="w-full tablet:w-5/6 laptop:w-96"/>
            </a>
            <div class="flex flex-col justify-center flex-1">
                <h3>
                    <a class="hover:text-o-blue" href=${item.link} target="_blank">
                        ${item.title}
                    </a>
                </h3>
                <p class="mt-3 authors">
                    ${item.author}
                </p>
                <div class="flex flex-row gap-6 flex-wrap justify-items-center mt-6">
                    <p class="bg-o-blue text-white p-1 pl-3 pr-3 rounded-3xl">
                        <a ${item.noteoption}>${item.note}</a>
                    </p>
                </div>
                <div class="flex flex-row gap-6 flex-wrap justify-items-center mt-6">
        `;
        if (item.star != "") {
            innerHTML += `
                    <a href="${item.starlink}" target="_blank"><img loading="lazy" src="${item.star}" class="h-8 hover:opacity-70"/></a>
            `;
        }
        item.icon.forEach((i, _) => {
            if (i.type != "github") {
                innerHTML += `                
                    <a href=${i.link} target="_blank"> 
                        <img loading="lazy" src="/assets/icon/${i.type}.png" class="h-8 hover:opacity-70"/> 
                    </a>
                `;
            };
        });
        innerHTML += `
                </div>
                <i class="mt-6 text-o-gray">
        `;
        if (item.tag != "") {
            item.tag.forEach((i, _) => {
                innerHTML += `
                <code>${i}</code>
                `;
            });
        };
        innerHTML += `
                    ${item.description}
                </i>
            </div>
        `;
        const pub = document.createElement("div");
        pub.className = "flex flex-col laptop:flex-row gap-6 laptop:gap-20";
        pub.innerHTML = innerHTML;
        homepubbody.appendChild(pub);
    });
}
