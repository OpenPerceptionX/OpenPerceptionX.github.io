<head>
    <title>Publication | OpenDriveLab</title>
    <meta name="keywords" content="OpenDriveLab, Publication, Autonomous Driving, Autonomous Agent, Embodied AI">
    <meta name="description" content="OpenDriveLab is committed to exploring cutting-edge autonomous driving technology, launching a series of benchmarking work, open source to serve the community, and promote the common development of the industry. Friends who are committed to making influential research are welcome to join!">

    <link rel="icon" type="image/png" href="/assets/icon/D_small.png">

    <link href="/ui2024/css/format.css" rel="stylesheet" type="text/css" media="all"/>
    <link href="/ui2024/css/font.css" rel="stylesheet" type="text/css" media="all"/>
    <link href="/ui2024/css/banner.css" rel="stylesheet" type="text/css" media="all"/>
    <link href="/ui2024/css/sticky.css" rel="stylesheet" type="text/css" media="all"/>

    <script src="/ui2024/js/jquery-3.7.1.min.js" integrity="sha256-/JqT3SQfawRcv/BIHPThkBvs0OEvtFFmqPF/lYI/Cxo=" crossorigin="anonymous"></script>
</head>



<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-L7VEDHS6G8"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-L7VEDHS6G8');
</script>



<script>
    $.get("/ui2024/components/header.html",function(result){
        $("body").prepend(result)
    })
    $.get("/ui2024/components/footer.html",function(result){
        $("body").append(result)
    })
</script>



<script>
    window.addEventListener("scroll", function () {
        if (sticky.getBoundingClientRect().top <= 64) {
            sticky.classList.add("sticky_top");
            header.style.display = "none";
        } else {
            sticky.classList.remove("sticky_top");
            header.style.display = "block";
        }
    })
</script>



<script>
    var url_list = [{
        name: 'Li Chen',
        url: 'https://scholar.google.com/citations?user=ulZxvY0AAAAJ'
    }, {
        name: 'Penghao Wu',
        url: 'https://scholar.google.com/citations?user=9mssd5EAAAAJ'
    }, {
        name: 'Chonghao Sima',
        url: 'https://scholar.google.com/citations?user=dgYJ6esAAAAJ'
    }, {
        name: 'Hongyang Li',
        url: 'https://scholar.google.com/citations?user=Hfrih1EAAAAJ'
    }, {
        name: 'Xiaosong Jia',
        url: 'https://scholar.google.com/citations?user=JeFQwxUAAAAJ'
    }, {
        name: 'Junchi Yan',
        url: 'https://scholar.google.com/citations?user=ga230VoAAAAJ'
    }, {
        name: 'Andreas Geiger',
        url: 'https://scholar.google.com/citations?user=SrVnrPcAAAAJ'
    }, {
        name: 'Yu Qiao',
        url: 'https://scholar.google.com/citations?user=gFtI-8QAAAAJ'
    }, {
        name: 'Huijie Wang',
        url: 'https://scholar.google.com/citations?user=Xg4cp-EAAAAJ'
    }, {
        name: 'Jiazhi Yang',
        url: 'https://scholar.google.com/citations?user=Ju7nGX8AAAAJ'
    }, {
        name: 'Zetong Yang',
        url: 'https://scholar.google.com/citations?user=oPiZSVYAAAAJ'
    }, {
        name: 'Bangjun Wang',
        url: 'https://scholar.google.com/citations?user=_LeSlzUAAAAJ'
    }, {
        name: 'Kashyap Chitta',
        url: 'https://scholar.google.com/citations?user=vX5i2CcAAAAJ'
    }, {
        name: 'Ping Luo',
        url: 'https://scholar.google.com/citations?user=aXdjxb4AAAAJ'
    }, {
        name: 'Tianyu Li',
        url: 'https://scholar.google.com/citations?user=X6vTmEMAAAAJ'
    },];
    function add_url(a) {
        var len = url_list.length;

        var h = a.toString();
        for (var i = 0; i < len; i++) {
        if (url_list[i]["name"] == h) {
            var htm = '<a href="' + url_list[i]["url"] + '" target="_blank" class="Odarkblue">' + h + '</a>';
            var h = htm;
        }
        if (h == "et al.") {
            var htm = '<i>et al.</i>';
            var h = htm;
        }
        }
        return h
    }
  
    function cur(a) {
        var h = a.split(",")
        var len = h.length;
        var htm = ""
        for (var i = 0; i < len; i++) {
            var fh = (i == 0) ? "" : ", "
            htm += fh + add_url(h[i].replace(/^\s+|\s+$/g, ""))
        }
        return htm
    }

    addEventListener("load", () => {
        var authors_lists = document.getElementsByClassName("authors");
        for (var i = 0; i < authors_lists.length; i++) {
            authors_list = authors_lists[i];
            authors_list.innerHTML = cur(authors_list.textContent);
        }
    })
</script>



<body>

    

    <div class="banner">
        <div class="banner_container">
            <div class="banner_title_en">
                <h1 class="Owhite">Publication</h1>
            </div>
            <div class="banner_subtitle_en">
                <h6 class="Owhite" id="editor_pick">We position OpenDriveLab as one of the most top research teams around globe, since we've got talented people and published work at top venues.</h6>
            </div>
        </div>
    </div>



    <div class="block sticky" id="sticky">
        <div class="sticky_row">
            <a href="#editor_pick">Editor's Pick</a>
            <a></a>
            <a></a>
            <a href="#" class="image"><img src="/assets/icon/top_white.png"/></a>
        </div>
        <div class="navigator"></div>
        <div class="sticky_row">
            <a href="#end_to_end_ad">End-to-End AD</a>
            <a href="#bev_perception">BEV Perception</a>
            <a href="#prediction_and_planning">Prediction and Planning</a>
            <a href="#cv_at_scale">CV at Scale</a>
        </div>
    </div>



    <br><br><br>



    <div class="block_left" style="margin-bottom: 0; padding-bottom: 0;">
        <div style="flex-grow: 1;">
            <a class="h1_title" href="#editor_pick">
                <h1>
                    Editor's Pick
                    <img loading="lazy" class="link_img"/>
                </h1>
            </a>
        </div> 
    </div>
    <br><br>
    <div class="block_left" style="margin-top: 0; padding-top: 0">
        <div style="flex-grow: 1;">

            <div class="publication_container">

                <div>
                    <div class="publication_container_first">
                        <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Hu_Planning-Oriented_Autonomous_Driving_CVPR_2023_paper.html" target="_blank"><img loading="lazy" src="/assets/publication/uniad.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Hu_Planning-Oriented_Autonomous_Driving_CVPR_2023_paper.html" target="_blank">Planning-oriented Autonomous Driving</a></h2>
                        <span class="authors">Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Hongyang Li, <i>et al.</i></span>
                        <div>
                            <div>
                                <span class="Odarkblue"><a href="https://cvpr2023.thecvf.com/Conferences/2023/Awards" target="_blank" style="text-decoration: underline;">CVPR 2023 Best Paper Award</a></span>
                            </div>
                            <div>
                                <a href="https://github.com/OpenDriveLab/UniAD" target="_blank"><img loading="lazy" src="https://img.shields.io/github/stars/OpenDriveLab/UniAD?style=social"/></a>
                                <a href="https://zhuanlan.zhihu.com/p/638780421" target="_blank"><img loading="lazy" src="/assets/icon/zhihu.png"/></a>
                                <a href="https://mp.weixin.qq.com/s?__biz=MzkyMDUzMDE2Mw==&mid=2247485809&idx=1&sn=5dbd00380bcf80cc32d11b6e15e93829" target="_blank"><img loading="lazy" src="/assets/icon/article.png"/></a>
                                <a href="https://www.youtube.com/watch?v=cyrxJJ_nnaQ" target="_blank"><img loading="lazy" src="/assets/icon/youtube.png"/></a>
                                <a href="/e2ead/UniAD_plenary_talk_slides.pdf" target="_blank"><img loading="lazy" src="/assets/icon/slides.png"/></a>
                            </div>
                        </div>
                        <span class="gray"><i>A comprehensive framework up-to-date that incorporates full-stack driving tasks in one network.</i></span>
                    </div>
                </div>
                <div>
                    <div class="publication_container_first">
                        <a href="https://arxiv.org/abs/2306.16927" target="_blank"><img loading="lazy" src="/assets/publication/e2esurvey.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://arxiv.org/abs/2306.16927" target="_blank">End-to-end Autonomous Driving: Challenges and Frontiers</a></h2>
                        <span class="authors">Li Chen, Penghao Wu, Kashyap Chitta, Bernhard Jaeger, Andreas Geiger, Hongyang Li</span>
                        <div>
                            <div>
                                <span class="Odarkblue">arXiv 2023</span>
                            </div>
                            <div>
                                <a href="https://github.com/OpenDriveLab/End-to-end-Autonomous-Driving" target="_blank"><img loading="lazy" src="https://img.shields.io/github/stars/OpenDriveLab/End-to-end-Autonomous-Driving?style=social"/></a>
                            </div>
                        </div>
                        <span class="gray"><i>In this survey, we provide a comprehensive analysis of more than 250 papers on the motivation, roadmap, methodology, challenges, and future trends in end-to-end autonomous driving.</i></span>
                    </div>
                </div>
                <div>
                    <div class="publication_container_first">
                        <a href="https://ieeexplore.ieee.org/document/10321736" target="_blank"><img loading="lazy" src="/assets/publication/bevsurvey.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://ieeexplore.ieee.org/document/10321736" target="_blank">Delving into the Devils of Bird's-Eye-View Perception: A Review, Evaluation and Recipe</a></h2>
                        <span class="authors">Hongyang Li, Chonghao Sima, Jifeng Dai, Wenhai Wang, Lewei Lu, <i>et al.</i></span>
                        <div>
                            <div>
                                <span class="Odarkblue">TPAMI 2023 <code>[Setup the Table]</code></span>
                            </div>
                            <div>
                                <a href="https://github.com/opendrivelab/bevperception-survey-recipe" target="_blank"><img loading="lazy" src="https://img.shields.io/github/stars/opendrivelab/bevperception-survey-recipe?style=social"/></a>
                            </div>
                        </div>
                        <span class="gray"><i>We review the most recent work on BEV perception and provide analysis of different solutions.</i></span>
                    </div>
                </div>
                <div>
                    <div class="publication_container_first">
                        <a href="https://openreview.net/forum?id=OMOOO3ls6g" target="_blank"><img loading="lazy" src="/assets/publication/openlanev2.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://openreview.net/forum?id=OMOOO3ls6g" target="_blank">OpenLane-V2: A Topology Reasoning Benchmark for Unified 3D HD Mapping</a></h2>
                        <span class="authors">Huijie Wang, Tianyu Li, Yang Li, Li Chen, Chonghao Sima, <i>et al.</i></span>
                        <div>
                            <div>
                                <span class="Odarkblue">NeurIPS 2023 Track Datasets and Benchmarks</span>
                            </div>
                            <div>
                                <a href="https://github.com/OpenDriveLab/OpenLane-V2" target="_blank"><img loading="lazy" src="https://img.shields.io/github/stars/opendrivelab/OpenLane-V2?style=social"/></a>
                            </div>
                        </div>
                        <span class="gray"><i>The world's first perception and reasoning benchmark for scene structure in autonomous driving.</i></span>
                    </div>
                </div>
                <div>
                    <div class="publication_container_first">
                        <a href="https://link.springer.com/chapter/10.1007/978-3-031-20077-9_1" target="_blank"><img loading="lazy" src="/assets/publication/bevformer.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://link.springer.com/chapter/10.1007/978-3-031-20077-9_1" target="_blank">BEVFormer: Learning Bird's-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers</a></h2>
                        <span class="authors">Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, <i>et al.</i></span>
                        <div>
                            <div>
                                <span class="Odarkblue">ECCV 2022 <code>[nuScenes First Place]</code> <code>[Waymo Challenge 2022 First Place]</code></span>
                            </div>
                            <div id="end_to_end_ad">
                                <a href="https://github.com/fundamentalvision/BEVFormer" target="_blank"><img loading="lazy" src="https://img.shields.io/github/stars/fundamentalvision/BEVFormer?style=social"/></a>
                                <a href="https://zhuanlan.zhihu.com/p/564295059" target="_blank"><img loading="lazy" src="/assets/icon/zhihu.png"/></a>
                            </div>
                        </div>
                        <span class="gray"><i>A paradigm for autonomous driving that applies both Transformer and Temporal structure to generate BEV features.</i></span>
                    </div>
                </div>

            </div>
        </div> 
    </div>



    <br>
    <div class="block" style="margin: 0;">
        <div class="navigator"></div>
    </div>
    <br><br><br>



    <div class="block_left" style="margin-bottom: 0; padding-bottom: 0;">
        <div style="flex-grow: 1;">
            <a class="h1_title" href="#end_to_end_ad">
                <h1>
                    End-to-End Autonomous Driving
                    <img loading="lazy" class="link_img"/>
                </h1>
            </a>
        </div> 
    </div>
    <br><br>
    <div class="block_left" style="margin-top: 0; padding-top: 0">
        <div style="flex-grow: 1;">
            <div class="publication_container">

                <div>
                    <div class="publication_container_first">
                        <a href="https://arxiv.org/abs/2312.14150" target="_blank"><img loading="lazy" src="/assets/publication/drivelm.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://arxiv.org/abs/2312.14150" target="_blank">DriveLM: Driving with Graph Visual Question Answering</a></h2>
                        <span class="authors">Chonghao Sima, Katrin Renz, Kashyap Chitta, Li Chen, Hanxue Zhang, Chengen Xie, Ping Luo, Andreas Geiger, Hongyang Li</span>
                        <div>
                            <div>
                                <span class="Odarkblue">arXiv 2024</span>
                            </div>
                            <div>
                                <a href="https://github.com/OpenDriveLab/DriveLM" target="_blank"><img loading="lazy" src="https://img.shields.io/github/stars/OpenDriveLab/DriveLM?style=social"/></a>
                                <a href="https://opendrivelab.com/DriveLM/" target="_blank"><img loading="lazy" src="/assets/icon/webpage.png"/></a>
                                <a href="https://huggingface.co/datasets/OpenDriveLab-org/DriveLM" target="_blank"><img loading="lazy" src="/assets/icon/hugging_face.png"/></a>
                            </div>
                        </div>
                    </div>
                </div>
                <div>
                    <div class="publication_container_first">
                        <a href="https://arxiv.org/abs/2311.01043" target="_blank"><img loading="lazy" src="/assets/publication/llm4drive.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://arxiv.org/abs/2311.01043" target="_blank">A Survey of Large Language Models for Autonomous Driving</a></h2>
                        <span class="authors">Zhenjie Yang, Xiaosong Jia, Hongyang Li, Junchi Yan</span>
                        <div>
                            <div>
                                <span class="Odarkblue">arXiv 2023</span>
                            </div>
                            <div>
                                <a href="https://github.com/Thinklab-SJTU/Awesome-LLM4AD" target="_blank"><img loading="lazy" src="https://img.shields.io/github/stars/Thinklab-SJTU/Awesome-LLM4AD?style=social"/></a>
                            </div>
                        </div>
                        <span class="gray"><i>A collection of research papers about LLM-for-Autonomous-Driving (LLM4AD).</i></span>
                    </div>
                </div>
                <div>
                    <div class="publication_container_first">
                        <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Jia_DriveAdapter_Breaking_the_Coupling_Barrier_of_Perception_and_Planning_in_ICCV_2023_paper.html" target="_blank"><img loading="lazy" src="/assets/publication/driveadapted.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://openaccess.thecvf.com/content/ICCV2023/html/Jia_DriveAdapter_Breaking_the_Coupling_Barrier_of_Perception_and_Planning_in_ICCV_2023_paper.html" target="_blank">DriveAdapter: Breaking the Coupling Barrier of Perception and Planning in End-to-End Autonomous Driving</a></h2>
                        <span class="authors">Xiaosong Jia, Yulu Gao, Li Chen, Junchi Yan, Patrick Langechuan Liu, Hongyang Li</span>
                        <div>
                            <div>
                                <span class="Odarkblue">ICCV 2023 (Oral)</span>
                            </div>
                            <div>
                                <a href="https://github.com/OpenDriveLab/DriveAdapter" target="_blank"><img loading="lazy" src="https://img.shields.io/github/stars/OpenDriveLab/DriveAdapter?style=social"/></a>
                            </div>
                        </div>
                        <span class="gray"><i>A new paradigm for end-to-end autonomous driving without causal confusion issue.</i></span>
                    </div>
                </div>
                <div>
                    <div class="publication_container_first">
                        <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Jia_Think_Twice_Before_Driving_Towards_Scalable_Decoders_for_End-to-End_Autonomous_CVPR_2023_paper.html" target="_blank"><img loading="lazy" src="/assets/publication/thinktwice.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Jia_Think_Twice_Before_Driving_Towards_Scalable_Decoders_for_End-to-End_Autonomous_CVPR_2023_paper.html" target="_blank">Think Twice before Driving: Towards Scalable Decoders for End-to-End Autonomous Driving</a></h2>
                        <span class="authors">Xiaosong Jia, Penghao Wu, Li Chen, <i>et al.</i></span>
                        <div>
                            <div>
                                <span class="Odarkblue">CVPR 2023</span>
                            </div>
                            <div>
                                <a href="https://github.com/OpenDriveLab/ThinkTwice" target="_blank"><img loading="lazy" src="https://img.shields.io/github/stars/opendrivelab/thinktwice?style=social"/></a>
                            </div>
                        </div>
                        <span class="gray"><i>A scalable decoder paradigm that generates the future trajectory and action of the ego vehicle for end-to-end autonomous driving.</i></span>
                    </div>
                </div>
                <div>
                    <div class="publication_container_first">
                        <a href="https://arxiv.org/abs/2301.01006" target="_blank"><img loading="lazy" src="/assets/publication/ppgeo.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://arxiv.org/abs/2301.01006" target="_blank">Policy Pre-Training for End-to-End Autonomous Driving via Self-Supervised Geometric Modeling</a></h2>
                        <span class="authors">Penghao Wu, Li Chen, Hongyang Li, Xiaosong Jia, Junchi Yan, Yu Qiao</span>
                        <div>
                            <div>
                                <span class="Odarkblue">ICLR 2023</span>
                            </div>
                            <div>
                                <a href="https://github.com/opendrivelab/ppgeo" target="_blank"><img loading="lazy" src="https://img.shields.io/github/stars/opendrivelab/ppgeo?style=social"/></a>
                                <a href="https://docs.google.com/presentation/d/1d0MGh3XCxuZujtYgZ0sr6xsAKZ4uS50p/edit?usp=sharing&ouid=118212253182146260973&rtpof=true&sd=true" target="_blank"><img loading="lazy" src="/assets/icon/slides.png"/></a>
                            </div>
                        </div>
                        <span class="gray"><i>An intuitive and straightforward fully self-supervised framework curated for the policy pre-training in visuomotor driving.</i></span>
                    </div>
                </div>
                <div>
                    <div class="publication_container_first">
                        <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/286a371d8a0a559281f682f8fbf89834-Abstract-Conference.html" target="_blank"><img loading="lazy" src="/assets/publication/tcp.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/286a371d8a0a559281f682f8fbf89834-Abstract-Conference.html" target="_blank">Trajectory-guided Control Prediction for End-to-end Autonomous Driving: A Simple yet Strong Baseline</a></h2>
                        <span class="authors">Penghao Wu, Xiaosong Jia, Li Chen, Junchi Yan, Hongyang Li, Yu Qiao</span>
                        <div>
                            <div>
                                <span class="Odarkblue">NeurIPS 2022 <code>[Carla First Place]</code></span>
                            </div>
                            <div>
                                <a href="https://github.com/OpenDriveLab/TCP" target="_blank"><img loading="lazy" src="https://img.shields.io/github/stars/OpenDriveLab/TCP?style=social"/></a>
                                <a href="https://zhuanlan.zhihu.com/p/532665469" target="_blank"><img loading="lazy" src="/assets/icon/zhihu.png"/></a>
                            </div>
                        </div>
                        <span class="gray"><i>Take the initiative to explore the combination of controller based on a planned trajectory and perform control prediction.</i></span>
                    </div>
                </div>
                <div>
                    <div class="publication_container_first">
                        <a href="https://link.springer.com/chapter/10.1007/978-3-031-19839-7_31" target="_blank"><img loading="lazy" src="/assets/publication/stp3.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://link.springer.com/chapter/10.1007/978-3-031-19839-7_31" target="_blank">ST-P3: End-to-end Vision-based Autonomous Driving via Spatial-Temporal Feature Learning</a></h2>
                        <span class="authors">Shengchao Hu, Li Chen, Penghao Wu, Hongyang Li, <i>et al.</i></span>
                        <div>
                            <div>
                                <span class="Odarkblue">ECCV 2022</span>
                            </div>
                            <div id="bev_perception">
                                <a href="https://github.com/OpenDriveLab/st-p3" target="_blank"><img loading="lazy" src="https://img.shields.io/github/stars/OpenDriveLab/st-p3?style=social"/></a>
                                <a href="https://zhuanlan.zhihu.com/p/544387122" target="_blank"><img loading="lazy" src="/assets/icon/zhihu.png"/></a>
                            </div>
                        </div>
                        <span class="gray"><i>A spatial-temporal feature learning scheme towards a set of more representative features for perception, prediction and planning tasks simultaneously.</i></span>
                    </div>
                </div>

            </div>
        </div> 
    </div>



    <br>
    <div class="block" style="margin: 0;">
        <div class="navigator"></div>
    </div>
    <br><br><br>



    <div class="block_left" style="margin-bottom: 0; padding-bottom: 0;">
        <div style="flex-grow: 1;">
            <a class="h1_title" href="#bev_perception">
                <h1>
                    Bird's-Eye-View Perception
                    <img loading="lazy" class="link_img"/>
                </h1>
            </a>
        </div> 
    </div>
    <br><br>
    <div class="block_left" style="margin-top: 0; padding-top: 0">
        <div style="flex-grow: 1;">
            <div class="publication_container">

                <div>
                    <div class="publication_container_first">
                        <a href="https://arxiv.org/abs/2312.17655" target="_blank"><img loading="lazy" src="/assets/publication/vidar.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://arxiv.org/abs/2312.17655" target="_blank">Visual Point Cloud Forecasting enables Scalable Autonomous Driving</a></h2>
                        <span class="authors">Zetong Yang, Li Chen, Yanan Sun, Hongyang Li</span>
                        <div>
                            <div>
                                <span class="Odarkblue">CVPR 2024</span>
                            </div>
                            <div>
                                <a href="https://github.com/OpenDriveLab/ViDAR" target="_blank"><img loading="lazy" src="https://img.shields.io/github/stars/OpenDriveLab/ViDAR?style=social"/></a>
                            </div>
                        </div>
                        <span class="gray"><i>A new self-supervised pre-training task for end-to-end autonomous driving, predicting future point clouds from historical visual inputs, joint modeling the 3D geometry and temporal dynamics for simultaneous perception, prediction, and planning.</i></span>
                    </div>
                </div>
                <div>
                    <div class="publication_container_first">
                        <a href="https://arxiv.org/abs/2312.16108" target="_blank"><img loading="lazy" src="/assets/publication/lanesegnet.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://arxiv.org/abs/2312.16108" target="_blank">LaneSegNet: Map Learning with Lane Segment Perception for Autonomous Driving</a></h2>
                        <span class="authors">Tianyu Li, Peijin Jia, Bangjun Wang, Li Chen, Kun Jiang, Junchi Yan, Hongyang Li</span>
                        <div>
                            <div>
                                <span class="Odarkblue">ICLR 2024</span>
                            </div>
                            <div>
                                <a href="https://github.com/OpenDriveLab/LaneSegNet" target="_blank"><img loading="lazy" src="https://img.shields.io/github/stars/opendrivelab/LaneSegNet?style=social"/></a>
                            </div>
                        </div>
                        <span class="gray"><i>We advocate Lane Segment as a map learning paradigm that seamlessly incorporates both map geometry and topology information.</i></span>
                    </div>
                </div>
                <div>
                    <div class="publication_container_first">
                        <a href="https://arxiv.org/abs/2310.15670" target="_blank"><img loading="lazy" src="/assets/publication/vcd.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://arxiv.org/abs/2310.15670" target="_blank">Leveraging Vision-Centric Multi-Modal Expertise for 3D Object Detection</a></h2>
                        <span class="authors">Linyan Huang, Zhiqi Li, Chonghao Sima, Wenhai Wang, Jingdong Wang, Yu Qiao, Hongyang Li</span>
                        <div>
                            <div>
                                <span class="Odarkblue">NeurIPS 2023</span>
                            </div>
                            <div>
                                <a href="https://github.com/OpenDriveLab/Birds-eye-view-Perception" target="_blank"><img loading="lazy" src="https://img.shields.io/github/stars/opendrivelab/Birds-eye-view-Perception?style=social"/></a>
                            </div>
                        </div>
                        <span class="gray"><i>The unified framework to enhance 3D object detection by uniting a multi-modal expert model with a trajectory distillation module.</i></span>
                    </div>
                </div>
                <div>
                    <div class="publication_container_first">
                        <a href="https://arxiv.org/abs/2306.02851" target="_blank"><img loading="lazy" src="/assets/publication/occnet.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://arxiv.org/abs/2306.02851" target="_blank">Scene as Occupancy</a></h2>
                        <span class="authors">Chonghao Sima, Wenwen Tong, Tai Wang, Li Chen, Silei Wu, Hanming Deng, Yi Gu, Lewei Lu, Ping Luo, Dahua Lin, Hongyang Li</span>
                        <div>
                            <div>
                                <span class="Odarkblue">ICCV 2023</span>
                            </div>
                            <div>
                                <a href="https://github.com/OpenDriveLab/OccNet" target="_blank"><img loading="lazy" src="https://img.shields.io/github/stars/opendrivelab/OccNet?style=social"/></a>
                            </div>
                        </div>
                        <span class="gray"><i>Occupancy serves as a general representation of the scene and could facilitate perception and planning in the full-stack of autonomous driving.</i></span>
                    </div>
                </div>
                <div>
                    <div class="publication_container_first">
                        <a href="https://arxiv.org/abs/2304.05277" target="_blank"><img loading="lazy" src="/assets/publication/toponet.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://arxiv.org/abs/2304.05277" target="_blank">Graph-based Topology Reasoning for Driving Scenes</a></h2>
                        <span class="authors">Tianyu Li, Li Chen, Huijie Wang, Yang Li, Jiazhi Yang, Xiangwei Geng, <i>et al.</i></span>
                        <div>
                            <div>
                                <span class="Odarkblue">arXiv 2023</span>
                            </div>
                            <div>
                                <a href="https://github.com/OpenDriveLab/TopoNet" target="_blank"><img loading="lazy" src="https://img.shields.io/github/stars/opendrivelab/TopoNet?style=social"/></a>
                            </div>
                        </div>
                        <span class="gray"><i>A new baseline for scene topology reasoning, which unifies heterogeneous feature learning and enhances feature interactions via the graph neural network architecture and the knowledge graph design.</i></span>
                    </div>
                </div>
                <div>
                    <div class="publication_container_first">
                        <a href="https://arxiv.org/abs/2304.04179" target="_blank"><img loading="lazy" src="/assets/publication/sparse_yulu.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://arxiv.org/abs/2304.04179" target="_blank">Sparse Dense Fusion for 3D Object Detection</a></h2>
                        <span class="authors">Yulu Gao, Chonghao Sima, Shaoshuai Shi, Shangzhe Di, Si Liu, Hongyang Li</i></span>
                        <div>
                            <div>
                                <span class="Odarkblue">IROS 2023</span>
                            </div>
                        </div>
                        <span class="gray"><i>We propose Sparse Dense Fusion (SDF), a complementary framework that incorporates both sparse-fusion and dense-fusion modules via the Transformer architecture.</i></span>
                    </div>
                </div>
                <div>
                    <div class="publication_container_first">
                        <a href="https://arxiv.org/abs/2303.10340" target="_blank"><img loading="lazy" src="/assets/publication/3daug.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://arxiv.org/abs/2303.10340" target="_blank">3D Data Augmentation for Driving Scenes on Camera</a></h2>
                        <span class="authors">Wenwen Tong, Jiangwei Xie, Tianyu Li, Hanming Deng, Xiangwei Geng, Ruoyi Zhou, Dingchen Yang, Bo Dai, Lewei Lu, Hongyang Li</i></span>
                        <div>
                            <div>
                                <span class="Odarkblue">arXiv 2023</span>
                            </div>
                        </div>
                        <span class="gray"><i>We propose a 3D data augmentation approach termed Drive-3DAug to augment the driving scenes on camera in the 3D space.</i></span>
                    </div>
                </div>
                <div>
                    <div class="publication_container_first">
                        <a href="https://arxiv.org/abs/2304.03105" target="_blank"><img loading="lazy" src="/assets/publication/gapretrain.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://arxiv.org/abs/2304.03105" target="_blank">Geometric-aware Pretraining for Vision-centric 3D Object Detection</a></h2>
                        <span class="authors">Linyan Huang, Huijie Wang, Jia Zeng, Shengchuan Zhang, Liujuan Cao, Rongrong Ji, Junchi Yan, Hongyang Li</i></span>
                        <div>
                            <div>
                                <span class="Odarkblue">arXiv 2023</span>
                            </div>
                        </div>
                        <span class="gray"><i>We propose GAPretrain, a plug-and-play framework that boosts 3D detection by pretraining with spatial-structural cues and BEV representation.</i></span>
                    </div>
                </div>
                <div>
                    <div class="publication_container_first">
                        <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yang_BEVFormer_v2_Adapting_Modern_Image_Backbones_to_Birds-Eye-View_Recognition_via_CVPR_2023_paper.html" target="_blank"><img loading="lazy" src="/assets/publication/bevformerv2.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Yang_BEVFormer_v2_Adapting_Modern_Image_Backbones_to_Birds-Eye-View_Recognition_via_CVPR_2023_paper.html" target="_blank">BEVFormer v2: Adapting Modern Image Backbones to Bird's-Eye-View Recognition via Perspective Supervision</a></h2>
                        <span class="authors">Chenyu Yang, Xizhou Zhu, Hongyang Li, Jifeng Dai, <i>et al.</i></span>
                        <div>
                            <div>
                                <span class="Odarkblue">CVPR 2023 Highlight</span>
                            </div>
                        </div>
                        <span class="gray"><i>A novel bird's-eye-view (BEV) detector with perspective supervision, which converges faster and better suits modern image backbones.</i></span>
                    </div>
                </div>
                <div>
                    <div class="publication_container_first">
                        <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zeng_Distilling_Focal_Knowledge_From_Imperfect_Expert_for_3D_Object_Detection_CVPR_2023_paper.html" target="_blank"><img loading="lazy" src="/assets/publication/fd3d.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zeng_Distilling_Focal_Knowledge_From_Imperfect_Expert_for_3D_Object_Detection_CVPR_2023_paper.html" target="_blank">Distilling Focal Knowledge from Imperfect Expert for 3D Object Detection</a></h2>
                        <span class="authors">Jia Zeng, Li Chen, <i>et al.</i></span>
                        <div>
                            <div>
                                <span class="Odarkblue">CVPR 2023</span>
                            </div>
                        </div>
                        <span class="gray"><i>We investigate on how to distill the knowledge from an imperfect expert. We propose FD3D, a Focal Distiller for 3D object detection.</i></span>
                    </div>
                </div>
                <div>
                    <div class="publication_container_first">
                        <a href="https://link.springer.com/chapter/10.1007/978-3-031-19839-7_32" target="_blank"><img loading="lazy" src="/assets/publication/persformer.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://link.springer.com/chapter/10.1007/978-3-031-19839-7_32" target="_blank">PersFormer: 3D Lane Detection via Perspective Transformer and the OpenLane Benchmark</a></h2>
                        <span class="authors">Li Chen, Chonghao Sima, Yang Li, Xiangwei Geng, Junchi Yan, <i>et al.</i></span>
                        <div>
                            <div>
                                <span class="Odarkblue">ECCV 2022 (Oral) <code>[Redefine the Community]</code></span>
                            </div>
                            <div id="prediction_and_planning">
                                <a href="https://github.com/OpenDriveLab/PersFormer_3DLane" target="_blank"><img loading="lazy" src="https://img.shields.io/github/stars/OpenDriveLab/PersFormer_3DLane?style=social"/></a>
                                <a href="https://github.com/OpenDriveLab/OpenLane" target="_blank"><img loading="lazy" src="/assets/icon/dataset.png"/></a>
                                <a href="https://patrick-llgc.github.io/Learning-Deep-Learning/paper_notes/persformer.html" target="_blank"><img loading="lazy" src="/assets/icon/article.png"/></a>
                                <a href="https://zhuanlan.zhihu.com/p/552908337" target="_blank"><img loading="lazy" src="/assets/icon/zhihu.png"/></a>
                            </div>
                        </div>
                        <span class="gray"><i>PersFormer adopts a unified 2D/3D anchor design and an auxiliary task to detect 2D/3D lanes; we release one of the first large-scale real-world 3D lane datasets, OpenLane.</i></span>
                    </div>
                </div>

            </div>
        </div> 
    </div>



    <br>
    <div class="block" style="margin: 0;">
        <div class="navigator"></div>
    </div>
    <br><br><br>



    <div class="block_left" style="margin-bottom: 0; padding-bottom: 0;">
        <div style="flex-grow: 1;">
            <a class="h1_title" href="#prediction_and_planning">
                <h1>
                    Prediction and Planning
                    <img loading="lazy" class="link_img"/>
                </h1>
            </a>
        </div> 
    </div>
    <br><br>
    <div class="block_left" style="margin-top: 0; padding-top: 0">
        <div style="flex-grow: 1;">
            <div class="publication_container">

                <div>
                    <div class="publication_container_first">
                        <a href="https://proceedings.mlr.press/v205/jia23a.html" target="_blank"><img loading="lazy" src="/assets/publication/traj_xiaosong.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://proceedings.mlr.press/v205/jia23a.html" target="_blank">Towards Capturing the Temporal Dynamics for Trajectory Prediction: a Coarse-to-Fine Approach</a></h2>
                        <span class="authors">Xiaosong Jia, Li Chen, Penghao Wu, Jia Zeng, <i>et al.</i></span>
                        <div>
                            <div>
                                <span class="Odarkblue">CoRL 2022</span>
                            </div>
                        </div>
                        <span class="gray"><i>We find taking scratch trajectories generated by MLP as input, a refinement module based on structures with temporal prior, could  boost the accuracy.</i></span>
                    </div>
                </div>
                <div>
                    <div class="publication_container_first">
                        <a href="https://arxiv.org/abs/2205.09753" target="_blank"><img loading="lazy" src="/assets/publication/hdgt.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://arxiv.org/abs/2205.09753" target="_blank">HDGT: Heterogeneous driving graph transformer for multi-agent trajectory prediction via scene encoding</a></h2>
                        <span class="authors">Xiaosong Jia, Penghao Wu, Li Chen, Hongyang Li, Yu Liu, Junchi Yan</i></span>
                        <div>
                            <div>
                                <span class="Odarkblue">TPAMI 2023</span>
                            </div>
                            <div id="cv_at_scale">
                                <a href="https://github.com/OpenPerceptionX/HDGT" target="_blank"><img loading="lazy" src="https://img.shields.io/github/stars/OpenPerceptionX/HDGT?style=social"/></a>
                            </div>
                        </div>
                        <span class="gray"><i>HDGT formulates the driving scene as a heterogeneous graph with different types of nodes and edges.</i></span>
                    </div>
                </div>

            </div>
        </div> 
    </div>



    <br>
    <div class="block" style="margin: 0;">
        <div class="navigator"></div>
    </div>
    <br><br><br>



    <div class="block_left" style="margin-bottom: 0; padding-bottom: 0;">
        <div style="flex-grow: 1;">
            <a class="h1_title" href="#cv_at_scale">
                <h1>
                    Computer Vision at Scale
                    <img loading="lazy" class="link_img"/>
                </h1>
            </a>
        </div> 
    </div>
    <br><br>
    <div class="block_left" style="margin-top: 0; padding-top: 0">
        <div style="flex-grow: 1;">

            <div class="publication_container">
                <div>
                    <div class="publication_container_first">
                        <a href="https://arxiv.org/abs/2305.16318" target="_blank"><img loading="lazy" src="/assets/publication/mutr.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://arxiv.org/abs/2305.16318" target="_blank">Referred by Multi-Modality: A Unified Temporal Transformer for Video Object Segmentation</a></h2>
                        <span class="authors">Shilin Yan, Renrui Zhang, Ziyu Guo, Wenchao Chen, Wei Zhang, Hongyang Li, Yu Qiao, Zhongjiang He, Peng Gao</span>
                        <div>
                            <div>
                                <span class="Odarkblue">AAAI 2024</span>
                            </div>
                            <div>
                                <a href="https://github.com/OpenGVLab/MUTR" target="_blank"><img loading="lazy" src="https://img.shields.io/github/stars/OpenGVLab/MUTR?style=social"/></a>
                            </div>
                        </div>
                    </div>
                </div>
                <div>
                    <div class="publication_container_first">
                        <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Xue_Stare_at_What_You_See_Masked_Image_Modeling_Without_Reconstruction_CVPR_2023_paper.html" target="_blank"><img loading="lazy" src="/assets/publication/maskalign.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Xue_Stare_at_What_You_See_Masked_Image_Modeling_Without_Reconstruction_CVPR_2023_paper.html" target="_blank">Stare at What You See: Masked Image Modeling without Reconstruction</a></h2>
                        <span class="authors">Hongwei Xue, Peng Gao, Hongyang Li, <i>et al.</i></span>
                        <div>
                            <div>
                                <span class="Odarkblue">CVPR 2023</span>
                            </div>
                            <div>
                                <a href="https://github.com/OpenDriveLab/maskalign" target="_blank"><img loading="lazy" src="https://img.shields.io/github/stars/OpenDriveLab/maskalign?style=social"/></a>
                            </div>
                        </div>
                        <span class="gray"><i>An efficient MIM paradigm MaskAlign and a Dynamic Alignment module to apply learnable alignment to tackle the problem of input inconsistency.</i></span>
                    </div>
                </div>
                <div>
                    <div class="publication_container_first">
                        <a href="https://link.springer.com/article/10.1007/s11263-023-01898-4" target="_blank"><img loading="lazy" src="/assets/publication/mimic.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://link.springer.com/article/10.1007/s11263-023-01898-4" target="_blank">Mimic before Reconstruct: Enhancing Masked Autoencoders with Feature Mimicking</a></h2>
                        <span class="authors">Peng Gao, Renrui Zhang, Rongyao Fang, Ziyi Lin, Hongyang Li, Hongsheng Li, Qiao Yu</span>
                        <div>
                            <div>
                                <span class="Odarkblue">IJCV 2023</span>
                            </div>
                            <div>
                                <a href="https://github.com/Alpha-VL/ConvMAE" target="_blank"><img loading="lazy" src="https://img.shields.io/github/stars/Alpha-VL/ConvMAE?style=social"/></a>
                            </div>
                        </div>
                        <span class="gray"><i>Introducing high-level and low-level representations to MAE without interference during pre-training.</i></span>
                    </div>
                </div>
                <div>
                    <div class="publication_container_first">
                        <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_Align_Representations_With_Base_A_New_Approach_to_Self-Supervised_Learning_CVPR_2022_paper.html" target="_blank"><img loading="lazy" src="/assets/publication/arb.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_Align_Representations_With_Base_A_New_Approach_to_Self-Supervised_Learning_CVPR_2022_paper.html" target="_blank">Align Representations With Base: A New Approach to Self-Supervised Learning</a></h2>
                        <span class="authors">Shaofeng Zhang, Lyn Qiu, Feng Zhu, Junchi Yan, Hengrui Zhang, Rui Zhao, Hongyang Li, Xiaokang Yang</span>
                        <div>
                            <div>
                                <span class="Odarkblue">CVPR 2022</span>
                            </div>
                        </div>
                    </div>
                </div>

            </div>
        </div> 
    </div>



    <br>



</body>
