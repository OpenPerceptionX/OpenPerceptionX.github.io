<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport"
    content="width=device-width,initial-scale=1.0,minimum-scale=1.0,maximum-scale=1.0,user-scalable=no" />
<title>Publications | OpenDriveLab</title>
<meta name="description" content="Most of the influential and interesting publications published by OpenDriveLab come from top conferences and journals in the computer field, such as ICML, ECCV, ICCV, NeurIPS, ICLR, CVPR.">
<meta name="keywords" content="OpenDriveLab Publication, Autonomous Driving, E2EAD, BEV, OpenLane, Bird-eye-view Perception, End-to-end Autonomous Driving, ">
<meta name="author" content="OpenDriveLab">
<link rel="icon" type="image/png" href="/style/img/team_logo.png">
<link href="/style/css/body.css" rel="stylesheet" type="text/css" media="all">
<link href="/style/css/head.css" rel="stylesheet" type="text/css" media="all">
<link href="/style/css/footer.css?1" rel="stylesheet" type="text/css" media="all">
<link href="/style/css/publications.css?11111" rel="stylesheet" type="text/css" media="all">
</head>
<body class="loadings">

  <div id="content" class="content">
    <div class="bg">
  <div class="pub_nav">
<div class="kin">
 
    <a href="#editor-pick" title="Editor's Pick">
      <div>
      <i class="icon__1"></i>
    <span>Editor's Pick</span></div>
    </a>
    
    <a href="#end-to-end" title="End-to-end Autonomous Driving"><div><i class="icon__2"></i>
      <span>End-to-end Autonomous Driving</span></div>
      </a>
  
    <a href="#bev-perception" title="Bird's-eye-view Perception"><div><i class="icon__3"></i><span>Bird's-eye-view Perception</span></div></a>
    
    <a href="#pp" title="Prediction and Planning"><div><i class="icon__4"></i>
    <span>Prediction and Planning</span></div>
    </a>
    
    <a href="#cv-at-large" title="Computer Vision at Large"><div><i class="icon__5"></i>
    <span>Computer Vision at Large</span></div>
    </a>
  </div>
  </div>
 <div class="publicationw " >
</div>
 </div></div>
<!-- 内容end -->

</body>
 
<script src="/style/js/jquery-3.1.1.min.js" type="text/javascript" charset="utf-8"></script>
<script src="/style/js/top_publications.js?12" type="text/javascript" charset="utf-8"></script>
<script>
  
    
 //publication-----------------------------------------------------------
 
 var itb = ["icon_cv","icon_gg","icon_jqzx","icon_zh"]
    var data = [
      {
   
       title:"Editor's Pick",
       id:"editor-pick",
       list:[
        {
        img:"goal.png",
        title:"Planning-oriented Autonomous Driving",
        url:"https://arxiv.org/abs/2212.10156",
        ititle:"Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Hongyang Li, <em>et al.</em>",
        title2:"CVPR 2023 Award Candidate",
        GitHub_url:"https://github.com/OpenDriveLab/UniAD",
        GitHub_img:"https://img.shields.io/github/stars/OpenDriveLab/UniAD?style=social",
        itb:totubao("0,https://zhuanlan.zhihu.com/p/597019546|1,https://mp.weixin.qq.com/s/QBRTiku0_rF6GM1fHfi4lw"),
        code:"[Sell a New Philosophy]" ,
        content:"A comprehensive framework up-to-date that incorporates full-stack driving tasks in one network.",
      },
        {
        img:"bevf1.png",
        title:"BEVFormer: Learning Bird's-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers",
        url:"https://arxiv.org/abs/2203.17270",
        ititle:"Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, <em>et al.</em>",
        title2:"ECCV 2022",
        GitHub_url:"https://github.com/fundamentalvision/BEVFormer",
        GitHub_img:"https://img.shields.io/github/stars/fundamentalvision/BEVFormer?style=social",
        itb:totubao("0,https://zhuanlan.zhihu.com/p/564295059"),
        code:"[Baseline] [nuScenes First Place] [Waymo Challenge 2022 Official First Place]" ,
        content:"A paradigm for autonomous driving that applies both Transformer and Temporal structure to generate BEV features.",
      },
      {
        img:"persfor.png",
        title:"PersFormer: 3D Lane Detection via Perspective Transformer and the OpenLane Benchmark",
        url:"https://arxiv.org/abs/2203.11089",
        ititle:"Li Chen, Chonghao Sima, Yang Li, Xiangwei Geng, Junchi Yan, <em>et al.</em>",
        title2:"ECCV 2022 (Oral)",
        GitHub_url:"https://github.com/OpenDriveLab/PersFormer_3DLane",
        GitHub_img:"https://img.shields.io/github/stars/OpenDriveLab/PersFormer_3DLane?style=social",
        itb:totubao("0,https://github.com/OpenDriveLab/OpenLane|1,https://zhuanlan.zhihu.com/p/495979738"),
        code:"[Redefine the Community]" ,
        content:"PersFormer adopts a unified 2D/3D anchor design and an auxiliary task to detect 2D/3D lanes; we release one of the first large-scale real-world 3D lane datasets, OpenLane.",
      }
        ]
      },
      {
   title:"End-to-end Autonomous Driving",
   id:"end-to-end",
   list:[
    {
    img:"thinktwice.png",
    title:"Think Twice before Driving: Towards Scalable Decoders for End-to-End Autonomous Driving",
    url:"https://arxiv.org/abs/2305.06242",
    ititle:"Xiaosong Jia, Penghao Wu, Li Chen, <em>et al.</em>",
    title2:"CVPR 2023",
    GitHub_url:"https://github.com/OpenDriveLab/UniAD",
    GitHub_img:"https://img.shields.io/github/stars/OpenDriveLab/UniAD?style=social",
    itb:totubao("0,https://opendrivelab.com|1,#|2,#|3,#"),
    code:"[Baseline] [nuScenes First Place]" ,
    content:"A comprehensive framework up-to-date that incorporates full-stack driving tasks in one network.",
  },
  {
    img:"thinktwice.png",
    title:"Planning-oriented Autonomous Driving",
    url:"https://opendrivelab.com/e2ead/cvpr23.html",
    ititle:"Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Hongyang Li, et al.",
    title2:"CVPR 2023 Award Candidate",
    GitHub_url:"https://github.com/OpenDriveLab/UniAD",
    GitHub_img:"https://img.shields.io/github/stars/OpenDriveLab/UniAD?style=social",
    itb:totubao("0,https://opendrivelab.com|1,#|2,#|3,#"),
    code:"[Baseline] [nuScenes First Place]" ,
    content:"A comprehensive framework up-to-date that incorporates full-stack driving tasks in one network.",
  },
  {
    img:"thinktwice.png",
    title:"Planning-oriented Autonomous Driving",
    url:"https://opendrivelab.com/e2ead/cvpr23.html",
    ititle:"Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Hongyang Li, et al.",
    title2:"CVPR 2023 Award Candidate",
    GitHub_url:"https://github.com/OpenDriveLab/UniAD",
    GitHub_img:"https://img.shields.io/github/stars/OpenDriveLab/UniAD?style=social",
    itb:totubao("0,https://opendrivelab.com|1,#|2,#|3,#"),
    code:"[Baseline] [nuScenes First Place]" ,
    content:"A comprehensive framework up-to-date that incorporates full-stack driving tasks in one network.",
  },
  {
    img:"thinktwice.png",
    title:"Planning-oriented Autonomous Driving",
    url:"https://opendrivelab.com/e2ead/cvpr23.html",
    ititle:"Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Hongyang Li, et al.",
    title2:"CVPR 2023 Award Candidate",
    GitHub_url:"https://github.com/OpenDriveLab/UniAD",
    GitHub_img:"https://img.shields.io/github/stars/OpenDriveLab/UniAD?style=social",
    itb:totubao("0,https://opendrivelab.com|1,#|2,#|3,#"),
    code:"[Baseline] [nuScenes First Place]" ,
    content:"A comprehensive framework up-to-date that incorporates full-stack driving tasks in one network.",
  },
    ]
  },
  {
   title:"Bird's-eye-view Perception",
   id:"bev-perception",
   list:[
    {
    img:"/style/img/team_logo.png",
    title:"Planning-oriented Autonomous Driving",
    url:"https://opendrivelab.com/e2ead/cvpr23.html",
    ititle:"Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Hongyang Li, et al.",
    title2:"CVPR 2023 Award Candidate",
    GitHub_url:"https://github.com/OpenDriveLab/UniAD",
    GitHub_img:"https://img.shields.io/github/stars/OpenDriveLab/UniAD?style=social",
    itb:totubao("0,https://opendrivelab.com|1,#|2,#|3,#"),
    code:"[Baseline] [nuScenes First Place]" ,
    content:"A comprehensive framework up-to-date that incorporates full-stack driving tasks in one network.",
  },
  {
    img:"/style/img/team_logo.png",
    title:"Planning-oriented Autonomous Driving",
    url:"https://opendrivelab.com/e2ead/cvpr23.html",
    ititle:"Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Hongyang Li, et al.",
    title2:"CVPR 2023 Award Candidate",
    GitHub_url:"https://github.com/OpenDriveLab/UniAD",
    GitHub_img:"https://img.shields.io/github/stars/OpenDriveLab/UniAD?style=social",
    itb:totubao("0,https://opendrivelab.com|1,#|2,#|3,#"),
    code:"[Baseline] [nuScenes First Place]" ,
    content:"A comprehensive framework up-to-date that incorporates full-stack driving tasks in one network.",
  },
  {
    img:"/style/img/team_logo.png",
    title:"Planning-oriented Autonomous Driving",
    url:"https://opendrivelab.com/e2ead/cvpr23.html",
    ititle:"Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Hongyang Li, et al.",
    title2:"CVPR 2023 Award Candidate",
    GitHub_url:"https://github.com/OpenDriveLab/UniAD",
    GitHub_img:"https://img.shields.io/github/stars/OpenDriveLab/UniAD?style=social",
    itb:totubao("0,https://opendrivelab.com|1,#|2,#|3,#"),
    code:"[Baseline] [nuScenes First Place]" ,
    content:"A comprehensive framework up-to-date that incorporates full-stack driving tasks in one network.",
  },
    ]
  },
  {
   title:"Prediction and Planning",
   id:"pp",
   list:[
    {
    img:"/style/img/team_logo.png",
    title:"Planning-oriented Autonomous Driving",
    url:"https://opendrivelab.com/e2ead/cvpr23.html",
    ititle:"Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Hongyang Li, et al.",
    title2:"CVPR 2023 Award Candidate",
    GitHub_url:"https://github.com/OpenDriveLab/UniAD",
    GitHub_img:"https://img.shields.io/github/stars/OpenDriveLab/UniAD?style=social",
    itb:totubao("0,https://opendrivelab.com|1,#|2,#|3,#"),
    code:"[Baseline] [nuScenes First Place]" ,
    content:"A comprehensive framework up-to-date that incorporates full-stack driving tasks in one network.",
  },
  {
    img:"/style/img/team_logo.png",
    title:"Planning-oriented Autonomous Driving",
    url:"https://opendrivelab.com/e2ead/cvpr23.html",
    ititle:"Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Hongyang Li, et al.",
    title2:"CVPR 2023 Award Candidate",
    GitHub_url:"https://github.com/OpenDriveLab/UniAD",
    GitHub_img:"https://img.shields.io/github/stars/OpenDriveLab/UniAD?style=social",
    itb:totubao("0,https://opendrivelab.com|1,#|2,#|3,#"),
    code:"[Baseline] [nuScenes First Place]" ,
    content:"A comprehensive framework up-to-date that incorporates full-stack driving tasks in one network.",
  },
    ]
  },
  {
   title:"Computer Vision at Large",
   id:"cv-at-large",
   list:[
    {
    img:"/style/img/team_logo.png",
    title:"Planning-oriented Autonomous Driving",
    url:"https://opendrivelab.com/e2ead/cvpr23.html",
    ititle:"Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Hongyang Li, et al.",
    title2:"CVPR 2023 Award Candidate",
    GitHub_url:"https://github.com/OpenDriveLab/UniAD",
    GitHub_img:"https://img.shields.io/github/stars/OpenDriveLab/UniAD?style=social",
    itb:totubao("0,https://opendrivelab.com|1,#|2,#|3,#"),
    code:"[Baseline] [nuScenes First Place]" ,
    content:"A comprehensive framework up-to-date that incorporates full-stack driving tasks in one network.",
  },
    ]
  },
       
    ];
     
   

    function totubao(b){
      var html = ""
      var x = b.split("|");
       var len = x.length;
      for(i=0;i<len;i++){
var s = x[i].split(",")
var a = s[0]
var url = s[1]
idata = '<a href="'+url+'"><img src="/style/img/home/'+itb[a]+'.svg" class="icon1"><img src="/style/img/home/'+itb[a]+'_this.svg" class="icon2"></a>'
html += idata;
}

return html
    }


//获取列表模板插入数据
 

var item = '<dl>{{img}}<dd><h2><a class="hover_color" href="{{url}}">{{title}}</a></h2><h3 class="hover_color2">{{ititle}}</h3><p><a>{{title2}}</a><br><span class="icon"><a href="{{GitHub_url}}"><img src="{{GitHub_img}}" alt="GitHub"  ></a>{{itb}}</span><code>{{code}}</code></p><p>{{content}}</p></dd></dl>';

var ilist = '<div class="publicationk"  ><name id="{{id}}"/><h3 class="title_1"><span>{{title}}</span></h3><div class="publication_list" ><div class="kin"><list></div></div></div>';

// 插入菜单
tohtml(data,$(".publicationw"),item,ilist)
// 插入列表


function tohtml(shuju,div,item,ilist){
var html = ""
  var len = shuju.length
  for(i=0;i<len;i++){
idata = template(ilist,shuju[i]);


var len2 = shuju[i].list.length;
var myshuju = shuju[i]
  var h = ""
  for(var ii=0;ii<len2;ii++){
    var img = myshuju.list[ii]["img"]
    var url = myshuju.list[ii]["url"]
    if(img !==""){
      myshuju.list[ii]["img"] = '<dt><a href="'+url+'"><div class="publication_img"><img data-src="'+img+'" src="/style/img/none.png"></div></a></dt>'
    }
  
    console.log(myshuju.list[ii]["img"])
  h += template(item,myshuju.list[ii])
  
};
html += template2(idata,h)
}
div.append(html);
start()
}

//滚动时，显示对应图片
	$(window).on('scroll',function(){
		start()
	})


 







  </script>
</html>