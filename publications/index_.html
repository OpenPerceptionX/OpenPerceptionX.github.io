<head>
    <title>Publication | OpenDriveLab</title>
    <meta name="keywords" content="OpenDriveLab, Publication, Autonomous Driving, Autonomous Agent, Embodied AI">
    <meta name="description" content="OpenDriveLab is committed to exploring cutting-edge autonomous driving technology, launching a series of benchmarking work, open source to serve the community, and promote the common development of the industry. Friends who are committed to making influential research are welcome to join!">

    <link rel="icon" type="image/png" href="/assets/icon/D_small.png">

    <link href="/ui2024/css/all.css" rel="stylesheet" type="text/css" media="all"/>
    <link href="/ui2024/css/narrow_banner.css" rel="stylesheet" type="text/css" media="all"/>

    <script src="/ui2024/js/jquery-3.7.1.min.js" integrity="sha256-/JqT3SQfawRcv/BIHPThkBvs0OEvtFFmqPF/lYI/Cxo=" crossorigin="anonymous"></script>
</head>



<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-L7VEDHS6G8"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-L7VEDHS6G8');
</script>



<script>
    $.get("/ui2024/components/header.html",function(result){
        $("body").prepend(result)
    })
    $.get("/ui2024/components/footer.html",function(result){
        $("body").append(result)
    })
</script>



<script>
    var url_list = [{
        name: 'Li Chen',
        url: 'https://scholar.google.com/citations?user=ulZxvY0AAAAJ'
    }, {
        name: 'Penghao Wu',
        url: 'https://scholar.google.com/citations?user=9mssd5EAAAAJ'
    }, {
        name: 'Chonghao Sima',
        url: 'https://scholar.google.com/citations?user=dgYJ6esAAAAJ'
    }, {
        name: 'Hongyang Li',
        url: 'https://lihongyang.info'
    }, {
        name: 'Xiaosong Jia',
        url: 'https://jiaxiaosong1002.github.io'
    }, {
        name: 'Junchi Yan',
        url: 'https://thinklab.sjtu.edu.cn'
    }, {
        name: 'Andreas Geiger',
        url: 'https://www.cvlibs.net'
    }, {
        name: 'Yu Qiao',
        url: 'https://scholar.google.com/citations?user=gFtI-8QAAAAJ'
    }, {
        name: 'Huijie Wang',
        url: 'https://scholar.google.com/citations?user=Xg4cp-EAAAAJ'
    }, {
        name: 'Jiazhi Yang',
        url: 'https://scholar.google.com/citations?user=Ju7nGX8AAAAJ'
    }, {
        name: 'Tianyu Li',
        url: 'https://scholar.google.com/citations?user=X6vTmEMAAAAJ'
    },];
    function add_url(a) {
        var len = url_list.length;

        var h = a.toString();
        for (var i = 0; i < len; i++) {
        if (url_list[i]["name"] == h) {
            var htm = '<a href="' + url_list[i]["url"] + '" target="_blank">' + h + '</a>'
            var h = htm;

        }
        }
        return h
    }

  
  function cur(a) {
    var h = a.split(",")
    var len = h.length;
    var htm = ""
    for (var i = 0; i < len; i++) {
      var fh = (i == 0) ? "" : ", "
      htm += fh + add_url(h[i].replace(/^\s+|\s+$/g, ""))
    }
    return htm
  }
    addEventListener("load", () => {
        var authors_lists = document.getElementsByClassName("authors");
        for (var i = 0; i < authors_lists.length; i++) {
            authors_list = authors_lists[i];
            // console.log(authors_list.textContent);
            // authors_list.textContent = cur(authors_list.textContent);
            // console.log(authors_list.textContent);
        }
    })
</script>



<body>

    

    <div class="banner">
        <div class="banner_container">
            <div class="banner_title_en">
                <span>Publication</span>
            </div>
            <div class="banner_subtitle_en">
                <span id="editor_pick">We position OpenDriveLab as one of the most top research teams around globe, since we've got talented people and published work at top venues.</span>
            </div>
        </div>
    </div>



    <div class="block sticky" id="sticky">
        <div class="sticky_row">
            <a href="#editor_pick">Editor's Pick</a>
            <a></a>
            <a></a>
            <a href="#" class="image"><img src="/assets/icon/top.png"/></a>
        </div>
        <div class="navigator"></div>
        <div class="sticky_row">
            <a href="#end_to_end_ad">End-to-End Autonomous Driving</a>
            <a href="#bev_perception">Bird's-Eye-View Perception</a>
            <a href="#prediction_and_planning">Prediction and Planning</a>
            <a href="#cv_at_scale">Computer Vision at Scale</a>
        </div>
    </div>



    <br><br><br>



    <div class="block_left" style="margin-bottom: 0; padding-bottom: 0;">
        <div style="flex-grow: 1;">
            <a class="h1_title" href="#editor_pick">
                <h1>
                    Editor's Pick
                    <img loading="lazy" class="link_img"/>
                </h1>
            </a>
        </div> 
    </div>
    <br><br>
    <div class="block_left" style="margin-top: 0; padding-top: 0">
        <div style="flex-grow: 1;">

            <div class="publication_container">

                <div>
                    <div class="publication_container_first">
                        <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Hu_Planning-Oriented_Autonomous_Driving_CVPR_2023_paper.html" target="_blank"><img loading="lazy" src="/assets/publication/uniad.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Hu_Planning-Oriented_Autonomous_Driving_CVPR_2023_paper.html" target="_blank">Planning-oriented Autonomous Driving</a></h2>
                        <h4 class="authors">Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Hongyang Li, <i>et al.</i></h4>
                        <div>
                            <div>
                                <a href="https://cvpr2023.thecvf.com/Conferences/2023/Awards" target="_blank" style="text-decoration: underline;">CVPR 2023 Best Paper Award</a>
                            </div>
                            <div>
                                <a href="https://github.com/OpenDriveLab/UniAD" target="_blank"><img loading="lazy" src="https://img.shields.io/github/stars/OpenDriveLab/UniAD?style=social"/></a>
                                <a href="https://zhuanlan.zhihu.com/p/638780421" target="_blank"><img loading="lazy" src="/assets/icon/zhihu.png"/></a>
                                <a href="https://mp.weixin.qq.com/s?__biz=MzkyMDUzMDE2Mw==&mid=2247485809&idx=1&sn=5dbd00380bcf80cc32d11b6e15e93829" target="_blank"><img loading="lazy" src="/assets/icon/article.png"/></a>
                                <a href="https://www.youtube.com/watch?v=cyrxJJ_nnaQ" target="_blank"><img loading="lazy" src="/assets/icon/youtube.png"/></a>
                                <a href="/e2ead/UniAD_plenary_talk_slides.pdf" target="_blank"><img loading="lazy" src="/assets/icon/slides.png"/></a>
                            </div>
                        </div>
                        <span>A comprehensive framework up-to-date that incorporates full-stack driving tasks in one network.</span>
                    </div>
                </div>
                <div>
                    <div class="publication_container_first">
                        <a href="https://arxiv.org/abs/2306.16927" target="_blank"><img loading="lazy" src="/assets/publication/e2esurvey.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://arxiv.org/abs/2306.16927" target="_blank">End-to-end Autonomous Driving: Challenges and Frontiers</a></h2>
                        <h4 class="authors">Li Chen, Penghao Wu, Kashyap Chitta, Bernhard Jaeger, Andreas Geiger, Hongyang Li</h4>
                        <div>
                            <div>
                                <a>arXiv 2023</a>
                            </div>
                            <div>
                                <a href="https://github.com/OpenDriveLab/End-to-end-Autonomous-Driving" target="_blank"><img loading="lazy" src="https://img.shields.io/github/stars/OpenDriveLab/End-to-end-Autonomous-Driving?style=social"/></a>
                            </div>
                        </div>
                        <span>In this survey, we provide a comprehensive analysis of more than 250 papers on the motivation, roadmap, methodology, challenges, and future trends in end-to-end autonomous driving.</span>
                    </div>
                </div>
                <div>
                    <div class="publication_container_first">
                        <a href="https://ieeexplore.ieee.org/document/10321736" target="_blank"><img loading="lazy" src="/assets/publication/bevsurvey.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://ieeexplore.ieee.org/document/10321736" target="_blank">Delving into the Devils of Bird's-Eye-View Perception: A Review, Evaluation and Recipe</a></h2>
                        <h4 class="authors">Hongyang Li, Chonghao Sima, Jifeng Dai, Wenhai Wang, Lewei Lu, <i>et al.</i></h4>
                        <div>
                            <div>
                                <a>TPAMI 2023 &nbsp <code>[Setup the Table]</code></a>
                            </div>
                            <div>
                                <a href="https://github.com/opendrivelab/bevperception-survey-recipe" target="_blank"><img loading="lazy" src="https://img.shields.io/github/stars/opendrivelab/bevperception-survey-recipe?style=social"/></a>
                            </div>
                        </div>
                        <span>We review the most recent work on BEV perception and provide analysis of different solutions.</span>
                    </div>
                </div>
                <div>
                    <div class="publication_container_first">
                        <a href="https://openreview.net/forum?id=OMOOO3ls6g" target="_blank"><img loading="lazy" src="/assets/publication/openlanev2.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://openreview.net/forum?id=OMOOO3ls6g" target="_blank">OpenLane-V2: A Topology Reasoning Benchmark for Unified 3D HD Mapping</a></h2>
                        <h4 class="authors">Huijie Wang, Tianyu Li, Yang Li, Li Chen, Chonghao Sima, <i>et al.</i></h4>
                        <div>
                            <div>
                                <a>NeurIPS 2023 Track Datasets and Benchmarks</a>
                            </div>
                            <div>
                                <a href="https://github.com/OpenDriveLab/OpenLane-V2" target="_blank"><img loading="lazy" src="https://img.shields.io/github/stars/opendrivelab/OpenLane-V2?style=social"/></a>
                            </div>
                        </div>
                        <span>The world's first perception and reasoning benchmark for scene structure in autonomous driving.</span>
                    </div>
                </div>
                <div>
                    <div class="publication_container_first">
                        <a href="https://link.springer.com/chapter/10.1007/978-3-031-20077-9_1" target="_blank"><img loading="lazy" src="/assets/publication/bevformer.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://link.springer.com/chapter/10.1007/978-3-031-20077-9_1" target="_blank">BEVFormer: Learning Bird's-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers</a></h2>
                        <h4 class="authors">Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, <i>et al.</i></h4>
                        <div>
                            <div>
                                <a>ECCV 2022 &nbsp <code>[nuScenes First Place]</code> <code>[Waymo Challenge 2022 First Place]</code></a>
                            </div>
                            <div id="end_to_end_ad">
                                <a href="https://github.com/fundamentalvision/BEVFormer" target="_blank"><img loading="lazy" src="https://img.shields.io/github/stars/fundamentalvision/BEVFormer?style=social"/></a>
                                <a href="https://zhuanlan.zhihu.com/p/564295059" target="_blank"><img loading="lazy" src="/assets/icon/zhihu.png"/></a>
                            </div>
                        </div>
                        <span>A paradigm for autonomous driving that applies both Transformer and Temporal structure to generate BEV features.</span>
                    </div>
                </div>
                <!-- <div>
                    <div class="publication_container_first">
                        <a href="xxx" target="_blank"><img loading="lazy" src="xxx"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="xxx" target="_blank">xxx</a></h2>
                        <h4 class="authors">xxx, <i>et al.</i></h4>
                        <div>
                            <div>
                                <a>xxx</a>
                            </div>
                            <div>
                                <a href="xxx" target="_blank"><img loading="lazy" src="xxx"/></a>
                                <a href="xxx" target="_blank"><img loading="lazy" src="xxx"/></a>
                                <a href="xxx" target="_blank"><img loading="lazy" src="xxx"/></a>
                                <a href="xxx" target="_blank"><img loading="lazy" src="xxx"/></a>
                            </div>
                        </div>
                        <span>xxx</span>
                    </div>
                </div> -->

            </div>
        </div> 
    </div>



    <br>
    <div class="block" style="margin: 0;">
        <div class="navigator"></div>
    </div>
    <br><br><br>



    <div class="block_left" style="margin-bottom: 0; padding-bottom: 0;">
        <div style="flex-grow: 1;">
            <a class="h1_title" href="#end_to_end_ad">
                <h1>
                    End-to-End Autonomous Driving
                    <img loading="lazy" class="link_img"/>
                </h1>
            </a>
        </div> 
    </div>
    <br><br>
    <div class="block_left" style="margin-top: 0; padding-top: 0">
        <div style="flex-grow: 1;">
            <div class="publication_container">

                <div>
                    <div class="publication_container_first">
                        <a href="https://arxiv.org/abs/2311.01043" target="_blank"><img loading="lazy" src="/assets/publication/llm4drive.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://arxiv.org/abs/2311.01043" target="_blank">A Survey of Large Language Models for Autonomous Driving</a></h2>
                        <h4 class="authors">Zhenjie Yang, Xiaosong Jia, Hongyang Li, Junchi Yan</h4>
                        <div>
                            <div>
                                <a>arXiv 2023</a>
                            </div>
                            <div>
                                <a href="https://github.com/Thinklab-SJTU/Awesome-LLM4AD" target="_blank"><img loading="lazy" src="https://img.shields.io/github/stars/Thinklab-SJTU/Awesome-LLM4AD?style=social"/></a>
                            </div>
                        </div>
                        <span>A collection of research papers about LLM-for-Autonomous-Driving (LLM4AD).</span>
                    </div>
                </div>
                <div>
                    <div class="publication_container_first">
                        <a href="https://arxiv.org/abs/2308.00398" target="_blank"><img loading="lazy" src="/assets/publication/driveadapted.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://arxiv.org/abs/2308.00398" target="_blank">DriveAdapter: Breaking the Coupling Barrier of Perception and Planning in End-to-End Autonomous Driving</a></h2>
                        <h4 class="authors">Xiaosong Jia, Yulu Gao, Li Chen, Junchi Yan, Patrick Langechuan Liu, Hongyang Li</i></h4>
                        <div>
                            <div>
                                <a>ICCV 2023 (Oral)</a>
                            </div>
                            <div>
                                <a href="https://github.com/OpenDriveLab/DriveAdapter" target="_blank"><img loading="lazy" src="https://img.shields.io/github/stars/OpenDriveLab/DriveAdapter?style=social"/></a>
                            </div>
                        </div>
                        <span>A new paradigm for end-to-end autonomous driving without causal confusion issue.</span>
                    </div>
                </div>
                <div>
                    <div class="publication_container_first">
                        <a href="https://arxiv.org/abs/2305.06242" target="_blank"><img loading="lazy" src="/assets/publication/thinktwice.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://arxiv.org/abs/2305.06242" target="_blank">Think Twice before Driving: Towards Scalable Decoders for End-to-End Autonomous Driving</a></h2>
                        <h4 class="authors">Xiaosong Jia, Penghao Wu, Li Chen, <i>et al.</i></h4>
                        <div>
                            <div>
                                <a>CVPR 2023</a>
                            </div>
                            <div>
                                <a href="https://github.com/OpenDriveLab/ThinkTwice" target="_blank"><img loading="lazy" src="https://img.shields.io/github/stars/opendrivelab/thinktwice?style=social"/></a>
                            </div>
                        </div>
                        <span>A scalable decoder paradigm that generates the future trajectory and action of the ego vehicle for end-to-end autonomous driving.</span>
                    </div>
                </div>
                <div>
                    <div class="publication_container_first">
                        <a href="https://arxiv.org/abs/2301.01006" target="_blank"><img loading="lazy" src="/assets/publication/ppgeo.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://arxiv.org/abs/2301.01006" target="_blank">Policy Pre-Training for End-to-End Autonomous Driving via Self-Supervised Geometric Modeling</a></h2>
                        <h4 class="authors">Penghao Wu, Li Chen, Hongyang Li, Xiaosong Jia, Junchi Yan, Yu Qiao</h4>
                        <div>
                            <div>
                                <a>ICLR 2023</a>
                            </div>
                            <div>
                                <a href="https://github.com/opendrivelab/ppgeo" target="_blank"><img loading="lazy" src="https://img.shields.io/github/stars/opendrivelab/ppgeo?style=social"/></a>
                                <a href="https://docs.google.com/presentation/d/1d0MGh3XCxuZujtYgZ0sr6xsAKZ4uS50p/edit?usp=sharing&ouid=118212253182146260973&rtpof=true&sd=true" target="_blank"><img loading="lazy" src="/assets/icon/slides.png"/></a>
                            </div>
                        </div>
                        <span>An intuitive and straightforward fully self-supervised framework curated for the policy pre-training in visuomotor driving.</span>
                    </div>
                </div>
                <div>
                    <div class="publication_container_first">
                        <a href="https://arxiv.org/abs/2206.08129" target="_blank"><img loading="lazy" src="/assets/publication/tcp.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://arxiv.org/abs/2206.08129" target="_blank">Trajectory-guided Control Prediction for End-to-end Autonomous Driving: A Simple yet Strong Baseline</a></h2>
                        <h4 class="authors">Penghao Wu, Xiaosong Jia, Li Chen, Junchi Yan, Hongyang Li, Yu Qiao</h4>
                        <div>
                            <div>
                                <a>NeurIPS 2022 &nbsp <code>[Carla First Place]</code></a>
                            </div>
                            <div>
                                <a href="https://github.com/OpenDriveLab/TCP" target="_blank"><img loading="lazy" src="https://img.shields.io/github/stars/OpenDriveLab/TCP?style=social"/></a>
                                <a href="https://zhuanlan.zhihu.com/p/532665469" target="_blank"><img loading="lazy" src="/assets/icon/zhihu.png"/></a>
                            </div>
                        </div>
                        <span>Take the initiative to explore the combination of controller based on a planned trajectory and perform control prediction.</span>
                    </div>
                </div>
                <div>
                    <div class="publication_container_first">
                        <a href="https://arxiv.org/abs/2207.07601" target="_blank"><img loading="lazy" src="/assets/publication/stp3.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://arxiv.org/abs/2207.07601" target="_blank">ST-P3: End-to-end Vision-based Autonomous Driving via Spatial-Temporal Feature Learning</a></h2>
                        <h4 class="authors">Shengchao Hu, Li Chen, Penghao Wu, Hongyang Li, <i>et al.</i></h4>
                        <div>
                            <div>
                                <a>ECCV 2022</a>
                            </div>
                            <div id="bev_perception">
                                <a href="https://github.com/OpenDriveLab/st-p3" target="_blank"><img loading="lazy" src="https://img.shields.io/github/stars/OpenDriveLab/st-p3?style=social"/></a>
                                <a href="https://zhuanlan.zhihu.com/p/544387122" target="_blank"><img loading="lazy" src="/assets/icon/zhihu.png"/></a>
                            </div>
                        </div>
                        <span>A spatial-temporal feature learning scheme towards a set of more representative features for perception, prediction and planning tasks simultaneously.</span>
                    </div>
                </div>

            </div>
        </div> 
    </div>



    <br>
    <div class="block" style="margin: 0;">
        <div class="navigator"></div>
    </div>
    <br><br><br>



    <div class="block_left" style="margin-bottom: 0; padding-bottom: 0;">
        <div style="flex-grow: 1;">
            <a class="h1_title" href="#bev_perception">
                <h1>
                    Bird's-Eye-View Perception
                    <img loading="lazy" class="link_img"/>
                </h1>
            </a>
        </div> 
    </div>
    <br><br>
    <div class="block_left" style="margin-top: 0; padding-top: 0">
        <div style="flex-grow: 1;">
            <div class="publication_container">

                <div>
                    <div class="publication_container_first">
                        <a href="https://arxiv.org/abs/2312.16108" target="_blank"><img loading="lazy" src="/assets/publication/lanesegnet.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://arxiv.org/abs/2312.16108" target="_blank">LaneSegNet: Map Learning with Lane Segment Perception for Autonomous Driving</a></h2>
                        <h4 class="authors">Tianyu Li, Peijin Jia, Bangjun Wang, Li Chen, Kun Jiang, Junchi Yan, Hongyang Li</i></h4>
                        <div>
                            <div>
                                <a>ICLR 2024</a>
                            </div>
                            <div>
                                <a href="https://github.com/OpenDriveLab/LaneSegNet" target="_blank"><img loading="lazy" src="https://img.shields.io/github/stars/opendrivelab/LaneSegNet?style=social"/></a>
                            </div>
                        </div>
                        <span>We advocate Lane Segment as a map learning paradigm that seamlessly incorporates both map geometry and topology information.</span>
                    </div>
                </div>
                <div>
                    <div class="publication_container_first">
                        <a href="https://arxiv.org/abs/2310.15670" target="_blank"><img loading="lazy" src="/assets/publication/vcd.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://arxiv.org/abs/2310.15670" target="_blank">Leveraging Vision-Centric Multi-Modal Expertise for 3D Object Detection</a></h2>
                        <h4 class="authors">Linyan Huang, Zhiqi Li, Chonghao Sima, Wenhai Wang, Jingdong Wang, Yu Qiao, Hongyang Li</i></h4>
                        <div>
                            <div>
                                <a>NeurIPS 2023</a>
                            </div>
                            <div>
                                <a href="https://github.com/OpenDriveLab/Birds-eye-view-Perception" target="_blank"><img loading="lazy" src="https://img.shields.io/github/stars/opendrivelab/Birds-eye-view-Perception?style=social"/></a>
                            </div>
                        </div>
                        <span>The unified framework to enhance 3D object detection by uniting a multi-modal expert model with a trajectory distillation module.</span>
                    </div>
                </div>
                <div>
                    <div class="publication_container_first">
                        <a href="https://arxiv.org/abs/2306.02851" target="_blank"><img loading="lazy" src="/assets/publication/occnet.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://arxiv.org/abs/2306.02851" target="_blank">Scene as Occupancy</a></h2>
                        <h4 class="authors">Chonghao Sima, Wenwen Tong, Tai Wang, Li Chen, Silei Wu, Hanming Deng, Yi Gu, Lewei Lu, Ping Luo, Dahua Lin, Hongyang Li</i></h4>
                        <div>
                            <div>
                                <a>ICCV 2023</a>
                            </div>
                            <div>
                                <a href="https://github.com/OpenDriveLab/OccNet" target="_blank"><img loading="lazy" src="https://img.shields.io/github/stars/opendrivelab/OccNet?style=social"/></a>
                            </div>
                        </div>
                        <span>Occupancy serves as a general representation of the scene and could facilitate perception and planning in the full-stack of autonomous driving.</span>
                    </div>
                </div>
                <div>
                    <div class="publication_container_first">
                        <a href="https://arxiv.org/abs/2304.05277" target="_blank"><img loading="lazy" src="/assets/publication/toponet.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://arxiv.org/abs/2304.05277" target="_blank">Graph-based Topology Reasoning for Driving Scenes</a></h2>
                        <h4 class="authors">Tianyu Li, Li Chen, Huijie Wang, Yang Li, Jiazhi Yang, Xiangwei Geng, <i>et al.</i></h4>
                        <div>
                            <div>
                                <a>arXiv 2023</a>
                            </div>
                            <div>
                                <a href="https://github.com/OpenDriveLab/TopoNet" target="_blank"><img loading="lazy" src="https://img.shields.io/github/stars/opendrivelab/TopoNet?style=social"/></a>
                            </div>
                        </div>
                        <span>A new baseline for scene topology reasoning, which unifies heterogeneous feature learning and enhances feature interactions via the graph neural network architecture and the knowledge graph design.</span>
                    </div>
                </div>
                <div>
                    <div class="publication_container_first">
                        <a href="https://arxiv.org/abs/2304.04179" target="_blank"><img loading="lazy" src="/assets/publication/sparse_yulu.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://arxiv.org/abs/2304.04179" target="_blank">Sparse Dense Fusion for 3D Object Detection</a></h2>
                        <h4 class="authors">Yulu Gao, Chonghao Sima, Shaoshuai Shi, Shangzhe Di, Si Liu, Hongyang Li</i></h4>
                        <div>
                            <div>
                                <a>IROS 2023</a>
                            </div>
                        </div>
                        <span>We propose Sparse Dense Fusion (SDF), a complementary framework that incorporates both sparse-fusion and dense-fusion modules via the Transformer architecture.</span>
                    </div>
                </div>
                <div>
                    <div class="publication_container_first">
                        <a href="https://arxiv.org/abs/2303.10340" target="_blank"><img loading="lazy" src="/assets/publication/3daug.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://arxiv.org/abs/2303.10340" target="_blank">3D Data Augmentation for Driving Scenes on Camera</a></h2>
                        <h4 class="authors">Wenwen Tong, Jiangwei Xie, Tianyu Li, Hanming Deng, Xiangwei Geng, Ruoyi Zhou, Dingchen Yang, Bo Dai, Lewei Lu, Hongyang Li</i></h4>
                        <div>
                            <div>
                                <a>arXiv 2023</a>
                            </div>
                        </div>
                        <span>We propose a 3D data augmentation approach termed Drive-3DAug to augment the driving scenes on camera in the 3D space.</span>
                    </div>
                </div>
                <div>
                    <div class="publication_container_first">
                        <a href="https://arxiv.org/abs/2304.03105" target="_blank"><img loading="lazy" src="/assets/publication/gapretrain.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://arxiv.org/abs/2304.03105" target="_blank">Geometric-aware Pretraining for Vision-centric 3D Object Detection</a></h2>
                        <h4 class="authors">Linyan Huang, Huijie Wang, Jia Zeng, Shengchuan Zhang, Liujuan Cao, Rongrong Ji, Junchi Yan, Hongyang Li</i></h4>
                        <div>
                            <div>
                                <a>arXiv 2023</a>
                            </div>
                        </div>
                        <span>We propose GAPretrain, a plug-and-play framework that boosts 3D detection by pretraining with spatial-structural cues and BEV representation.</span>
                    </div>
                </div>
                <div>
                    <div class="publication_container_first">
                        <a href="https://arxiv.org/abs/2211.10439" target="_blank"><img loading="lazy" src="/assets/publication/bevformerv2.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://arxiv.org/abs/2211.10439" target="_blank">BEVFormer v2: Adapting Modern Image Backbones to Bird's-Eye-View Recognition via Perspective Supervision</a></h2>
                        <h4 class="authors">Chenyu Yang, Xizhou Zhu, Hongyang Li, Jifeng Dai, <i>et al.</i></h4>
                        <div>
                            <div>
                                <a>CVPR 2023 Highlight</a>
                            </div>
                        </div>
                        <span>A novel bird's-eye-view (BEV) detector with perspective supervision, which converges faster and better suits modern image backbones.</span>
                    </div>
                </div>
                <div>
                    <div class="publication_container_first">
                        <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zeng_Distilling_Focal_Knowledge_From_Imperfect_Expert_for_3D_Object_Detection_CVPR_2023_paper.html" target="_blank"><img loading="lazy" src="/assets/publication/fd3d.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://openaccess.thecvf.com/content/CVPR2023/html/Zeng_Distilling_Focal_Knowledge_From_Imperfect_Expert_for_3D_Object_Detection_CVPR_2023_paper.html" target="_blank">Distilling Focal Knowledge from Imperfect Expert for 3D Object Detection</a></h2>
                        <h4 class="authors">Jia Zeng, Li Chen, <i>et al.</i></h4>
                        <div>
                            <div>
                                <a>CVPR 2023</a>
                            </div>
                        </div>
                        <span>We investigate on how to distill the knowledge from an imperfect expert. We propose FD3D, a Focal Distiller for 3D object detection.</span>
                    </div>
                </div>
                <div>
                    <div class="publication_container_first">
                        <a href="https://arxiv.org/abs/2203.11089" target="_blank"><img loading="lazy" src="/assets/publication/persformer.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://arxiv.org/abs/2203.11089" target="_blank">PersFormer: 3D Lane Detection via Perspective Transformer and the OpenLane Benchmark</a></h2>
                        <h4 class="authors">Li Chen, Chonghao Sima, Yang Li, Xiangwei Geng, Junchi Yan, <i>et al.</i></h4>
                        <div>
                            <div>
                                <a>ECCV 2022 (Oral) &nbsp <code>[Redefine the Community]</code></a>
                            </div>
                            <div id="prediction_and_planning">
                                <a href="https://github.com/OpenDriveLab/PersFormer_3DLane" target="_blank"><img loading="lazy" src="https://img.shields.io/github/stars/OpenDriveLab/PersFormer_3DLane?style=social"/></a>
                                <a href="https://github.com/OpenDriveLab/OpenLane" target="_blank"><img loading="lazy" src="/assets/icon/dataset.png"/></a>
                                <a href="https://patrick-llgc.github.io/Learning-Deep-Learning/paper_notes/persformer.html" target="_blank"><img loading="lazy" src="/assets/icon/article.png"/></a>
                                <a href="https://zhuanlan.zhihu.com/p/552908337" target="_blank"><img loading="lazy" src="/assets/icon/zhihu.png"/></a>
                            </div>
                        </div>
                        <span>PersFormer adopts a unified 2D/3D anchor design and an auxiliary task to detect 2D/3D lanes; we release one of the first large-scale real-world 3D lane datasets, OpenLane.</span>
                    </div>
                </div>

            </div>
        </div> 
    </div>



    <br>
    <div class="block" style="margin: 0;">
        <div class="navigator"></div>
    </div>
    <br><br><br>



    <div class="block_left" style="margin-bottom: 0; padding-bottom: 0;">
        <div style="flex-grow: 1;">
            <a class="h1_title" href="#prediction_and_planning">
                <h1>
                    Prediction and Planning
                    <img loading="lazy" class="link_img"/>
                </h1>
            </a>
        </div> 
    </div>
    <br><br>
    <div class="block_left" style="margin-top: 0; padding-top: 0">
        <div style="flex-grow: 1;">
            <div class="publication_container">

                <div>
                    <div class="publication_container_first">
                        <a href="https://openreview.net/forum?id=PZiKO7mjC43" target="_blank"><img loading="lazy" src="/assets/publication/traj_xiaosong.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://openreview.net/forum?id=PZiKO7mjC43" target="_blank">Towards Capturing the Temporal Dynamics for Trajectory Prediction: a Coarse-to-Fine Approach</a></h2>
                        <h4 class="authors">Xiaosong Jia, Li Chen, Penghao Wu, Jia Zeng, <i>et al.</i></h4>
                        <div>
                            <div>
                                <a>CoRL 2022</a>
                            </div>
                        </div>
                        <span>We find taking scratch trajectories generated by MLP as input, a refinement module based on structures with temporal prior, could  boost the accuracy.</span>
                    </div>
                </div>
                <div>
                    <div class="publication_container_first">
                        <a href="https://arxiv.org/abs/2205.09753" target="_blank"><img loading="lazy" src="/assets/publication/hdgt.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://arxiv.org/abs/2205.09753" target="_blank">HDGT: Heterogeneous driving graph transformer for multi-agent trajectory prediction via scene encoding</a></h2>
                        <h4 class="authors">Xiaosong Jia, Penghao Wu, Li Chen, Hongyang Li, Yu Liu, Junchi Yan</i></h4>
                        <div>
                            <div>
                                <a>TPAMI 2023</a>
                            </div>
                            <div id="cv_at_scale">
                                <a href="https://github.com/OpenPerceptionX/HDGT" target="_blank"><img loading="lazy" src="https://img.shields.io/github/stars/OpenPerceptionX/HDGT?style=social"/></a>
                            </div>
                        </div>
                        <span>HDGT formulates the driving scene as a heterogeneous graph with different types of nodes and edges.</span>
                    </div>
                </div>

            </div>
        </div> 
    </div>



    <br>
    <div class="block" style="margin: 0;">
        <div class="navigator"></div>
    </div>
    <br><br><br>



    <div class="block_left" style="margin-bottom: 0; padding-bottom: 0;">
        <div style="flex-grow: 1;">
            <a class="h1_title" href="#cv_at_scale">
                <h1>
                    Computer Vision at Scale
                    <img loading="lazy" class="link_img"/>
                </h1>
            </a>
        </div> 
    </div>
    <br><br>
    <div class="block_left" style="margin-top: 0; padding-top: 0">
        <div style="flex-grow: 1;">

            <div class="publication_container">
                <div>
                    <div class="publication_container_first">
                        <a href="https://arxiv.org/abs/2305.16318" target="_blank"><img loading="lazy" src="/assets/publication/mutr.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://arxiv.org/abs/2305.16318" target="_blank">Referred by Multi-Modality: A Unified Temporal Transformer for Video Object Segmentation</a></h2>
                        <h4 class="authors">Shilin Yan, Renrui Zhang, Ziyu Guo, Wenchao Chen, Wei Zhang, Hongyang Li, Yu Qiao, Zhongjiang He, Peng Gao</h4>
                        <div>
                            <div>
                                <a>AAAI 2024</a>
                            </div>
                            <div>
                                <a href="https://github.com/OpenGVLab/MUTR" target="_blank"><img loading="lazy" src="https://img.shields.io/github/stars/OpenGVLab/MUTR?style=social"/></a>
                            </div>
                        </div>
                    </div>
                </div>
                <div>
                    <div class="publication_container_first">
                        <a href="https://arxiv.org/abs/2211.08887" target="_blank"><img loading="lazy" src="/assets/publication/maskalign.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://arxiv.org/abs/2211.08887" target="_blank">Stare at What You See: Masked Image Modeling without Reconstruction</a></h2>
                        <h4 class="authors">Hongwei Xue, Peng Gao, Hongyang Li, <i>et al.</i></h4>
                        <div>
                            <div>
                                <a>CVPR 2023</a>
                            </div>
                            <div>
                                <a href="https://github.com/OpenDriveLab/maskalign" target="_blank"><img loading="lazy" src="https://img.shields.io/github/stars/OpenDriveLab/maskalign?style=social"/></a>
                            </div>
                        </div>
                        <span>An efficient MIM paradigm MaskAlign and a Dynamic Alignment module to apply learnable alignment to tackle the problem of input inconsistency.</span>
                    </div>
                </div>
                <div>
                    <div class="publication_container_first">
                        <a href="https://arxiv.org/abs/2303.05475" target="_blank"><img loading="lazy" src="/assets/publication/mimic.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://arxiv.org/abs/2303.05475" target="_blank">Mimic before Reconstruct: Enhancing Masked Autoencoders with Feature Mimicking</a></h2>
                        <h4 class="authors">Peng Gao, Renrui Zhang, Rongyao Fang, Ziyi Lin, Hongyang Li, Hongsheng Li, Qiao Yu</h4>
                        <div>
                            <div>
                                <a>IJCV 2023</a>
                            </div>
                            <div>
                                <a href="https://github.com/Alpha-VL/ConvMAE" target="_blank"><img loading="lazy" src="https://img.shields.io/github/stars/Alpha-VL/ConvMAE?style=social"/></a>
                            </div>
                        </div>
                        <span>Introducing high-level and low-level representations to MAE without interference during pre-training.</span>
                    </div>
                </div>
                <div>
                    <div class="publication_container_first">
                        <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_Align_Representations_With_Base_A_New_Approach_to_Self-Supervised_Learning_CVPR_2022_paper.html" target="_blank"><img loading="lazy" src="/assets/publication/arb.jpg"/></a>
                    </div>
                    <div class="publication_container_second">
                        <h2><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_Align_Representations_With_Base_A_New_Approach_to_Self-Supervised_Learning_CVPR_2022_paper.html" target="_blank">Align Representations With Base: A New Approach to Self-Supervised Learning</a></h2>
                        <h4 class="authors">Shaofeng Zhang, Lyn Qiu, Feng Zhu, Junchi Yan, Hengrui Zhang, Rui Zhao, Hongyang Li, Xiaokang Yang</h4>
                        <div>
                            <div>
                                <a>CVPR 2022</a>
                            </div>
                        </div>
                    </div>
                </div>

            </div>
        </div> 
    </div>



    <br>



</body>
