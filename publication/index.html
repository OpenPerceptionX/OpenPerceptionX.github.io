<!DOCTYPE html>
<!--
Template Name: Besloor
Author: <a href="https://www.os-templates.com/">OS Templates</a>
Author URI: https://www.os-templates.com/
Copyright: OS-Templates.com
Licence: Free to use under our free template licence terms
Licence URI: https://www.os-templates.com/template-terms
-->
<html lang="">
<!-- To declare your language - read more here: https://www.w3.org/International/questions/qa-html-language-declarations -->
<head>
<title> Publication | OpenDriveLab 浦驾开放平台</title>

<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
<link href="../layout/styles/layout.css" rel="stylesheet" type="text/css" media="all">
</head>
<body id="top">
<!-- ################################################################################################ -->
<!-- ################################################################################################ -->
<!-- ################################################################################################ -->
<!-- Top Background Image Wrapper -->
<div class="bgded overlay " style="background-image:url('../images/gallery/stage-publications.jpeg');"> 
  

  <!-- ################################################################################################ -->
  <div class="wrapper row0">
    <div id="topbar" class="hoc clear"> 
      <!-- ################################################################################################ -->
      <div class="fl_left"> 
        <!-- ################################################################################################ -->
        <ul class="nospace">
<!--           <li><i class="fas fa-phone"></i> +00 (123) 456 7890</li>
          <li><i class="far fa-envelope"></i> info@domain.com</li> -->
        </ul>
        <!-- ################################################################################################ -->
      </div>
      <div class="fl_right"> 
        <!-- ################################################################################################ -->
        <ul class="nospace">
<!--           <li><a href="index.html"><i class="fas fa-home"></i></a></li>
          <li><a href="#" title="Help Centre"><i class="fas fa-life-ring"></i></a></li>
          <li><a href="#" title="Login"><i class="fas fa-sign-in-alt"></i></a></li>
          <li><a href="#" title="Sign Up"><i class="fas fa-edit"></i></a></li> -->
        </ul>
        <!-- ################################################################################################ -->
      </div>
      <!-- ################################################################################################ -->
    </div>
  </div>
  <!-- ################################################################################################ -->
  <!-- ################################################################################################ -->
  <!-- ################################################################################################ -->
  <div class="wrapper row1">
    <header id="header" class="hoc clear"> 
      <!-- ################################################################################################ -->
      <div id="logo" class="fl_left">
        <h1>OpenDriveLab | 浦驾自动驾驶开放平台</h1>
      </div>
      <!-- ################################################################################################ -->
      <nav id="mainav" class="fl_right">
        <ul class="clear">
          <li><a href="../index.html">Home</a></li>
          <!-- <li><a class="drop" href="../research/index.html">Research</a>
          	 <ul>
              <li><a href="../research/bev.html">BEV Perception</a></li>
              <li><a href="../research/e2e.html">End-to-end Autonomous Driving</a></li>
              <li><a href="../research/openlane.html">OpenLane</a></li>
            </ul>
          </li> -->
          <li><a class="drop">Event</a>
            <ul>
              <li><a href="../sr4ad/iclr23.html">ICLR 2023 Workshop</a></li>
              <li><a href="../event/cvpr23_ADworkshop.html">CVPR 2023 Workshop</a></li>

              <!-- <li><a class="drop">Past Events</a>
                <ul>
                  <li><a href="https://zhuanlan.zhihu.com/p/573144047">PRCV 2022 自动驾驶专业论坛（主办）</a></li>
                  <li><a href="https://docs.google.com/presentation/d/1DBJ417-31FPvodk4mK5BG_2NNL6nHW3jR8IMFJZlJpI/edit?usp=sharing">VALSE 2022 专业论坛报告</a></li>
                  <li><a href="https://docs.google.com/presentation/d/1cfBy_1X10AQOx0NbF6izfqJkhViTVGlQ3JcQ9wokrfo/edit?usp=sharing">ECCV 2022 Workshop Talk </a></li>
                </ul>
              </li>     -->
            </ul>
          </li>
          <li><a target="_blank" href="https://www.zhihu.com/people/PerceptionX/posts">Blog In Chinese <i class="fas fa-external-link-square-alt"></i></a></li>
          <li class="active"><a href="../publication/index.html">Publication</a></li>
        </ul>
      </nav>
      <!-- ################################################################################################ -->
    </header>
  </div>
  <!-- ################################################################################################ -->
  <!-- ################################################################################################ -->
  <!-- ################################################################################################ -->
  
  <div id="breadcrumb" class="hoc clear"> 
    <!-- ################################################################################################ -->
    <h2 class="heading">Publications</h2>
    <!-- ################################################################################################ -->
  </div>
</div>

<div class="wrapper row3">
  <main class="hoc container clear"> 
    <!-- main body -->
    <ul class="nospace group overview btmspace-80">
      <div class="sidebar one_quarter first"> 
        <!-- ################################################################################################ -->
        <h6>Navigation</h6>
        <nav class="sdb_holder">
          <!-- <div class="fix"> -->

          <ul>
            <h6>Publications</h6>
            <li><a href="#Selected Publications">Selected Publications</a></li> </ul>
            <ul><li><a href="#Autonomous Driving">Autonomous Driving</a></li></ul>
            <ul><li><a href="#General Vision">General Vision</a></li>
        
          </ul>
        </nav>
        <!-- <div class="sdb_holder">
          <h6>Lorem ipsum dolor</h6>
          <address>
          Full Name<br>
          Address Line 1<br>
          Address Line 2<br>
          Town/City<br>
          Postcode/Zip<br>
          <br>
          Tel: xxxx xxxx xxxxxx<br>
          Email: <a href="#">contact@domain.com</a>
          </address>
        </div>
        <div class="sdb_holder">
          <article>
            <h6>Lorem ipsum dolor</h6>
            <p>Nuncsed sed conseque a at quismodo tris mauristibus sed habiturpiscinia sed.</p>
            <ul>
              <li><a href="#">Lorem ipsum dolor sit</a></li>
              <li>Etiam vel sapien et</li>
              <li><a href="#">Etiam vel sapien et</a></li>
            </ul>
            <p>Nuncsed sed conseque a at quismodo tris mauristibus sed habiturpiscinia sed. Condimentumsantincidunt dui mattis magna intesque purus orci augue lor nibh.</p>
            <p class="more"><a href="#">Continue Reading &raquo;</a></p>
          </article>
        </div> -->
        <!-- ################################################################################################ -->

    </div>
      <!-- ################################################################################################ -->
      
      <div class="content three_quarter"> 

        <article>

          <!-- ################################################################################################ --> 


          <h3><b><p class="btb" target="_blank" id="Selected Publications" >Selected Publications
          </p></b></h3>



          <div><div class="wsite-multicol"><div class="wsite-multicol-table-wrap" style="margin:0 -15px;">
            <table class="wsite-multicol-table">
              <tbody class="wsite-multicol-tbody">
                <tr class="wsite-multicol-tr">
                  <td class="wsite-multicol-col" style="width:26.598465473146%; padding:0 15px;">
                    
                      
          
                <!-- 11################################################################################################ --> 

                <div><div class="wsite-multicol"><div class="wsite-multicol-table-wrap" style="margin:0 -15px;">
                  <table class="wsite-multicol-table">
                    <tbody class="wsite-multicol-tbody">
                      <tr class="wsite-multicol-tr">
                        <td class="wsite-multicol-col" style="width:26.598465473146%; padding:0 15px;">
                          
                            
                
                <div><div class="wsite-image wsite-image-border-none " style="padding-top:10px;padding-bottom:10px;margin-left:0;margin-right:0;text-align:center">
                <a>
                <img src="../images/pub/delv.png" alt="Picture" style="width:auto;max-width:100%;height: 200px;;" />
                </a>
                <div style="display:block;font-size:90%"></div>
                </div></div>
                
                
                          
                        </td>				<td class="wsite-multicol-col" style="width:73.401534526854%; padding:0 15px;">
                          
                            
                
                <div class="paragraph">         <p class="bts" target="_blank" >3. <b>Hongyang Li</b>*<b>, Chonghao Sima, Jifeng Dai, Wenhai Wang, Lewei Lu, Huijie Wang, Enze Xie, Zhiqi Li, Hanming Deng, Hao Tian</b>, Xizhou Zhu, Li Chen, Yulu Gao, Xiangwei Geng, Jia Zeng, Yang Li, Jiazhi Yang, Xiaosong Jia, Bohan Yu, Yu Qiao, Dahua Lin, Si Liu, Junchi Yan, Jianping Shi, Ping Luo, "Delving into the Devils of Bird"s-Eye-View Perception: A Review, Evaluation and Recipe" (arXiv, 2022) <body>
                  <a href="https://doi.org/10.48550/arXiv.2209.05324" style="color: #0040ff;">[Paper]</a>
                </body>
                <body>
                  <a href="https://github.com/openperceptionx/bevperception-survey-recipe" style="color: #FF0000;">[Code]</a>
                </body><iframe
                style="margin-left: 2px; margin-bottom:-5px;"
                frameborder="0" scrolling="0" width="91px" height="20px"
                src="https://ghbtns.com/github-btn.html?user=openperceptionx&repo=bevperception-survey-recipe&type=star&count=true" >
               </iframe></p>
              <a style="color: gray;font-style:italic;">
                We review the most recent work on BEV perception and provide anin-depth analysis of different solutions. Moreover, several systematic designs of BEV approach from the industry are depicted as well.Furthermore, we introduce a full suite of practical guidebook to improve the performance of BEV perception tasks, including camera,LiDAR and fusion inputs.   
            </a>
              </p></div>
                                    
                        </td>			</tr>
                    </tbody>
                  </table>
                </div></div></div>

                    <!-- ##5############################################################################################## --> 

          <div><div class="wsite-multicol"><div class="wsite-multicol-table-wrap" style="margin:0 -15px;">
            <table class="wsite-multicol-table">
              <tbody class="wsite-multicol-tbody">
                <tr class="wsite-multicol-tr">
                  <td class="wsite-multicol-col" style="width:26.598465473146%; padding:0 15px;">
                    
                      
          
          <div><div class="wsite-image wsite-image-border-none " style="padding-top:10px;padding-bottom:10px;margin-left:0;margin-right:0;text-align:center">
          <a>
          <img src="../images/pub/bevf.png" alt="Picture" style="width:auto;max-width:100%;height:200px;" />
          </a>
          <div style="display:block;font-size:90%"></div>
          </div></div>
          
          
                    
                  </td>				<td class="wsite-multicol-col" style="width:73.401534526854%; padding:0 15px;">
                    
                      
          
          <div class="paragraph">        <p class="bts" target="_blank" >2. <b>Zhiqi Li, Wenhai Wang, Hongyang Li</b>, Enze Xie, Chonghao Sima, Tong Lu, Qiao Yu, Jifeng Dai*, "BEVFormer: Learning Bird's-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers", <b>ECCV 2022</b>  <body>
            <a href="https://doi.org/10.48550/arXiv.2203.17270" style="color: #0040ff;">[Paper]</a>
          </body>
          <body>
            <a href="https://github.com/fundamentalvision/BEVFormer" style="color: #FF0000;">[Code]</a>
          </body><iframe
          style="margin-left: 2px; margin-bottom:-5px;"
          frameborder="0" scrolling="0" width="191px" height="20px"
          src="https://ghbtns.com/github-btn.html?user=fundamentalvision&repo=BEVFormer&type=star&count=true" >
         </iframe></p>
         We proposeBEVFormer, a paradigm for autonomous driving that applies both Transformerand Temporal structure to generate bird’s-eye-view (BEV) features from multi-camera inputs. BEV-Former leverages queries to lookup spatial/temporal space and aggregate spatiotemporal informationcorrespondingly, hence benefiting stronger representations for perception tasks.          </p></div>
                              
                  </td>			</tr>
              </tbody>
            </table>
          </div></div></div>

          <!-- ##4############################################################################################## --> 

          <div><div class="wsite-multicol"><div class="wsite-multicol-table-wrap" style="margin:0 -15px;">
            <table class="wsite-multicol-table">
              <tbody class="wsite-multicol-tbody">
                <tr class="wsite-multicol-tr">
                  <td class="wsite-multicol-col" style="width:26.598465473146%; padding:0 15px;">
                    
                      
          
          <div><div class="wsite-image wsite-image-border-none " style="padding-top:10px;padding-bottom:10px;margin-left:0;margin-right:0;text-align:center">
          <a>
          <img src="../images/pub/pers.png" alt="Picture" style="width:auto;max-width:100%;height:200px;" />
          </a>
          <div style="display:block;font-size:90%"></div>
          </div></div>
          
          
                    
                  </td>				<td class="wsite-multicol-col" style="width:73.401534526854%; padding:0 15px;">
                    
                      
          
                    <p class="bts" target="_blank" >1. <b>Li Chen</b>*, <b>Chonghao Sima, Yang Li</b>, Zehan Zheng, Jiajie Xu, Xiangwei Geng, Hongyang Li, Conghui He, Jianping Shi, Yu Qiao, Junchi Yan, "PersFormer: 3D Lane Detection via Perspective Transformer and the OpenLane Benchmark", <b>ECCV 2022 (Oral) </b>  <body>
                      <a href="https://doi.org/10.48550/arXiv.2203.11089" style="color: #0040ff;">[Paper]</a>
                    </body>
                    <body>
                      <a href="https://github.com/OpenPerceptionX/OpenLane" style="color: #FF0000;">[Code]</a>
                    </body><iframe
                    style="margin-left: 2px; margin-bottom:-5px;"
                    frameborder="0" scrolling="0" width="91px" height="20px"
                    src="https://ghbtns.com/github-btn.html?user=OpenPerceptionX&repo=OpenLane&type=star&count=true" >
                   </iframe></p>
                   PersFormer  adopts  a  unified  2D/3D  anchor  design  andan auxiliary task to detect 2D/3D lanes simultaneously, enhancing thefeature consistency and sharing the benefits of multi-task learning. More-over, we release one of the first large-scale real-world 3D lane datasets:OpenLane,  with  high-quality  annotation  and  scenario  diversity.   </p></div>
                   <!-- Open-Lane contains 200,000 frames, over 880,000 instance-level lanes, 14 lanecategories, along with scene tags and the closed-in-path object annota-tions to encourage the development of lane detection and more industrial-related autonomous driving methods.          </p></div> -->
                              
                  </td>			</tr>
              </tbody>
            </table>
          </div></div></div>

                    

<!-- ################################################################################################ -->

<!-- 

            <div><div class="wsite-multicol"><div class="wsite-multicol-table-wrap" style="margin:0 -15px;">
              <table class="wsite-multicol-table">
                <tbody class="wsite-multicol-tbody">
                  <tr class="wsite-multicol-tr">
                    <td class="wsite-multicol-col" style="width:26.598465473146%; padding:0 15px;">
                      
                        

            <div><div class="wsite-image wsite-image-border-none " style="padding-top:10px;padding-bottom:10px;margin-left:0;margin-right:0;text-align:center">
            <a>
            <img src="../images/pub/Policy.png" alt="Picture" style="width:auto;max-width:100%;height:150px;" />
            </a>
            <div style="display:block;font-size:90%"></div>
            </div></div>


                      
                    </td>				<td class="wsite-multicol-col" style="width:73.401534526854%; padding:0 15px;">
                      
                        
            <div class="paragraph">
              <p class="bts" target="_blank" >
              <strong style="color:rgb(98, 98, 98)">
              Policy Pre-Training for End-to-End Autonomous Driving via Self-Supervised Geometric Modeling                <br />
                &#8203;</span></strong><span>
                  <b>Penghao Wu</b>, Li Chen, Hongyang Li*, Xiaosong Jia, Junchi Yan, Yu Qiao</u><br />
                  <span style="color:rgb(98, 98, 98)">ArXiv 2023</span><body>
                    <a href="https://doi.org/10.48550/arXiv.2301.01006" style="color: #0040ff;">[Paper]</a></body>
                    <body>
                    <a href="https://github.com/opendrivelab/ppgeo" style="color: #FF0000;">[Code]</a></body>
      
                    <iframe
                    style="margin-left: 2px; margin-bottom:-5px;"
                    frameborder="0" scrolling="0" width="91px" height="20px"
                    src="https://ghbtns.com/github-btn.html?user=opendrivelab&repo=ppgeo&type=star&count=true" >
                  </iframe><br />
                  <br>
                  We  proposePPGeo(Policy  Pre-training  via  Geometric  modeling),  an  intuitiveand straightforward fully self-supervised framework curated for the policy pre-training in visuomotor driving.  
                  
                </p>
                </div>

                
                    </td>			</tr>
                </tbody>
              </table>
            </div></div></div> -->





            <br><br>
                      
            <h3><b><p class="btb" target="_blank" id="Autonomous Driving" >Autonomous Driving</p></b></h3>
<!-- #####14########################################################################################### --> 

          <div><div class="wsite-multicol"><div class="wsite-multicol-table-wrap" style="margin:0 -15px;">
            <table class="wsite-multicol-table">
              <tbody class="wsite-multicol-tbody">
                <tr class="wsite-multicol-tr">
                  <td class="wsite-multicol-col" style="width:26.598465473146%; padding:0 15px;">
                    
                      
          
          <div><div class="wsite-image wsite-image-border-none " style="padding-top:10px;padding-bottom:10px;margin-left:0;margin-right:0;text-align:center">
          <a>
          <img src="../images/pub/Policy.png" alt="Picture" style="width:auto;max-width:100%;height:150px;" />
          </a>
          <div style="display:block;font-size:90%"></div>
          </div></div>
          
          
                    
                  </td>				<td class="wsite-multicol-col" style="width:73.401534526854%; padding:0 15px;">
                    
                      
          
          <div class="paragraph">          <p class="bts" target="_blank" >14. <b>Penghao Wu</b>, Li Chen, Hongyang Li*, Xiaosong Jia, Junchi Yan, Yu Qiao, "Policy Pre-Training for End-to-End Autonomous Driving via Self-Supervised Geometric Modeling" (arXiv, 2023) 
            <body>
            <a href="https://doi.org/10.48550/arXiv.2301.01006" style="color: #0040ff;">[Paper]</a></body>
            <body>
            <a href="https://github.com/opendrivelab/ppgeo" style="color: #FF0000;">[Code]</a></body>

            <iframe
            style="margin-left: 2px; margin-bottom:-5px;"
            frameborder="0" scrolling="0" width="91px" height="20px"
            src="https://ghbtns.com/github-btn.html?user=opendrivelab&repo=ppgeo&type=star&count=true" >
           </iframe>
           <br><br>
           We  propose PPGeo(Policy  Pre-training  via  Geometric  modeling),  an  intuitiveand straightforward fully self-supervised framework curated for the policy pre-training in visuomotor driving.  
          </p></div>
                              
                  </td>			</tr>
              </tbody>
            </table>
          </div></div></div>



<!-- #####13########################################################################################### --> 

          <div><div class="wsite-multicol"><div class="wsite-multicol-table-wrap" style="margin:0 -15px;">
            <table class="wsite-multicol-table">
              <tbody class="wsite-multicol-tbody">
                <tr class="wsite-multicol-tr">
                  <td class="wsite-multicol-col" style="width:26.598465473146%; padding:0 15px;">
                    
                      
          
          <div><div class="wsite-image wsite-image-border-none " style="padding-top:10px;padding-bottom:10px;margin-left:0;margin-right:0;text-align:center">
          <a>
          <img src="../images/pub/goal.png" alt="Picture" style="width:auto;max-width:100%;height:180px;" />
          </a>
          <div style="display:block;font-size:90%"></div>
          </div></div>
          
          
                    
            </td>				<td class="wsite-multicol-col" style="width:73.401534526854%; padding:0 15px;">
              
                
    
              <p class="bts" target="_blank" >13. <b>Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li</b>, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, Lewei Lu, Xiaosong Jia, Qiang Liu, Jifeng Dai, Yu Qiao, Hongyang Li*, "Goal-Oriented Autonomous Driving" (arXiv, 2022)          
                <body>
                <a href="https://doi.org/10.48550/arXiv.2212.10156" style="color: #0040ff;">[Paper]</a>
              </body>
              <body>
                <a href="https://github.com/OpenDriveLab/UniAD" style="color: #FF0000;">[Code]</a>
              </body><iframe
              style="margin-left: 2px; margin-bottom:-5px;"
              frameborder="0" scrolling="0" width="91px" height="20px"
              src="https://ghbtns.com/github-btn.html?user=OpenDriveLab&repo=UniAD&type=star&count=true" >
              </iframe>
          
           <br><br>
           We introduce Unified Au-tonomous Driving (UniAD), the first comprehensive frame-work  up-to-date  that  incorporates  full-stack  driving  tasksin one network.  It is exquisitely devised to leverage advan-tages of each module, and provide complementary feature abstractions for agent interaction from a global perspective.          </p></div>
         
                  </td>			</tr>
              </tbody>
            </table>
          </div></div></div>

          <!-- ###12############################################################################################# --> 

          <div><div class="wsite-multicol"><div class="wsite-multicol-table-wrap" style="margin:0 -15px;">
            <table class="wsite-multicol-table">
              <tbody class="wsite-multicol-tbody">
                <tr class="wsite-multicol-tr">
                  <td class="wsite-multicol-col" style="width:26.598465473146%; padding:0 15px;">
                    
                      
          
          <div><div class="wsite-image wsite-image-border-none " style="padding-top:10px;padding-bottom:10px;margin-left:0;margin-right:0;text-align:center">
          <a>
          <img src="../images/pub/befv2.png" alt="Picture" style="width:auto;max-width:100%;height:180px;" />
          </a>
          <div style="display:block;font-size:90%"></div>
          </div></div>
          
          
                    
                  </td>				<td class="wsite-multicol-col" style="width:73.401534526854%; padding:0 15px;">
                    
                      
          
          <div class="paragraph">                    <p class="bts" target="_blank" >12. <b>Chenyu Yang, Yuntao Chen, Hao Tian</b>, Chenxin Tao, Xizhou Zhu, Zhaoxiang Zhang, Gao Huang, Hongyang Li, Yu Qiao, Lewei Lu, Jie Zhou, Jifeng Dai*, "BEVFormer v2: Adapting Modern Image Backbones to Bird's-Eye-View Recognition via Perspective Supervision" (arXiv, 2022)  <body>
            <a href="https://doi.org/10.48550/arXiv.2211.10439" style="color: #0040ff;">[Paper]</a>
          </body>
         </p>
         We present a novel bird’s-eye-view (BEV) detector withperspective  supervision,  which  converges  faster  and  bet-ter  suits  modern  image  backbones. The proposed method is ver-ified with a wide spectrum of traditional and modern imagebackbones and achieves new SoTA results on the large-scalenuScenes dataset. 
          </p></div>
                              
                  </td>			</tr>
              </tbody>
            </table>
          </div></div></div>

          <!-- 11################################################################################################ --> 

          <div><div class="wsite-multicol"><div class="wsite-multicol-table-wrap" style="margin:0 -15px;">
            <table class="wsite-multicol-table">
              <tbody class="wsite-multicol-tbody">
                <tr class="wsite-multicol-tr">
                  <td class="wsite-multicol-col" style="width:26.598465473146%; padding:0 15px;">
                    
                      
          
          <div><div class="wsite-image wsite-image-border-none " style="padding-top:10px;padding-bottom:10px;margin-left:0;margin-right:0;text-align:center">
          <a>
          <img src="../images/pub/delv.png" alt="Picture" style="width:auto;max-width:100%;height:260px;" />
          </a>
          <div style="display:block;font-size:90%"></div>
          </div></div>
          
          
                    
                  </td>				<td class="wsite-multicol-col" style="width:73.401534526854%; padding:0 15px;">
                    
                      
          
          <div class="paragraph">         <p class="bts" target="_blank" >11. <b>Hongyang Li</b>*<b>, Chonghao Sima, Jifeng Dai, Wenhai Wang, Lewei Lu, Huijie Wang, Enze Xie, Zhiqi Li, Hanming Deng, Hao Tian</b>, Xizhou Zhu, Li Chen, Yulu Gao, Xiangwei Geng, Jia Zeng, Yang Li, Jiazhi Yang, Xiaosong Jia, Bohan Yu, Yu Qiao, Dahua Lin, Si Liu, Junchi Yan, Jianping Shi, Ping Luo, "Delving into the Devils of Bird"s-Eye-View Perception: A Review, Evaluation and Recipe" (arXiv, 2022) <body>
            <a href="https://doi.org/10.48550/arXiv.2209.05324" style="color: #0040ff;">[Paper]</a>
          </body>
          <body>
            <a href="https://github.com/openperceptionx/bevperception-survey-recipe" style="color: #FF0000;">[Code]</a>
          </body><iframe
          style="margin-left: 2px; margin-bottom:-5px;"
          frameborder="0" scrolling="0" width="91px" height="20px"
          src="https://ghbtns.com/github-btn.html?user=openperceptionx&repo=bevperception-survey-recipe&type=star&count=true" >
         </iframe></p>
         We review the most recent work on BEV perception and provide anin-depth analysis of different solutions. Moreover, several systematic designs of BEV approach from the industry are depicted as well.Furthermore, we introduce a full suite of practical guidebook to improve the performance of BEV perception tasks, including camera,LiDAR and fusion inputs.   
        </p></div>
                              
                  </td>			</tr>
              </tbody>
            </table>
          </div></div></div>

          <!-- ##10############################################################################################## --> 

          <div><div class="wsite-multicol"><div class="wsite-multicol-table-wrap" style="margin:0 -15px;">
            <table class="wsite-multicol-table">
              <tbody class="wsite-multicol-tbody">
                <tr class="wsite-multicol-tr">
                  <td class="wsite-multicol-col" style="width:26.598465473146%; padding:0 15px;">
                    
                      
          
          <div><div class="wsite-image wsite-image-border-none " style="padding-top:10px;padding-bottom:10px;margin-left:0;margin-right:0;text-align:center">
          <a>
          <img src="../images/pub/towards.png" alt="Picture" style="width:auto;max-width:100%;height:180px;" />
          </a>
          <div style="display:block;font-size:90%"></div>
          </div></div>
          
          
                    
                  </td>				<td class="wsite-multicol-col" style="width:73.401534526854%; padding:0 15px;">
                    
                      
          
          <div class="paragraph">         <p class="bts" target="_blank" >10. <b>Xiaosong Jia</b>, Li Chen, Penghao Wu, Jia Zeng, Junchi Yan*, Hongyang Li, Yu Qiao, "Towards Capturing the Temporal Dynamics for Trajectory Prediction: A Coarse-to-Fine Approach", <b>CoRL 2022</b>
            <body>
            <a href="https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=Hfrih1EAAAAJ&sortby=pubdate&citation_for_view=Hfrih1EAAAAJ:VaXvl8Fpj5cC" style="color: #0040ff;">[Paper]</a>
          </body></p>
          We examine this design choice and investigate several ways to apply the temporal inductivebias into the generation of future trajectories on top of a SOTA encoder.  We findthat simply using autoregressive RNN to generate future positions would lead tosignificant performance drop even with techniques such as history highway andteacher forcing.          </p></div>
                              
                  </td>			</tr>
              </tbody>
            </table>
          </div></div></div>

          <!-- ###9############################################################################################# --> 

          <div><div class="wsite-multicol"><div class="wsite-multicol-table-wrap" style="margin:0 -15px;">
            <table class="wsite-multicol-table">
              <tbody class="wsite-multicol-tbody">
                <tr class="wsite-multicol-tr">
                  <td class="wsite-multicol-col" style="width:26.598465473146%; padding:0 15px;">
                    
                      
          
          <div><div class="wsite-image wsite-image-border-none " style="padding-top:10px;padding-bottom:10px;margin-left:0;margin-right:0;text-align:center">
          <a>
          <img src="../images/pub/stp3.png" alt="Picture" style="width:auto;max-width:100%;height:180px;" />
          </a>
          <div style="display:block;font-size:90%"></div>
          </div></div>
          
          
                    
                  </td>				<td class="wsite-multicol-col" style="width:73.401534526854%; padding:0 15px;">
                    
                      
          
          <div class="paragraph">          <p class="bts" target="_blank" >9. <b>Shengchao Hu</b>, Li Chen*, Penghao Wu, Hongyang Li, Junchi Yan, Dacheng Tao, "ST-P3: End-to-End Vision-Based Autonomous Driving via Spatial-Temporal Feature Learning", <b>ECCV 2022</b>  <body>
            <a href="https://doi.org/10.48550/arXiv.2207.07601" style="color: #0040ff;">[Paper]</a>
          </body>
          <body>
            <a href="https://github.com/openperceptionx/st-p3" style="color: #FF0000;">[Code]</a>
          </body><iframe
          style="margin-left: 2px; margin-bottom:-5px;"
          frameborder="0" scrolling="0" width="91px" height="20px"
          src="https://ghbtns.com/github-btn.html?user=openperceptionx&repo=st-p3&type=star&count=true" >
         </iframe></p>
         To  the  best  of  our  knowledge,  we  are  the  firstto  systematically  investigate  each  part  of  an  interpretable  end-to-endvision-based autonomous driving system. We benchmark our approachagainst previous state-of-the-arts on both open-loop nuScenes dataset aswell as closed-loop CARLA simulation.           </p></div>
                              
                  </td>			</tr>
              </tbody>
            </table>
          </div></div></div>

          <!-- #8############################################################################################### --> 

          <div><div class="wsite-multicol"><div class="wsite-multicol-table-wrap" style="margin:0 -15px;">
            <table class="wsite-multicol-table">
              <tbody class="wsite-multicol-tbody">
                <tr class="wsite-multicol-tr">
                  <td class="wsite-multicol-col" style="width:26.598465473146%; padding:0 15px;">
                    
                      
          
          <div><div class="wsite-image wsite-image-border-none " style="padding-top:10px;padding-bottom:10px;margin-left:0;margin-right:0;text-align:center">
          <a>
          <img src="../images/pub/lv2.png" alt="Picture" style="width:auto;max-width:100%;height:150px;" />
          </a>
          <div style="display:block;font-size:90%"></div>
          </div></div>
          
          
                    
                  </td>				<td class="wsite-multicol-col" style="width:73.401534526854%; padding:0 15px;">
                    
                      
          
          <div class="paragraph">          <p class="bts" target="_blank" >8. <b>Li Chen</b>*<b>, Tutian Tang, Zhitian Cai, Yang Li</b>, Penghao Wu, Hongyang Li*, Jianping Shi, Junchi Yan, Yu Qiao, "Level 2 Autonomous Driving on a Single Device: Diving into the Devils of Openpilot" (arXiv, 2022)  <body>
            <a href="https://doi.org/10.48550/arXiv.2206.08176" style="color: #0040ff;">[Paper]</a>
          </body>
          <body>
            <a href="https://github.com/openperceptionx/openpilot-deepdive" style="color: #FF0000;">[Code]</a>
          </body><iframe
          style="margin-left: 2px; margin-bottom:-5px;"
          frameborder="0" scrolling="0" width="91px" height="20px"
          src="https://ghbtns.com/github-btn.html?user=openperceptionx&repo=openpilot-deepdive&type=star&count=true" >
         </iframe></p>

         For a fair comparison of our version to the originalSupercombo, we introduce a dual-model deployment scheme to test the drivingperformance in the real world.          </p></div>
                              
                  </td>			</tr>
              </tbody>
            </table>
          </div></div></div>

          <!-- ###7############################################################################################# --> 

          <div><div class="wsite-multicol"><div class="wsite-multicol-table-wrap" style="margin:0 -15px;">
            <table class="wsite-multicol-table">
              <tbody class="wsite-multicol-tbody">
                <tr class="wsite-multicol-tr">
                  <td class="wsite-multicol-col" style="width:26.598465473146%; padding:0 15px;">
                    
                      
          
          <div><div class="wsite-image wsite-image-border-none " style="padding-top:10px;padding-bottom:10px;margin-left:0;margin-right:0;text-align:center">
          <a>
          <img src="../images/pub/traj.png" alt="Picture" style="width:auto;max-width:100%;height:150px;" />
          </a>
          <div style="display:block;font-size:90%"></div>
          </div></div>
          
          
                    
                  </td>				<td class="wsite-multicol-col" style="width:73.401534526854%; padding:0 15px;">
                    
                      
          
          <div class="paragraph">          <p class="bts" target="_blank" >7. <b>Penghao Wu</b>*<b>, Xiaosong Jia, Li Chen</b>, Junchi Yan*, Hongyang Li, Yu Qiao, "Trajectory-Guided Control Prediction for End-to-End Autonomous Driving: A Simple yet Strong Baseline", <b>NeurIPS 2022</b>  <body>
            <a href="https://doi.org/10.48550/arXiv.2206.08129" style="color: #0040ff;">[Paper]</a>
          </body>
          <body>
            <a href="https://github.com/OpenPerceptionX/TCP" style="color: #FF0000;">[Code]</a>
          </body><iframe
          style="margin-left: 2px; margin-bottom:-5px;"
          frameborder="0" scrolling="0" width="91px" height="20px"
          src="https://ghbtns.com/github-btn.html?user=OpenPerceptionX&repo=TCP&type=star&count=true" >
         </iframe></p>

         Specifically, our integrated approach has two branches fortrajectory planning and direct control, respectively. The trajectory branch predictsthe future trajectory, while the control branch involves a novel multi-step predictionscheme such that the relationship between current actions and future states can bereasoned.           </p></div>
                              
                  </td>			</tr>
              </tbody>
            </table>
          </div></div></div>

          <!-- ##6############################################################################################## --> 

          <div><div class="wsite-multicol"><div class="wsite-multicol-table-wrap" style="margin:0 -15px;">
            <table class="wsite-multicol-table">
              <tbody class="wsite-multicol-tbody">
                <tr class="wsite-multicol-tr">
                  <td class="wsite-multicol-col" style="width:26.598465473146%; padding:0 15px;">
                    
                      
          
          <div><div class="wsite-image wsite-image-border-none " style="padding-top:10px;padding-bottom:10px;margin-left:0;margin-right:0;text-align:center">
          <a>
          <img src="../images/pub/htgd.png" alt="Picture" style="width:auto;max-width:100%;height:200px;" />
          </a>
          <div style="display:block;font-size:90%"></div>
          </div></div>
          
          
                    
                  </td>				<td class="wsite-multicol-col" style="width:73.401534526854%; padding:0 15px;">
                    
                      
          
          <div class="paragraph">         <p class="bts" target="_blank" >6. <b>Xiaosong Jia</b>, Penghao Wu, Li Chen, Hongyang Li, Yu Liu, Junchi Yan*, "HDGT: Heterogeneous Driving Graph Transformer for Multi-Agent Trajectory Prediction via Scene Encoding" (arXiv, 2022) <body>
            <a href="https://doi.org/10.48550/arXiv.2205.09753" style="color: #0040ff;">[Paper]</a>
          </body><body>
            <a href="https://github.com/OpenPerceptionX/HDGT
            " style="color: #FF0000;">[Code]</a>
          </body><iframe
          style="margin-left: 2px; margin-bottom:-5px;"
          frameborder="0" scrolling="0" width="91px" height="20px"
          src="https://ghbtns.com/github-btn.html?user=OpenPerceptionX&repo=HDGT&type=star&count=true" >
         </iframe></p>
         We propose a novel backbone, namely Heterogeneous Driving Graph Transformer (HDGT), whichmodels the driving scene as a heterogeneous graph with different types of nodes and edges. For graph construction, each noderepresents either an agent or a road element and each edge represents their semantics relations such as Pedestrian-To-Crosswalk,Lane-To-Left-Lane.           </p></div>
                              
                  </td>			</tr>
              </tbody>
            </table>
          </div></div></div>

          <!-- ##5############################################################################################## --> 

          <div><div class="wsite-multicol"><div class="wsite-multicol-table-wrap" style="margin:0 -15px;">
            <table class="wsite-multicol-table">
              <tbody class="wsite-multicol-tbody">
                <tr class="wsite-multicol-tr">
                  <td class="wsite-multicol-col" style="width:26.598465473146%; padding:0 15px;">
                    
                      
          
          <div><div class="wsite-image wsite-image-border-none " style="padding-top:10px;padding-bottom:10px;margin-left:0;margin-right:0;text-align:center">
          <a>
          <img src="../images/pub/bevf.png" alt="Picture" style="width:auto;max-width:100%;height:200px;" />
          </a>
          <div style="display:block;font-size:90%"></div>
          </div></div>
          
          
                    
                  </td>				<td class="wsite-multicol-col" style="width:73.401534526854%; padding:0 15px;">
                    
                      
          
          <div class="paragraph">        <p class="bts" target="_blank" >5. <b>Zhiqi Li, Wenhai Wang, Hongyang Li</b>, Enze Xie, Chonghao Sima, Tong Lu, Qiao Yu, Jifeng Dai*, "BEVFormer: Learning Bird's-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers", <b>ECCV 2022</b>  <body>
            <a href="https://doi.org/10.48550/arXiv.2203.17270" style="color: #0040ff;">[Paper]</a>
          </body>
          <body>
            <a href="https://github.com/fundamentalvision/BEVFormer" style="color: #FF0000;">[Code]</a>
          </body><iframe
          style="margin-left: 2px; margin-bottom:-5px;"
          frameborder="0" scrolling="0" width="191px" height="20px"
          src="https://ghbtns.com/github-btn.html?user=fundamentalvision&repo=BEVFormer&type=star&count=true" >
         </iframe></p>
         We propose BEVFormer, a paradigm for autonomous driving that applies both Transformerand Temporal structure to generate bird’s-eye-view (BEV) features from multi-camera inputs. BEV-Former leverages queries to lookup spatial/temporal space and aggregate spatiotemporal informationcorrespondingly, hence benefiting stronger representations for perception tasks.          </p></div>
                              
                  </td>			</tr>
              </tbody>
            </table>
          </div></div></div>

          <!-- ##4############################################################################################## --> 

          <div><div class="wsite-multicol"><div class="wsite-multicol-table-wrap" style="margin:0 -15px;">
            <table class="wsite-multicol-table">
              <tbody class="wsite-multicol-tbody">
                <tr class="wsite-multicol-tr">
                  <td class="wsite-multicol-col" style="width:26.598465473146%; padding:0 15px;">
                    
                      
          
          <div><div class="wsite-image wsite-image-border-none " style="padding-top:10px;padding-bottom:10px;margin-left:0;margin-right:0;text-align:center">
          <a>
          <img src="../images/pub/pers.png" alt="Picture" style="width:auto;max-width:100%;height:200px;" />
          </a>
          <div style="display:block;font-size:90%"></div>
          </div></div>
          
          
                    
                  </td>				<td class="wsite-multicol-col" style="width:73.401534526854%; padding:0 15px;">
                    
                      
          
                    <p class="bts" target="_blank" >4. <b>Li Chen</b>*, <b>Chonghao Sima, Yang Li</b>, Zehan Zheng, Jiajie Xu, Xiangwei Geng, Hongyang Li, Conghui He, Jianping Shi, Yu Qiao, Junchi Yan, "PersFormer: 3D Lane Detection via Perspective Transformer and the OpenLane Benchmark", <b>ECCV 2022 (Oral) </b>  <body>
                      <a href="https://doi.org/10.48550/arXiv.2203.11089" style="color: #0040ff;">[Paper]</a>
                    </body>
                    <body>
                      <a href="https://github.com/OpenPerceptionX/OpenLane" style="color: #FF0000;">[Code]</a>
                    </body><iframe
                    style="margin-left: 2px; margin-bottom:-5px;"
                    frameborder="0" scrolling="0" width="91px" height="20px"
                    src="https://ghbtns.com/github-btn.html?user=OpenPerceptionX&repo=OpenLane&type=star&count=true" >
                   </iframe></p>
                   PersFormer  adopts  a  unified  2D/3D  anchor  design  andan auxiliary task to detect 2D/3D lanes simultaneously, enhancing thefeature consistency and sharing the benefits of multi-task learning. More-over, we release one of the first large-scale real-world 3D lane datasets:OpenLane,  with  high-quality  annotation  and  scenario  diversity.   </p></div>
                   <!-- Open-Lane contains 200,000 frames, over 880,000 instance-level lanes, 14 lanecategories, along with scene tags and the closed-in-path object annota-tions to encourage the development of lane detection and more industrial-related autonomous driving methods.          </p></div> -->
                              
                  </td>			</tr>
              </tbody>
            </table>
          </div></div>

          <!-- ##3############################################################################################## --> 

          <div><div class="wsite-multicol"><div class="wsite-multicol-table-wrap" style="margin:0 -15px;">
            <table class="wsite-multicol-table">
              <tbody class="wsite-multicol-tbody">
                <tr class="wsite-multicol-tr">
                  <td class="wsite-multicol-col" style="width:26.598465473146%; padding:0 15px;">
                    
                      
          
          <div><div class="wsite-image wsite-image-border-none " style="padding-top:10px;padding-bottom:10px;margin-left:0;margin-right:0;text-align:center">
          <a>
          <img src="../images/pub/monoef.png" alt="Picture" style="width:auto;max-width:100%;height:150px;" />
          </a>
          <div style="display:block;font-size:90%"></div>
          </div></div>
          
          
                    
                  </td>				<td class="wsite-multicol-col" style="width:73.401534526854%; padding:0 15px;">
                    
                      
          
          <div class="paragraph">          <p class="bts" target="_blank" >3. <b>Yunsong Zhou</b>*, Yuan He, Hongzi Zhu, Cheng Wang, Hongyang Li, Qinhong Jiang, "MonoEF: Extrinsic Parameter Free Monocular 3D Object Detection", <b>PAMI 2022</b> <body>
            <a href="https://doi.org/10.1109/TPAMI.2021.3136899" style="color: #0040ff;">[Paper]</a>
          </body>
          </p>

          Our 3D detector works independent of the extrinsic parameter variations and produces accurate results in realistic cases, e.g., potholed and uneven roads, where almost all existing monocular detectors fail to handle.          </p></div>
                              
                  </td>			</tr>
              </tbody>
            </table>
          </div></div></div>

          <!-- ##2############################################################################################## --> 

          <div><div class="wsite-multicol"><div class="wsite-multicol-table-wrap" style="margin:0 -15px;">
            <table class="wsite-multicol-table">
              <tbody class="wsite-multicol-tbody">
                <tr class="wsite-multicol-tr">
                  <td class="wsite-multicol-col" style="width:26.598465473146%; padding:0 15px;">
                    
                      
          
          <div><div class="wsite-image wsite-image-border-none " style="padding-top:10px;padding-bottom:10px;margin-left:0;margin-right:0;text-align:center">
          <a>
          <img src="../images/pub/Screenshot_2023-01-27 Monocular 3D Object Detection An Extrinsic Parameter Free Approach - Zhou_Monocular_3D_Object_Detecti[...].png" alt="Picture" style="width:auto;max-width:100%;height:150px;" />
          </a>
          <div style="display:block;font-size:90%"></div>
          </div></div>
          
          
                    
                  </td>				<td class="wsite-multicol-col" style="width:73.401534526854%; padding:0 15px;">
                    
                      
          
          <div class="paragraph">         <p class="bts" target="_blank" >2. <b>Yunsong Zhou</b>, Yuan He*, Hongzi Zhu*, Cheng Wang, Hongyang Li, Qinhong Jiang, "Monocular 3D Object Detection: An Extrinsic Parameter Free Approach", <b>CVPR 2021</b> <body>
            <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Zhou_Monocular_3D_Object_Detection_An_Extrinsic_Parameter_Free_Approach_CVPR_2021_paper.html" style="color: #0040ff;">[Paper]</a>
          </body><body>
            <a href=" https://github.com/ZhouYunsong-SJTU/MonoEF" style="color: #FF0000;">[Code]</a>
          </body>
          <iframe
          style="margin-left: 2px; margin-bottom:-5px;"
          frameborder="0" scrolling="0" width="91px" height="20px"
          src="https://ghbtns.com/github-btn.html?user=ZhouYunsong-SJTU&repo=MonoEF&type=star&count=true">
         </iframe></p>


         Aconverter is designed to rectify perturbative features in thelatent space. By doing so, our 3D detector works indepen-dent of the extrinsic parameter variations and produces ac-curate results in realistic cases, e.g., potholed and unevenroads, where almostallexisting monocular detectors failto handle.          </p></div>
                              
                  </td>			</tr>
              </tbody>
            </table>
          </div></div></div>

          <!-- ##1############################################################################################## --> 

          <div><div class="wsite-multicol"><div class="wsite-multicol-table-wrap" style="margin:0 -15px;">
            <table class="wsite-multicol-table">
              <tbody class="wsite-multicol-tbody">
                <tr class="wsite-multicol-tr">
                  <td class="wsite-multicol-col" style="width:26.598465473146%; padding:0 15px;">
                    
                      
          
          <div><div class="wsite-image wsite-image-border-none " style="padding-top:10px;padding-bottom:10px;margin-left:0;margin-right:0;text-align:center">
          <a>
          <img src="../images/pub/expl.png" alt="Picture" style="width:auto;max-width:100%;height:150px;" />
          </a>
          <div style="display:block;font-size:90%"></div>
          </div></div>
          
          
                    
                  </td>				<td class="wsite-multicol-col" style="width:73.401534526854%; padding:0 15px;">
                    
                      
                    <p class="bts" target="_blank" >1. <b>Shichao Li</b>, Zengqiang Yan, Hongyang Li, Kwang-Ting Cheng, "Exploring Intermediate Representation for Monocular Vehicle Pose Estimation", <b>CVPR 2021</b> <body>
                      <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Li_Exploring_intermediate_representation_for_monocular_vehicle_pose_estimation_CVPR_2021_paper.html" style="color: #0040ff;">[Paper]</a>
                    </body>
                    <body>
                      <a href="https://github.com/Nicholasli1995/EgoNet" style="color: #FF0000;">[Code]</a>
                    </body><iframe
                    style="margin-left: 2px; margin-bottom:-5px;"
                    frameborder="0" scrolling="0" width="91px" height="20px"
                    src="https://ghbtns.com/github-btn.html?user=Nicholasli1995&repo=EgoNet&type=star&count=true" >
                   </iframe></p>
          
                   A novel deep model, Ego-Net, is proposed for accurate vehicle orientation estimation in the camera coordinatesystem via extracting a set of explicit 2D/3D geometrical representations.          </p></div>
                              
                  </td>			</tr>
              </tbody>
            </table>
          </div></div>

<br><br>
          <h3><b><p class="btb" target="_blank" id="General Vision">General Vision</p></b></h3>


          <!-- ##########gvgvgvgv```2###################################################################################### --> 

          <div><div class="wsite-multicol"><div class="wsite-multicol-table-wrap" style="margin:0 -15px;">
            <table class="wsite-multicol-table">
              <tbody class="wsite-multicol-tbody">
                <tr class="wsite-multicol-tr">
                  <td class="wsite-multicol-col" style="width:26.598465473146%; padding:0 15px;">
                    
                      
          
          <div><div class="wsite-image wsite-image-border-none " style="padding-top:10px;padding-bottom:10px;margin-left:0;margin-right:0;text-align:center">
          <a>
          <img src="../images/pub/stare.png" alt="Picture" style="width:auto;max-width:100%;height:150px;" />
          </a>
          <div style="display:block;font-size:90%"></div>
          </div></div>
          
          
                    
                  </td>				<td class="wsite-multicol-col" style="width:73.401534526854%; padding:0 15px;">
                    
                      
          
          <div class="paragraph">          <p class="bts" target="_blank" >2. <b>Hongwei Xue</b>, Peng Gao, Hongyang Li, Yu Qiao, Hao Sun, Houqiang Li, Jiebo Luo, "Stare at What You See: Masked Image Modeling without Reconstruction" (arXiv, 2022) <body>
            <a href="https://doi.org/10.48550/arXiv.2211.08887" style="color: #0040ff;">[Paper]</a>
          </body>
          <body>
            <a href="https://github.com/OpenPerceptionX/maskalign" style="color: #FF0000;">[Code]</a>
          </body><iframe
          style="margin-left: 2px; margin-bottom:-5px;"
          frameborder="0" scrolling="0" width="91px" height="20px"
          src="https://ghbtns.com/github-btn.html?user=OpenPerceptionX&repo=maskalign&type=star&count=true" >
         </iframe></p>
         We propose an efficient MIM paradigm named MaskAlign.MaskAlign  simply  learns  the  consistency  of  visible  patchfeatures  extracted  by  the  student  model  and  intact  imagefeatures  extracted  by  the  teacher  model.          </p></div>
                              
                  </td>			</tr>
              </tbody>
            </table>
          </div></div>

          <!-- ###1############################################################################################# --> 

          <div>
            <div class="wsite-multicol">
              <div class="wsite-multicol-table-wrap" style="margin:0 -15px;">
              <table class="wsite-multicol-table">
              <tbody class="wsite-multicol-tbody">
                <tr class="wsite-multicol-tr">
                  <td class="wsite-multicol-col" style="width:26.598465473146%; padding:0 15px;">
                    
                      
          
                  <div><div class="wsite-image wsite-image-border-none " style="padding-top:10px;padding-bottom:10px;margin-left:0;margin-right:0;text-align:center">
                  <a>
                  <img src="../images/pub/align.png" alt="Picture" style="width:auto;max-width:100%;height:150px;" />
                  </a>
                  <div style="display:block;font-size:90%"></div>
                  </div></div>
          
          
                    
                  </td>				<td class="wsite-multicol-col" style="width:73.401534526854%; padding:0 15px;">
                    
                      
                          <p class="bts" target="_blank" >1. <b>Shaofeng Zhang</b>, Lyn Qiu, Feng Zhu, Junchi Yan*, Hengrui Zhang, Rui Zhao, Hongyang Li, Xiaokang Yang, "Align Representations With Base: A New Approach to Self-Supervised Learning", <b>CVPR 2022</b><body>
                            <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_Align_Representations_With_Base_A_New_Approach_to_Self-Supervised_Learning_CVPR_2022_paper.html" style="color: #0040ff;">[Paper]</a>
                          </body></p>
                We  propose PPGeo(Policy  Pre-training  via  Geometric  modeling),  an  intuitiveand straightforward fully self-supervised framework curated for the policy pre-training in visuomotor driving.  
                </p></div>
                              
                  </td>			</tr>
              </tbody>
            </table>
          </div></div>

 

          <!-- <hr /> -->
        
          Note:
          <br>
          The name in bold is the first author or the same contributing author of the paper.
          <br>
          The name with * is the correspondence author of the paper. 
        </article>
    </ul>

    <!-- ################################################################################################ -->


    <!-- ################################################################################################ -->
    <!-- ################################################################################################ -->
    <!-- / main body -->
  </main>
</div>



<div class="wrapper row4">
  <footer id="footer" class="hoc clear"  style="background-image:url('../images/gallery/stage-publications.jpeg');"> 
    <!-- ################################################################################################ -->
    <div class="center btmspace-80">
     <div class="four_quarter"> 

      <h6 class="heading">OpenDriveLab</h6>
      <nav>
        <ul class="nospace inline pushright uppercase">
          <li><a href="../index.html"><i class="fas fa-lg fa-home"></i></a></li>
          <li><a href="#">About</a></li>
          <!-- <li><a href="#">Contact</a></li> -->
          <li><a href="#">Terms</a></li>
          <li><a href="#">Privacy</a></li>
          <!-- <li><a href="#">Cookies</a></li> -->
          <li><a href="#">Disclaimer</a></li>
        </ul>
      </nav>
    </div>
  </div>
    <!-- ################################################################################################ -->
  </footer>
</div>
<!-- ################################################################################################ -->
<!-- ################################################################################################ -->
<!-- ################################################################################################ -->
<div class="wrapper row5">
  <div id="copyright" class="hoc clear"> 
    <!-- ################################################################################################ -->
    <p class="fl_left">Copyright &copy; 2023 - All Rights Reserved - OpenDriveLab</p>
    <p class="fl_right">Template by <a target="_blank" href="https://www.os-templates.com/" title="Free Website Templates">OS Templates</a></p>
    <!-- ################################################################################################ -->
  </div>
</div>
<!-- ################################################################################################ -->
<!-- ################################################################################################ -->
<!-- ################################################################################################ -->
<a id="backtotop" href="#top"><i class="fas fa-chevron-up"></i></a>
<!-- JAVASCRIPTS -->
<script src="../layout/scripts/jquery.min.js"></script>
<script src="../layout/scripts/jquery.backtotop.js"></script>
<script src="../layout/scripts/jquery.mobilemenu.js"></script>
</body>
</html>