<!DOCTYPE html>
<!--
Template Name: Besloor
Author: <a href="https://www.os-templates.com/">OS Templates</a>
Author URI: https://www.os-templates.com/
Copyright: OS-Templates.com
Licence: Free to use under our free template licence terms
Licence URI: https://www.os-templates.com/template-terms
-->
<html lang="">
<!-- To declare your language - read more here: https://www.w3.org/International/questions/qa-html-language-declarations -->

<head>
  <title> Publication | OpenDriveLab </title>
  <meta name="description" content="Most of the influential and interesting publications published by OpenDriveLab come from top conferences and journals in the computer field, such as ICML, ECCV, ICCV, NeurIPS, ICLR, CVPR.">
  <meta name="keywords" content="OpenDriveLab Publication, Autonomous Driving, E2EAD, BEV, OpenLane, Bird-eye-view Perception, End-to-end Autonomous Driving, ">
  <meta name="author" content="OpenDriveLab">
  <link rel="icon" 
      type="image/png" 
      href="../images/team_logo.png">

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
  <link href="../layout/styles/layout.css" rel="stylesheet" type="text/css" media="all">
</head>

<body id="top">

  <!-- Top Background Image Wrapper -->
  <div class="bgded overlay " style="background-image:url('../images/gallery/stage-publications.jpeg');overflow: hidden;">


    <!-- ################################################################################################ -->
    <div class="wrapper row0">
      <div id="topbar" class="hoc clear">
        <!-- ################################################################################################ -->
        <div class="fl_left">
          <!-- ################################################################################################ -->
          <ul class="nospace"></ul>
          <!-- ################################################################################################ -->
        </div>
        <div class="fl_right">
          <!-- ################################################################################################ -->
          <ul class="nospace"></ul>
          <!-- ################################################################################################ -->
        </div>
        <!-- ################################################################################################ -->
      </div>
    </div>
    <!-- ################################################################################################ -->
    <!-- ################################################################################################ -->
    <!-- ################################################################################################ -->
    <div class="wrapper row1">
      <header id="header" class="hoc clear">
        <!-- ################################################################################################ -->
        <div id="logo" class="fl_left">
          <h1>OpenDriveLab </h1>
        </div>
        <!-- ################################################################################################ -->
        <nav id="mainav" class="fl_right">
          <ul class="clear">
            <li><a href="../index.html">Home</a></li>

            <li><a href="../event/index.html" class="drop">Event</a>
              <ul>
                <li><a href="../sr4ad/iclr23.html">ICLR 2023 Workshop</a></li>
                <li><a href="../e2ead/cvpr23.html">CVPR 2023 Workshop</a></li>

              </ul>
            </li>
            <li class="active"><a href="../publication/index.html">Publication</a></li>
          </ul>
        </nav>
        <!-- ################################################################################################ -->
      </header>
    </div>
    <!-- ################################################################################################ -->
    <!-- ################################################################################################ -->
    <!-- ################################################################################################ -->

    <div id="breadcrumb" class="hoc clear">
      <!-- ################################################################################################ -->
      <h2 class="heading">Publications</h2>
      <!-- ################################################################################################ -->
    </div>
  </div>

  <div class="wrapper row3">
    <main class="hoc container clear">
      <style>

        .wrapper .sidebar nav {
          position: relative !important;
          top: 0;
          width: 100%;
        }
        .sidebar.one_quarter.first {
          position: sticky;
          top: 50%;
        }
        

      </style>
      <!-- main body -->
      <ul class="nospace group overview btmspace-80">
        <div class="sidebar one_quarter first">
          <!-- ################################################################################################ -->
          <h6> </h6>
          <nav class="sdb_holder" >
            <ul>
              <h6>Publications</h6>
              <li><a href="#editor-pick">Editor's Pick</a></li>
              <li><a href="#end-to-end">End-to-end Autonomous Driving</a></li>
              <li><a href="#bev-perception">Bird's-eye-view Perception</a></li>
              <li><a href="#pp">Prediction and Planning</a></li>
              <li><a href="#cv-at-large">Computer Vision at Large</a>
            </ul>
          </nav>

        </div>
        <!-- ################################################################################################ -->

        <div class="content three_quarter publication">

          <article>

            <!-- ################################################################################################ -->
            <p> <topic>
            We position OpenDriveLab as one of the most top research teams around globe, 
            since we've got talented people and published work at top venues.
          </topic>
            </p>

            <h3><b><p class="btb" target="_blank" id="editor-pick">Editor's Pick</p></b></h3><br>

            <table>
              <tr>
                <td class="wsite-multicol-col-left" >
                  <div class="wsite-image" style="padding-top:10px">
                    <a><img src="../images/pub/goal.png" alt="Picture"/></a>
                  </div>
              </td>
              <td class="wsite-multicol-col-right" style="padding: 0px 15px;">
                <div>
                  <p class="medium"><b><a class = 'hover_color'  href="https://arxiv.org/abs/2212.10156" >Planning-oriented Autonomous Driving</a></b></p>
                  <p class="bts" target="_blank">Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Hongyang Li,  <em>et al.</em><br>
                    <a style="background-color: #FEF5E7; color: #D68910;">CVPR 2023 Award Candidate </a>

                    <a href="https://github.com/OpenDriveLab/UniAD">
                      <img
                        src="https://img.shields.io/github/stars/OpenDriveLab/UniAD?style=social"
                        alt="GitHub" style="height: 20px"
                      />
                    </a>
                    <a href="https://zhuanlan.zhihu.com/p/597019546">
                      <img
                        src="../images/pub/zhihu2.png"
                        style="height: 20px"
                      />
                    </a>
                    <a href="https://mp.weixin.qq.com/s/QBRTiku0_rF6GM1fHfi4lw">
                      <img
                        src="../images/pub/gvlab.jpg"
                        style="height: 20px"
                      />
                    </a>
                    
                    <code>[Sell a New Philosophy]</code>
                    <!-- <code>   <a class = 'hover_color' href="https://mp.weixin.qq.com/s/QBRTiku0_rF6GM1fHfi4lw">[The publicity of OpenGVLab]</a> </code> -->
                    <br><a style="color: gray;font-style:italic;">A comprehensive framework up-to-date that incorporates full-stack driving tasks in one network. </a>
                  </p>
                </div>
              </td>
              </tr>
              

              <tr>
                <td class="wsite-multicol-col-left" >
                  <div class="wsite-image" style="padding-top:30px">
                      <a><img src="../images/pub/bevf1.png" alt="Picture"></a>
                    </div>
                </td>
                <td class="wsite-multicol-col-right" style="padding: 0px 15px;">
                  <div>
                    <p class="medium"><b><a class = 'hover_color' href="https://arxiv.org/abs/2203.17270" >BEVFormer: Learning Bird's-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers</a></b></p>
                    <p class="bts" target="_blank">Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, <em>et al.</em>
                      <br> <a style="background-color: #FEF5E7; color: #D68910;">ECCV 2022</a> 

                      <a href="https://github.com/fundamentalvision/BEVFormer">
                        <img
                          src="https://img.shields.io/github/stars/fundamentalvision/BEVFormer?style=social"
                          alt="GitHub" style="height: 20px"
                        />
                        
                      <a href="https://zhuanlan.zhihu.com/p/564295059">
                        <img
                          src="../images/pub/zhihu2.png"
                          style="height: 20px"
                        />
                      </a>

                      <code>[Baseline][nuScenes First Place]
                        <br>[Waymo Challenge 2022 Official First Place]</code>
                      <br>
                      <a style="color: gray;font-style:italic;">A paradigm for autonomous driving that applies both Transformer and Temporal structure to generate BEV features.</a>
                    </p>
                  </div>
                </td>
              </tr>


              <tr>
                <td class="wsite-multicol-col-left" >
                  <div class="wsite-image" style="padding-top:30px">
                      <a><img src="../images/pub/persfor.png" alt="Picture"></a>
                    </div>
                </td>
                <td class="wsite-multicol-col-right" style="padding: 0px 15px;">
                  <div>
                    <p class="medium"><b><a class = 'hover_color' href="https://arxiv.org/abs/2203.11089">PersFormer: 3D Lane Detection via Perspective Transformer and the OpenLane Benchmark</a></b></p>
                    <p class="bts" target="_blank"> <a href="https://scholar.google.com/citations?user=ulZxvY0AAAAJ&hl=en" style="color:blue">Li Chen</a>, Chonghao Sima, Yang Li, Xiangwei Geng, Junchi Yan, <em>et al.</em>
                      <br>
                      <a style="background-color: #FEF5E7; color: #D68910;">ECCV 2022 (Oral)</a>
                      
                      <a href="https://github.com/OpenDriveLab/OpenLane">
                        <img
                          src="https://img.shields.io/github/stars/OpenDriveLab/OpenLane?style=social"
                          alt="GitHub" style="height: 20px"
                        />
                      </a>
                      <a href="https://github.com/OpenDriveLab/PersFormer_3DLane">
                        <img
                          src="https://img.shields.io/github/stars/OpenDriveLab/PersFormer_3DLane?style=social"
                          alt="GitHub" style="height: 20px"
                        />
                      </a>

                      <a href="https://zhuanlan.zhihu.com/p/495979738">
                        <img
                          src="../images/pub/zhihu2.png"
                          style="height: 20px"
                        />
                      </a>
                      
                      <code>[Redefine the Community]</code>
                      <br><a style="color: gray;font-style:italic;">PersFormer adopts a unified 2D/3D anchor design and an auxiliary task to detect 2D/3D lanes; we release one of the first large-scale real-world 3D lane datasets, OpenLane. </a>
                    </p>
                  </div>
                </td>
              </tr>

            </table>

            <br><br>
            <!-- #####14########################################################################################### -->

            <h3><b><p class="btb" target="_blank" id="end-to-end">End-to-end Autonomous Driving</p></b></h3>
            <table>

              <tr>
                <td class="wsite-multicol-col-left" >
                  <div class="wsite-image" style="padding-top:20px">
                      <a><img src="../images/pub/thinktwice.png"></a>
                    </div>
                </td>
                <td class="wsite-multicol-col-right" style="padding: 0px 15px;">
                  <div>
                    <p class="medium"><b><a  class = 'hover_color' href="../file/1617_thinktwice_camera_ready.pdf">Think Twice before Driving: Towards Scalable Decoders for End-to-End Autonomous Driving</a></b></p>
                    <p class="bts" target="_blank">Xiaosong Jia, Penghao Wu, Li Chen, <em>et al.</em>
                      <br>
                    <a  style="background-color: #FEF5E7; color: #D68910;">CVPR 2023</a>

                   <a href="https://github.com/OpenDriveLab/ThinkTwice">
                    <img
                      src="https://img.shields.io/github/stars/opendrivelab/thinktwice?style=social"
                      alt="GitHub" style="height: 20px"
                    />
                  </a>
                    <br><a style="color: gray;font-style:italic;">
                    A scalable decoder paradigm that generates the future trajectory and action of the ego vehicle for 
                  end-to-end autonomous driving. </a>
                  </p>
                  </div>
                </td>
              </tr>

              <tr>
                <td class="wsite-multicol-col-left" >
                    <div class="wsite-image" style="padding-top:20px">
                      <a><img src="../images/pub/Policy.png"></a>
                    </div>
                </td>
                <td class="wsite-multicol-col-right" style="padding: 0px 15px;">
                  <div>
                    <p class="medium"><b><a  class = 'hover_color' href="https://arxiv.org/abs/2301.01006" >Policy Pre-Training for End-to-End Autonomous Driving via Self-Supervised Geometric Modeling</a></b></p>
                    <p class="bts" target="_blank"> 
                      <a href="https://scholar.google.com/citations?user=9mssd5EAAAAJ&hl=en" style="color:blue">Penghao Wu</a>, Li Chen, Hongyang Li, Xiaosong Jia, Junchi Yan, Yu Qiao<br>
                      <a style="background-color: #FEF5E7; color: #D68910;">ICLR 2023</a>

                        <a href="https://github.com/opendrivelab/ppgeo">
                          <img
                            src="https://img.shields.io/github/stars/opendrivelab/ppgeo?style=social"
                            alt="GitHub" style="height: 20px"
                          />
                        </a>
                        
                      <a href="https://zhuanlan.zhihu.com/p/601456429">
                        <img
                          src="../images/pub/zhihu2.png"
                          style="height: 20px"
                        />
                      </a>
                      <a href="https://www.jiqizhixin.com/articles/2023-01-27-2?from=synced&keyword=opendrivelab">
                        <img
                          src="../images/pub/jiqizhixin.png"
                          style="height: 20px"
                        />
                      </a>
                      <b style="user-select: none; color: gray; font-size: 20px; font-weight: 900;">|</b>                     
                       <a href="https://docs.google.com/presentation/d/1d0MGh3XCxuZujtYgZ0sr6xsAKZ4uS50p/edit?usp=sharing&ouid=118212253182146260973&rtpof=true&sd=true">
                        <img
                          src="/style/img/icon/blue_presentation.png"
                          style="height: 20px"
                        />
                      </a>

                        <br><a style="color: gray;font-style:italic;"> An intuitive and straightforward fully self-supervised framework curated for the policy pre-training in visuomotor driving. </a>
                    </p>
                  </div>
                </td>
              </tr>
              

              <tr>
                <td class="wsite-multicol-col-left" >
                  <div class="wsite-image" style="padding-top:20px">
                      <a><img src="../images/pub/traj.png" alt="Picture"></a>
                    </div>
                </td>
                <td class="wsite-multicol-col-right" style="padding: 0px 15px;">
                  <div>
                    <p class="medium"><b><a  class = 'hover_color' href="https://arxiv.org/abs/2206.08129">Trajectory-guided Control Prediction for End-to-end Autonomous Driving: A Simple yet Strong Baseline</a></b></p>
                    <p class="bts" target="_blank">Penghao Wu, Xiaosong Jia, Li Chen, Junchi Yan, Hongyang Li, Yu Qiao 
                      <br>
                    <a  style="background-color: #FEF5E7; color: #D68910;">NeurIPS 2022</a>

                    <a href="https://github.com/OpenPerceptionX/TCP">
                      <img
                        src="https://img.shields.io/github/stars/OpenPerceptionX/TCP?style=social"
                        alt="GitHub" style="height: 20px"
                      />
                    </a>
                    <a href="https://zhuanlan.zhihu.com/p/532665469">
                      <img
                        src="../images/pub/zhihu2.png"
                        style="height: 20px"
                      />
                    </a>

                    
                    <code>[Carla First Place]</code>
                    <br><a style="color: gray;font-style:italic;">Take the initiative to explore the combination of controller based on a planned trajectory and perform control prediction. </a>
                  </p>
                  </div>
                </td>
              </tr>


              <tr>
                <td class="wsite-multicol-col-left" >
                  <div class="wsite-image" style="padding-top:20px">
                      <a><img src="../images/pub/stp3.png" alt="Picture"></a>
                    </div>
                </td>
                <td class="wsite-multicol-col-right" style="padding: 0px 15px;">
                  <div>
                    <p class="medium"><b><a  class = 'hover_color' href="https://arxiv.org/abs/2207.07601">ST-P3: End-to-end Vision-based Autonomous Driving via Spatial-Temporal Feature Learning</a></b></p>
                    <p class="bts" target="_blank">Shengchao Hu, Li Chen, Penghao Wu, Hongyang Li, <em>et al.</em>
                      <br>
                    <a  style="background-color: #FEF5E7; color: #D68910;">ECCV 2022</a>

                   <a href="https://github.com/openperceptionx/st-p3">
                    <img
                      src="https://img.shields.io/github/stars/openperceptionx/st-p3?style=social"
                      alt="GitHub" style="height: 20px"
                    />
                  </a>
                  <a href="https://zhuanlan.zhihu.com/p/544387122">
                    <img
                      src="../images/pub/zhihu2.png"
                      style="height: 20px"
                    />
                  </a>
                    <br><a style="color: gray;font-style:italic;">A spatial-temporal feature learning scheme towards a set of more representative features for perception, prediction and planning tasks simultaneously. </a>
                  </p>
                  </div>
                </td>
              </tr>

            </table>


            <br><br>
            <!-- #####14########################################################################################### -->

            <h3><b><p class="btb" target="_blank" id="bev-perception">Bird's-eye-view Perception</p></b></h3>
            <table>





              
              <!-- <tr>
                <td class="wsite-multicol-col-left" >
                  <div class="wsite-image" >
                      <a><img src="../style/img/publications/focaldistill.jpg" alt="Picture"></a>
                    </div>
                </td>
                <td class="wsite-multicol-col-right" style="padding: 0px 15px;">
                  <div>
                    <p class="medium"><b><a>Distilling Focal Knowledge from Imperfect Expert for 3D Object Detection</a></b></p>
                    <p class="bts" target="_blank">Jia Zeng, Li Chen, Hanming Deng, Lewei Lu, <em>et al.</em><br>
                      <br><a style="color: gray;font-style:italic;">We investigate how to distill the focal knowledge from an imperfect bird's-eye-view (BEV) detector expert.</a>
                    </p>
                  </div>
                </td>
              </tr> -->






              <tr>


                <td class="wsite-multicol-col-left" >
                  <div class="wsite-image"style="padding-top:10px">
                    <a><img src="../images/pub/Delving.png" alt="Picture"/></a>
                  </div>
              </td>
              <td class="wsite-multicol-col-right" style="padding: 0px 15px;">
                <div>
                  <p class="medium"><b><a class = 'hover_color' href="https://arxiv.org/abs/2209.05324">Delving into the Devils of Bird's-Eye-View Perception: A
                      Review, Evaluation and Recipe</a></b></p>
                  <p class="bts" target="_blank"> Hongyang Li, <a href="https://scholar.google.com/citations?user=dgYJ6esAAAAJ&hl=en" style="color:blue">Chonghao Sima</a>, Jifeng Dai, Wenhai Wang, Lewei Lu, <em>et al.</em> <br>
                    <a style="background-color: #FEF5E7; color: #D68910;">arXiv 2022</a> 
                    <a href="https://github.com/opendrivelab/bevperception-survey-recipe">
                      <img
                        src="https://img.shields.io/github/stars/opendrivelab/bevperception-survey-recipe?style=social"
                        alt="GitHub" style="height: 20px"
                      />
                    </a>
                    <a href="https://zhuanlan.zhihu.com/p/565212506">
                      <img
                        src="../images/pub/zhihu2.png"
                        style="height: 20px"
                      />
                    </a>
                    <a href="https://www.jiqizhixin.com/articles/2023-02-14-4?from=synced&keyword=opendrivelab">
                      <img
                        src="../images/pub/jiqizhixin.png"
                        style="height: 20px"
                      />
                    </a>

                    <code>[Setup the Table]</code>
                    <br>
                    <a style="color: gray;font-style:italic;">We review the most recent work on BEV perception and provide analysis of different solutions.</a>
                  </p>
                </div>
              </td>



              </tr>
              

              <tr>
                <td class="wsite-multicol-col-left" >
                  <div class="wsite-image" style="padding-top:25px">
                      <a><img src="../images/pub/befv2.png" alt="Picture"></a>
                    </div>
                </td>
                <td class="wsite-multicol-col-right" style="padding: 0px 15px;">
                  <div>
                    <p class="medium"><b><a class = 'hover_color'  href="https://arxiv.org/abs/2211.10439">BEVFormer v2: Adapting Modern Image Backbones to Bird's-Eye-View Recognition via Perspective Supervision</a></b></p>
                    <p class="bts" target="_blank">Chenyu Yang, Xizhou Zhu, Hongyang Li, Jifeng Dai, <em>et al.</em><br>
                      <a  style="background-color: #FEF5E7; color: #D68910;">CVPR 2023 Highlight</a>
                      <br><a style="color: gray;font-style:italic;">A novel bird's-eye-view (BEV) detector with perspective supervision, which converges faster and better suits modern image backbones. </a>
                    </p>
                  </div>
                </td>
              </tr>

              <tr>
                <td class="wsite-multicol-col-left" >
                  <div class="wsite-image" style="padding-top:20px">
                      <a><img src="../images/pub/focaldistiller.png"></a>
                    </div>
                </td>
                <td class="wsite-multicol-col-right" style="padding: 0px 15px;">
                  <div>
                    <p class="medium"><b><a  class = 'hover_color' href="../file/1662_focaldistiller_camera_ready.pdf">Distilling Focal Knowledge from Imperfect Expert for 3D Object Detection</a></b></p>
                    <p class="bts" target="_blank">Jia Zeng, Li Chen, <em>et al.</em>
                      <br>
                    <a  style="background-color: #FEF5E7; color: #D68910;">CVPR 2023</a>

                    <br><a style="color: gray;font-style:italic;">
                    We investigate on how to distill the knowledge from an imperfect expert. We propose FD3D, a
                  Focal Distiller for 3D object detection. </a>
                  </p>
                  </div>
                </td>
              </tr>


            </table>


      <br><br>
            <!-- #####14########################################################################################### -->


            <h3><b><p class="btb" target="_blank" id="pp">Prediction and Planning</p></b></h3>

            <table>

              <!-- <tr>
                <td class="wsite-multicol-col-left" >
                    <div class="wsite-image" >
                      <a><img src="../style/img/publications/thinktwice.jpg" alt="Picture"/></a>
                    </div>
                </td>
                <td class="wsite-multicol-col-right" style="padding: 0px 15px;">
                  <div>
                    <p class="medium"><b>Think Twice before Driving:Towards Scalable Decoders for End-to-End Autonomous Driving</a></b></p>
                    <p class="bts" target="_blank">Xiaosong Jia, Penghao Wu, Li Chen, Jiangwei Xie, <em>et al.</em><br>
                      
                      <a href="https://github.com/OpenDriveLab/ThinkTwice">
                        <img
                          src="https://img.shields.io/github/stars/OpenDriveLab/ThinkTwice?style=social"
                          alt="GitHub" style="height: 20px"
                        />
                      </a>
                      <br><a style="color: gray;font-style:italic;">Scale up the decoder to achieve SOTA performance by gradually refining decisions according to their corresponding consequences.</a>
                    </p>
                  </div>
                </td>
              </tr> -->





              <tr>
                <td class="wsite-multicol-col-left" >
                    <div class="wsite-image" style="padding-top:30px">
                      <a><img src="../images/pub/towards.png" alt="Picture"/></a>
                    </div>
                </td>
                <td class="wsite-multicol-col-right" style="padding: 0px 15px;">
                  <div>
                    <p class="medium"><b><a  class = 'hover_color' href="https://openreview.net/forum?id=PZiKO7mjC43" >Towards Capturing the Temporal Dynamics for Trajectory Prediction: a Coarse-to-Fine Approach</a></b></p>
                    <p class="bts" target="_blank">Xiaosong Jia, Li Chen, Penghao Wu, Jia Zeng, <em>et al.</em><br>
                      <a style="background-color: #FEF5E7; color: #D68910;">CoRL 2022 </a>

                      <br><a style="color: gray;font-style:italic;">We find taking scratch trajectories generated by MLP as input, a refinement module based on structures with temporal prior, could  boost the accuracy. </a>
                    </p>
                  </div>
                </td>
              </tr>
              

              <tr>
                <td class="wsite-multicol-col-left" >
                  <div class="wsite-image" style="padding-top:00px">
                      <a><img src="../images/pub/hdgt-2.jpg" alt="Picture"></a>
                    </div>
                </td>
                <td class="wsite-multicol-col-right" style="padding: 0px 15px;">
                  <div>
                    <p class="medium"><b><a class = 'hover_color'  href="https://arxiv.org/abs/2205.09753">HDGT: Heterogeneous driving graph transformer for multi-agent trajectory prediction via scene encoding</a></b></p>
                    <p class="bts" target="_blank">Xiaosong Jia, Penghao Wu, Li Chen, Hongyang Li, Yu Liu, Junchi Yan<br>
                      <a  style="background-color: #FEF5E7; color: #D68910;">arXiv 2022 </a>
                      <a href="https://github.com/OpenPerceptionX/HDGT">
                        <img
                          src="https://img.shields.io/github/stars/OpenPerceptionX/HDGT?style=social"
                          alt="GitHub" style="height: 20px"
                        />
                      </a>
                      <br><a style="color: gray;font-style:italic;"> HDGT formulates the driving scene as a heterogeneous graph with different types of nodes and edges.
                      </a>
                    </p>
                  </div>
                </td>
              </tr>

            </table>



      <br><br>
            <!-- #####14########################################################################################### -->

            <h3><b><p class="btb" target="_blank" id="cv-at-large">Computer Vision at Large</p></b></h3>


            <table>
              <tr>
                <td class="wsite-multicol-col-left" >
                    <div class="wsite-image" style="padding-top:20px">
                      <a><img src="../images/pub/stare.png" alt="Picture"/></a>
                    </div>
                </td>
                <td class="wsite-multicol-col-right" style="padding: 0px 15px;">
                  <div>
                    <p class="medium"><b><a  class = 'hover_color'  href="https://arxiv.org/abs/2211.08887" >Stare at What You See: Masked Image Modeling without Reconstruction</a></b></p>
                    <p class="bts" target="_blank">Hongwei Xue, Peng Gao, Hongyang Li, <em>et al.</em><br>
                      <a style="background-color: #FEF5E7; color: #D68910;">CVPR 2023 </a>
                      <a href="https://github.com/OpenPerceptionX/maskalign">
                        <img
                          src="https://img.shields.io/github/stars/OpenPerceptionX/maskalign?style=social"
                          alt="GitHub" style="height: 20px"
                        />
                      </a>

                      
                      
                      <!-- <code>[Sell a New Philosophy]</code> -->
                      <br><a style="color: gray;font-style:italic;">
                        An efficient MIM paradigm MaskAlign and a Dynamic Alignment module to apply learnable alignment to tackle the problem of input inconsistency. </a>
                    </p>
                  </div>
                </td>
              </tr>
              

            </table>



      <!-- ################################################################################################ -->
      <!-- ################################################################################################ -->
        
          </article>
        </div> 
      </ul>

      <!-- ################################################################################################ -->


    </main>
  </div>



  <div class="wrapper row4">
    <footer id="footer" class="hoc clear">
      <!-- ################################################################################################ -->
      <div class="center btmspace-80">
        <div class="four_quarter">

          <h6 class="heading">OpenDriveLab</h6>
          <nav>
            <ul class="nospace inline pushright uppercase">
              <li><a style="color:#D68910;" href="../index.html"><i class="fas fa-lg fa-home"></i></a></li>

            </ul>
          </nav>
        </div>
      </div>
      <!-- ################################################################################################ -->
    </footer>
  </div>
  <!-- ################################################################################################ -->
  <!-- ################################################################################################ -->
  <!-- ################################################################################################ -->
  <div class="wrapper row5">
    <div id="copyright" class="hoc clear">
      <!-- ################################################################################################ -->
      <p class="fl_left">Copyright &copy; 2023 - All Rights Reserved - OpenDriveLab</p>

      <!-- ################################################################################################ -->
    </div>
  </div>
  <!-- ################################################################################################ -->
  <!-- ################################################################## ############################## -->
  <!-- ################################################################################################ -->
  <a id="backtotop" href="#top"><i class="fas fa-chevron-up"></i></a>
  <!-- JAVASCRIPTS -->
  <script src="../layout/scripts/jquery.min.js"></script>
  <script src="../layout/scripts/jquery.backtotop.js"></script>
  <script src="../layout/scripts/jquery.mobilemenu.js"></script>
  
</body>

</html>
